From 8205b764298e1e1e3f9a05f7b5e3d3d94ce56229 Mon Sep 17 00:00:00 2001
From: Maksim Levental <maksim.levental@gmail.com>
Date: Tue, 28 Jan 2025 13:45:21 -0600
Subject: [PATCH 14/16] [NFC] replace TritonGPUToLLVM/Utility.h macros with
 TritonLLVMOpBuilder (#5717)

Fixes #5691 and unblocks #5684
---
 .../Conversion/TritonGPUToLLVM/Utility.h      | 609 +++++++++-----
 .../TritonGPUToLLVM/AssertOpToLLVM.cpp        |  13 +-
 .../TritonGPUToLLVM/ControlFlowOpToLLVM.cpp   |   7 +-
 .../TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp |  75 +-
 .../SharedToDotOperandFMA.cpp                 |  50 +-
 .../TritonGPUToLLVM/ElementwiseOpToLLVM.cpp   |  35 +-
 .../TritonGPUToLLVM/HistogramOpToLLVM.cpp     |  58 +-
 .../TritonGPUToLLVM/MakeRangeOpToLLVM.cpp     |   3 +-
 .../TritonGPUToLLVM/MemoryOpToLLVM.cpp        |  13 +-
 .../TritonGPUToLLVM/ReduceOpToLLVM.cpp        |  48 +-
 .../TritonGPUToLLVM/ReduceScanCommon.h        |   5 +-
 .../TritonGPUToLLVM/ScanOpToLLVM.cpp          |  86 +-
 lib/Conversion/TritonGPUToLLVM/Utility.cpp    | 246 +++---
 .../TritonGPUToLLVM/ViewOpToLLVM.cpp          |  12 +-
 .../TritonAMDGPUToLLVM/BufferOpsEmitter.cpp   |  28 +-
 .../ConvertLayoutOpToLLVM.cpp                 |   3 +-
 .../SharedToDotOperandHelper.cpp              |  46 +-
 .../SharedToDotOperandMFMA.cpp                |  78 +-
 .../SharedToDotOperandWMMA.cpp                |  64 +-
 .../TritonAMDGPUToLLVM/DotOpToLLVM/MFMA.cpp   |  60 +-
 .../TritonAMDGPUToLLVM/DotOpToLLVM/WMMA.cpp   |  40 +-
 .../ElementwiseOpToLLVM.cpp                   | 794 +++++++++---------
 .../TritonAMDGPUToLLVM/LoadStoreOpToLLVM.cpp  | 102 ++-
 .../amd/lib/TritonAMDGPUToLLVM/TargetInfo.cpp |  29 +-
 .../amd/lib/TritonAMDGPUToLLVM/Utility.cpp    |  73 +-
 .../lib/NVGPUToLLVM/NVGPUToLLVMPass.cpp       |  10 +-
 .../TritonNVIDIAGPUToLLVM/BarrierOpToLLVM.cpp |  11 +-
 .../ConvertLayoutOpToLLVM.cpp                 | 168 ++--
 .../SharedToDotOperandMMAv1.cpp               | 157 ++--
 .../SharedToDotOperandMMAv2.cpp               | 172 ++--
 .../DotOpToLLVM/MMAv1.cpp                     |   3 +-
 .../DotOpToLLVM/MMAv2.cpp                     |  13 +-
 .../DotOpToLLVM/WGMMA.cpp                     |  79 +-
 .../ElementwiseOpToLLVM.cpp                   |  22 +-
 .../LoadStoreOpToLLVM.cpp                     | 131 +--
 .../lib/TritonNVIDIAGPUToLLVM/TMAToLLVM.cpp   |  21 +-
 .../lib/TritonNVIDIAGPUToLLVM/TargetInfo.cpp  |  88 +-
 .../TensorPtrOpsToLLVM.cpp                    |   3 +-
 .../UpcastMXFPToLLVM.cpp                      |  74 +-
 .../lib/TritonNVIDIAGPUToLLVM/Utility.cpp     |  41 +-
 40 files changed, 1972 insertions(+), 1598 deletions(-)

diff --git a/include/triton/Conversion/TritonGPUToLLVM/Utility.h b/include/triton/Conversion/TritonGPUToLLVM/Utility.h
index 29b8865c0..cc5569005 100644
--- a/include/triton/Conversion/TritonGPUToLLVM/Utility.h
+++ b/include/triton/Conversion/TritonGPUToLLVM/Utility.h
@@ -27,84 +27,253 @@
 using namespace mlir;
 using namespace mlir::triton;
 
-// Shortcuts for some commonly used LLVM ops to keep code simple and intuitive
-// Operators
-#define inttofloat(...) rewriter.create<LLVM::SIToFPOp>(loc, __VA_ARGS__)
-#define inttoptr(...) rewriter.create<LLVM::IntToPtrOp>(loc, __VA_ARGS__)
-#define ptrtoint(...) rewriter.create<LLVM::PtrToIntOp>(loc, __VA_ARGS__)
-#define zext(...) rewriter.create<LLVM::ZExtOp>(loc, __VA_ARGS__)
-#define sext(...) rewriter.create<LLVM::SExtOp>(loc, __VA_ARGS__)
-#define fpext(...) rewriter.create<LLVM::FPExtOp>(loc, __VA_ARGS__)
-#define trunc(...) rewriter.create<LLVM::TruncOp>(loc, __VA_ARGS__)
-#define udiv(...) rewriter.create<LLVM::UDivOp>(loc, __VA_ARGS__)
-#define urem(...) rewriter.create<LLVM::URemOp>(loc, __VA_ARGS__)
-#define add(...) rewriter.create<LLVM::AddOp>(loc, __VA_ARGS__)
-#define sub(...) rewriter.create<LLVM::SubOp>(loc, __VA_ARGS__)
-#define fadd(...) rewriter.create<LLVM::FAddOp>(loc, __VA_ARGS__)
-#define mul(...) rewriter.create<LLVM::MulOp>(loc, __VA_ARGS__)
-#define fmul(...) rewriter.create<LLVM::FMulOp>(loc, __VA_ARGS__)
-#define smax(...) rewriter.create<LLVM::SMaxOp>(loc, __VA_ARGS__)
-#define umax(...) rewriter.create<LLVM::UMaxOp>(loc, __VA_ARGS__)
-#define fmax(...) rewriter.create<LLVM::MaxNumOp>(loc, __VA_ARGS__)
-#define smin(...) rewriter.create<LLVM::SMinOp>(loc, __VA_ARGS__)
-#define umin(...) rewriter.create<LLVM::UMinOp>(loc, __VA_ARGS__)
-#define fmin(...) rewriter.create<LLVM::MinNumOp>(loc, __VA_ARGS__)
-#define shl(...) rewriter.create<LLVM::ShlOp>(loc, __VA_ARGS__)
-#define lshr(...) rewriter.create<LLVM::LShrOp>(loc, __VA_ARGS__)
-#define and_(...) rewriter.create<LLVM::AndOp>(loc, __VA_ARGS__)
-#define xor_(...) rewriter.create<LLVM::XOrOp>(loc, __VA_ARGS__)
-#define or_(...) rewriter.create<LLVM::OrOp>(loc, __VA_ARGS__)
-#define bitcast(val__, type__)                                                 \
-  rewriter.create<LLVM::BitcastOp>(loc, type__, val__)
-#define addrspacecast(...)                                                     \
-  rewriter.create<LLVM::AddrSpaceCastOp>(loc, __VA_ARGS__)
-#define gep(...) rewriter.create<LLVM::GEPOp>(loc, __VA_ARGS__)
-#define ptr_ty(...) LLVM::LLVMPointerType::get(__VA_ARGS__)
-#define insert_val(...) rewriter.create<LLVM::InsertValueOp>(loc, __VA_ARGS__)
-#define extract_val(...) rewriter.create<LLVM::ExtractValueOp>(loc, __VA_ARGS__)
-#define insert_element(...)                                                    \
-  rewriter.create<LLVM::InsertElementOp>(loc, __VA_ARGS__)
-#define extract_element(...)                                                   \
-  rewriter.create<LLVM::ExtractElementOp>(loc, __VA_ARGS__)
-#define load(...) rewriter.create<LLVM::LoadOp>(loc, __VA_ARGS__)
-#define store(...) rewriter.create<LLVM::StoreOp>(loc, __VA_ARGS__)
-#define fcmp_ogt(lhs, rhs)                                                     \
-  rewriter.create<LLVM::FCmpOp>(loc, rewriter.getI1Type(),                     \
-                                LLVM::FCmpPredicate::ogt, lhs, rhs)
-#define fcmp_olt(lhs, rhs)                                                     \
-  rewriter.create<LLVM::FCmpOp>(loc, rewriter.getI1Type(),                     \
-                                LLVM::FCmpPredicate::olt, lhs, rhs)
-#define fcmp_eq(lhs, rhs)                                                      \
-  rewriter.create<LLVM::FCmpOp>(loc, rewriter.getI1Type(),                     \
-                                LLVM::FCmpPredicate::oeq, lhs, rhs)
-#define icmp_eq(...)                                                           \
-  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::eq, __VA_ARGS__)
-#define icmp_ne(...)                                                           \
-  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::ne, __VA_ARGS__)
-#define icmp_slt(...)                                                          \
-  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::slt, __VA_ARGS__)
-#define icmp_sle(...)                                                          \
-  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::sle, __VA_ARGS__)
-#define icmp_sgt(...)                                                          \
-  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::sgt, __VA_ARGS__)
-#define icmp_sge(...)                                                          \
-  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::sge, __VA_ARGS__)
-#define icmp_ult(...)                                                          \
-  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::ult, __VA_ARGS__)
-#define icmp_ule(...)                                                          \
-  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::ule, __VA_ARGS__)
-#define icmp_ugt(...)                                                          \
-  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::ugt, __VA_ARGS__)
-#define icmp_uge(...)                                                          \
-  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::uge, __VA_ARGS__)
-#define select(...) rewriter.create<LLVM::SelectOp>(loc, __VA_ARGS__)
-#define address_of(...) rewriter.create<LLVM::AddressOfOp>(loc, __VA_ARGS__)
-#define barrier() rewriter.create<mlir::gpu::BarrierOp>(loc)
-#define undef(...) rewriter.create<LLVM::UndefOp>(loc, __VA_ARGS__)
-#define null(...) rewriter.create<LLVM::ZeroOp>(loc, __VA_ARGS__)
-#define call(...) LLVM::createLLVMCallOp(rewriter, loc, __VA_ARGS__)
+namespace mlir::triton {
+
+// Returns CTA level thread idx
+inline Value getThreadId(OpBuilder &rewriter, Location loc) {
+  Value tid =
+      rewriter.create<::mlir::gpu::ThreadIdOp>(loc, ::mlir::gpu::Dimension::x);
+  Type i32_ty = rewriter.getIntegerType(32);
+  return rewriter.create<arith::IndexCastOp>(loc, i32_ty, tid);
+}
+
+struct TritonLLVMOpBuilder {
+  TritonLLVMOpBuilder(const Location &loc, OpBuilder &builder)
+      : loc(loc), builder(&builder) {}
+  // Shortcuts for some commonly used LLVM ops to keep code simple and intuitive
+  // Operators
+  template <typename... Args> LLVM::SIToFPOp inttofloat(Args &&...args) {
+    return builder->create<LLVM::SIToFPOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::IntToPtrOp inttoptr(Args &&...args) {
+    return builder->create<LLVM::IntToPtrOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::PtrToIntOp ptrtoint(Args &&...args) {
+    return builder->create<LLVM::PtrToIntOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::ZExtOp zext(Args &&...args) {
+    return builder->create<LLVM::ZExtOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::SExtOp sext(Args &&...args) {
+    return builder->create<LLVM::SExtOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::FPExtOp fpext(Args &&...args) {
+    return builder->create<LLVM::FPExtOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::FPTruncOp fptrunc(Args &&...args) {
+    return builder->create<LLVM::FPTruncOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::TruncOp trunc(Args &&...args) {
+    return builder->create<LLVM::TruncOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::UDivOp udiv(Args &&...args) {
+    return builder->create<LLVM::UDivOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::SDivOp sdiv(Args &&...args) {
+    return builder->create<LLVM::SDivOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::URemOp urem(Args &&...args) {
+    return builder->create<LLVM::URemOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::AddOp add(Args &&...args) {
+    return builder->create<LLVM::AddOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::SubOp sub(Args &&...args) {
+    return builder->create<LLVM::SubOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::FAddOp fadd(Args &&...args) {
+    return builder->create<LLVM::FAddOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::MulOp mul(Args &&...args) {
+    return builder->create<LLVM::MulOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::FMulOp fmul(Args &&...args) {
+    return builder->create<LLVM::FMulOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::FMAOp fma(Args &&...args) {
+    return builder->create<LLVM::FMAOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::FNegOp neg(Args &&...args) {
+    return builder->create<LLVM::FNegOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::SMaxOp smax(Args &&...args) {
+    return builder->create<LLVM::SMaxOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::UMaxOp umax(Args &&...args) {
+    return builder->create<LLVM::UMaxOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::MaxNumOp fmax(Args &&...args) {
+    return builder->create<LLVM::MaxNumOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::SMinOp smin(Args &&...args) {
+    return builder->create<LLVM::SMinOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::UMinOp umin(Args &&...args) {
+    return builder->create<LLVM::UMinOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::MinNumOp fmin(Args &&...args) {
+    return builder->create<LLVM::MinNumOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::ShlOp shl(Args &&...args) {
+    return builder->create<LLVM::ShlOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::LShrOp lshr(Args &&...args) {
+    return builder->create<LLVM::LShrOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::AShrOp ashr(Args &&...args) {
+    return builder->create<LLVM::AShrOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::AndOp and_(Args &&...args) {
+    return builder->create<LLVM::AndOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::XOrOp xor_(Args &&...args) {
+    return builder->create<LLVM::XOrOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::OrOp or_(Args &&...args) {
+    return builder->create<LLVM::OrOp>(loc, std::forward<Args>(args)...);
+  }
+  LLVM::BitcastOp bitcast(Value val, Type type) {
+    return builder->create<LLVM::BitcastOp>(loc, type, val);
+  }
+  template <typename... Args>
+  LLVM::AddrSpaceCastOp addrspacecast(Args &&...args) {
+    return builder->create<LLVM::AddrSpaceCastOp>(loc,
+                                                  std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::GEPOp gep(Args &&...args) {
+    return builder->create<LLVM::GEPOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::InsertValueOp insert_val(Args &&...args) {
+    return builder->create<LLVM::InsertValueOp>(loc,
+                                                std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::ExtractValueOp extract_val(Args &&...args) {
+    return builder->create<LLVM::ExtractValueOp>(loc,
+                                                 std::forward<Args>(args)...);
+  }
+  template <typename... Args>
+  LLVM::InsertElementOp insert_element(Args &&...args) {
+    return builder->create<LLVM::InsertElementOp>(loc,
+                                                  std::forward<Args>(args)...);
+  }
+  template <typename... Args>
+  LLVM::ExtractElementOp extract_element(Args &&...args) {
+    return builder->create<LLVM::ExtractElementOp>(loc,
+                                                   std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::LoadOp load(Args &&...args) {
+    return builder->create<LLVM::LoadOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::StoreOp store(Args &&...args) {
+    return builder->create<LLVM::StoreOp>(loc, std::forward<Args>(args)...);
+  }
+  LLVM::FCmpOp fcmp_ogt(Value lhs, Value rhs) {
+    return builder->create<LLVM::FCmpOp>(loc, builder->getI1Type(),
+                                         LLVM::FCmpPredicate::ogt, lhs, rhs);
+  }
+  LLVM::FCmpOp fcmp_olt(Value lhs, Value rhs) {
+    return builder->create<LLVM::FCmpOp>(loc, builder->getI1Type(),
+                                         LLVM::FCmpPredicate::olt, lhs, rhs);
+  }
+  LLVM::FCmpOp fcmp_eq(Value lhs, Value rhs) {
+    return builder->create<LLVM::FCmpOp>(loc, builder->getI1Type(),
+                                         LLVM::FCmpPredicate::oeq, lhs, rhs);
+  }
+  template <typename... Args> LLVM::ICmpOp icmp_eq(Args &&...args) {
+    return builder->create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::eq,
+                                         std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::ICmpOp icmp_ne(Args &&...args) {
+    return builder->create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::ne,
+                                         std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::ICmpOp icmp_slt(Args &&...args) {
+    return builder->create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::slt,
+                                         std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::ICmpOp icmp_sle(Args &&...args) {
+    return builder->create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::sle,
+                                         std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::ICmpOp icmp_sgt(Args &&...args) {
+    return builder->create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::sgt,
+                                         std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::ICmpOp icmp_sge(Args &&...args) {
+    return builder->create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::sge,
+                                         std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::ICmpOp icmp_ult(Args &&...args) {
+    return builder->create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::ult,
+                                         std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::ICmpOp icmp_ule(Args &&...args) {
+    return builder->create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::ule,
+                                         std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::ICmpOp icmp_ugt(Args &&...args) {
+    return builder->create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::ugt,
+                                         std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::ICmpOp icmp_uge(Args &&...args) {
+    return builder->create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::uge,
+                                         std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::SelectOp select(Args &&...args) {
+    return builder->create<LLVM::SelectOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::AddressOfOp address_of(Args &&...args) {
+    return builder->create<LLVM::AddressOfOp>(loc, std::forward<Args>(args)...);
+  }
+  mlir::gpu::BarrierOp barrier() {
+    return builder->create<mlir::gpu::BarrierOp>(loc);
+  }
+  template <typename... Args> LLVM::UndefOp undef(Args &&...args) {
+    return builder->create<LLVM::UndefOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::ZeroOp null(Args &&...args) {
+    return builder->create<LLVM::ZeroOp>(loc, std::forward<Args>(args)...);
+  }
+  template <typename... Args> LLVM::CallOp call(Args &&...args) {
+    return builder->create<LLVM::CallOp>(loc, std::forward<Args>(args)...);
+  }
+  // Constants
+  Value int_val(short bitwidth, int64_t val) {
+    Type ty = builder->getIntegerType(bitwidth);
+    return builder->create<LLVM::ConstantOp>(loc, ty,
+                                             builder->getIntegerAttr(ty, val));
+  }
+  Value i1_val(int64_t val) { return int_val(1, val); }
+  Value true_val() { return int_val(1, true); }
+  Value false_val() { return int_val(1, false); }
+  Value f16_val(float v) {
+    auto type = type::f16Ty(builder->getContext());
+    return builder->create<LLVM::ConstantOp>(loc, type,
+                                             builder->getF16FloatAttr(v));
+  }
+  Value f32_val(float v) {
+    auto type = type::f32Ty(builder->getContext());
+    return builder->create<LLVM::ConstantOp>(loc, type,
+                                             builder->getF32FloatAttr(v));
+  }
+  Value f64_val(double v) {
+    auto type = type::f64Ty(builder->getContext());
+    return builder->create<LLVM::ConstantOp>(loc, type,
+                                             builder->getF64FloatAttr(v));
+  }
+  Value i8_val(int64_t val) { return int_val(8, val); }
+  Value i16_val(int64_t val) { return int_val(16, val); }
+  Value i32_val(int64_t val) { return int_val(32, val); }
+  Value i64_val(int64_t val) { return int_val(64, val); }
+  Value tid_val() { return getThreadId(*this->builder, loc); }
+
+  Location loc;
+  OpBuilder *builder;
+};
+} // namespace mlir::triton
 
 // Types
+#define ptr_ty(...) LLVM::LLVMPointerType::get(__VA_ARGS__)
 #define int_ty(width) rewriter.getIntegerType(width)
 #define i64_ty rewriter.getIntegerType(64)
 #define i32_ty rewriter.getIntegerType(32)
@@ -124,21 +293,6 @@ using namespace mlir::triton;
 #define struct_ty(...) LLVM::LLVMStructType::getLiteral(ctx, __VA_ARGS__)
 #define array_ty(elemTy, count) LLVM::LLVMArrayType::get(elemTy, count)
 
-// Constants
-#define int_val(bitwidth, val)                                                 \
-  LLVM::createLLVMIntegerConstant(rewriter, loc, bitwidth, val)
-#define i1_val(val) LLVM::createConstantI1(loc, rewriter, val)
-#define true_val() i1_val(true)
-#define false_val() i1_val(false)
-#define f16_val(...) LLVM::createConstantF16(loc, rewriter, __VA_ARGS__)
-#define f32_val(...) LLVM::createConstantF32(loc, rewriter, __VA_ARGS__)
-#define f64_val(...) LLVM::createConstantF64(loc, rewriter, __VA_ARGS__)
-#define i8_val(val) int_val(8, val)
-#define i16_val(val) int_val(16, val)
-#define i32_val(...) LLVM::createConstantI32(loc, rewriter, __VA_ARGS__)
-#define i64_val(...) LLVM::createConstantI64(loc, rewriter, __VA_ARGS__)
-#define tid_val() getThreadId(rewriter, loc)
-
 // Attributes
 #define i32_arr_attr(...) rewriter.getI32ArrayAttr({__VA_ARGS__})
 #define i64_arr_attr(...) rewriter.getI64ArrayAttr({__VA_ARGS__})
@@ -276,8 +430,9 @@ struct SharedMemoryObject {
                      ArrayRef<unsigned> order, Location loc,
                      RewriterBase &rewriter)
       : base(base), baseElemType(baseElemType) {
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     strides = getStridesFromShapeAndOrder(shape, order, loc, rewriter);
-    offsets.append(order.size(), i32_val(0));
+    offsets.append(order.size(), b.i32_val(0));
   }
 
   SmallVector<Value> getStrides() const { return strides; }
@@ -308,10 +463,11 @@ struct SharedMemoryObject {
 
   Value getBaseBeforeSlice(int order, Location loc,
                            RewriterBase &rewriter) const {
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     Value cSwizzleOffset = getCSwizzleOffset(order);
-    Value offset = sub(i32_val(0), cSwizzleOffset);
+    Value offset = b.sub(b.i32_val(0), cSwizzleOffset);
     Type type = base.getType();
-    return gep(type, baseElemType, base, offset);
+    return b.gep(type, baseElemType, base, offset);
   }
 };
 
@@ -387,20 +543,14 @@ inline Value getSharedMemoryBase(Location loc, RewriterBase &rewriter,
   size_t offset = cast<IntegerAttr>(op->getAttr("allocation.offset"))
                       .getValue()
                       .getZExtValue();
-  Value offVal = i32_val(offset);
-  Value base = gep(ptrTy, i8_ty, LLVM::getStackPointer(rewriter, func), offVal);
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  Value offVal = b.i32_val(offset);
+  Value base =
+      b.gep(ptrTy, i8_ty, LLVM::getStackPointer(rewriter, func), offVal);
   return base;
 }
 } // namespace LLVM
 
-/* ------------------------------------ */
-// Returns CTA level thread idx
-inline Value getThreadId(RewriterBase &rewriter, Location loc) {
-  Value tid =
-      rewriter.create<::mlir::gpu::ThreadIdOp>(loc, ::mlir::gpu::Dimension::x);
-  return rewriter.create<arith::IndexCastOp>(loc, i32_ty, tid);
-}
-
 // -----------------------------------------------------------------------
 // Shared memory utilities
 // -----------------------------------------------------------------------
@@ -419,9 +569,10 @@ using ::mlir::triton::gpu::SliceEncodingAttr;
 inline Value dot(RewriterBase &rewriter, Location loc, ArrayRef<Value> offsets,
                  ArrayRef<Value> strides) {
   assert(offsets.size() == strides.size());
-  Value ret = i32_val(0);
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  Value ret = b.i32_val(0);
   for (auto [offset, stride] : llvm::zip(offsets, strides)) {
-    ret = add(ret, mul(offset, stride));
+    ret = b.add(ret, b.mul(offset, stride));
   }
   return ret;
 }
@@ -447,9 +598,10 @@ emitBaseIndexWithinCTAForBlockedLayout(Location loc, RewriterBase &rewriter,
   MLIRContext *ctx = rewriter.getContext();
   auto shape = type.getShape();
   Value threadId = getThreadId(rewriter, loc);
-  Value warpSize = i32_val(triton::gpu::getWarpSize(blockedLayout));
-  Value laneId = urem(threadId, warpSize);
-  Value warpId = udiv(threadId, warpSize);
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  Value warpSize = b.i32_val(triton::gpu::getWarpSize(blockedLayout));
+  Value laneId = b.urem(threadId, warpSize);
+  Value warpId = b.udiv(threadId, warpSize);
   auto sizePerThread = blockedLayout.getSizePerThread();
   auto threadsPerWarp = blockedLayout.getThreadsPerWarp();
   auto warpsPerCTA = blockedLayout.getWarpsPerCTA();
@@ -470,16 +622,16 @@ emitBaseIndexWithinCTAForBlockedLayout(Location loc, RewriterBase &rewriter,
     auto maxWarps =
         ceil<unsigned>(shapePerCTA[k], sizePerThread[k] * threadsPerWarp[k]);
     auto maxThreads = ceil<unsigned>(shapePerCTA[k], sizePerThread[k]);
-    multiDimWarpId[k] = urem(multiDimWarpId[k], i32_val(maxWarps));
-    multiDimThreadId[k] = urem(multiDimThreadId[k], i32_val(maxThreads));
+    multiDimWarpId[k] = b.urem(multiDimWarpId[k], b.i32_val(maxWarps));
+    multiDimThreadId[k] = b.urem(multiDimThreadId[k], b.i32_val(maxThreads));
     // multiDimBase[k] = (multiDimThreadId[k] +
     //                    multiDimWarpId[k] * threadsPerWarp[k]) *
     //                   sizePerThread[k];
-    Value threadsPerWarpK = i32_val(threadsPerWarp[k]);
-    Value sizePerThreadK = i32_val(sizePerThread[k]);
+    Value threadsPerWarpK = b.i32_val(threadsPerWarp[k]);
+    Value sizePerThreadK = b.i32_val(sizePerThread[k]);
     multiDimBase[k] =
-        mul(sizePerThreadK,
-            add(multiDimThreadId[k], mul(multiDimWarpId[k], threadsPerWarpK)));
+        b.mul(sizePerThreadK, b.add(multiDimThreadId[k],
+                                    b.mul(multiDimWarpId[k], threadsPerWarpK)));
   }
 
   return multiDimBase;
@@ -534,6 +686,7 @@ inline SmallVector<Value>
 emitBaseIndexWithinCTAForMmaLayoutV1(Location loc, RewriterBase &rewriter,
                                      const NvidiaMmaEncodingAttr &mmaLayout,
                                      RankedTensorType type) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto shape = type.getShape();
   auto wpt = mmaLayout.getWarpsPerCTA();
   static constexpr std::array<int, 3> fpw{{2, 2, 1}};
@@ -542,13 +695,13 @@ emitBaseIndexWithinCTAForMmaLayoutV1(Location loc, RewriterBase &rewriter,
 
   Value thread = getThreadId(rewriter, loc);
   auto *ctx = thread.getContext();
-  Value _1 = i32_val(1);
-  Value _2 = i32_val(2);
-  Value _4 = i32_val(4);
-  Value _16 = i32_val(16);
-  Value _32 = i32_val(32);
-  Value _fpw0 = i32_val(fpw[0]);
-  Value _fpw1 = i32_val(fpw[1]);
+  Value _1 = b.i32_val(1);
+  Value _2 = b.i32_val(2);
+  Value _4 = b.i32_val(4);
+  Value _16 = b.i32_val(16);
+  Value _32 = b.i32_val(32);
+  Value _fpw0 = b.i32_val(fpw[0]);
+  Value _fpw1 = b.i32_val(fpw[1]);
 
   // A info
   auto aRep = mmaLayout.getMMAv1Rep(0);
@@ -561,41 +714,41 @@ emitBaseIndexWithinCTAForMmaLayoutV1(Location loc, RewriterBase &rewriter,
   SmallVector<int, 2> spw({aSpw[0], bSpw[1]});
   SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});
 
-  Value lane = urem(thread, _32);
-  Value warp = udiv(thread, _32);
+  Value lane = b.urem(thread, _32);
+  Value warp = b.udiv(thread, _32);
 
-  Value warp0 = urem(warp, i32_val(wpt[0]));
-  Value warp12 = udiv(warp, i32_val(wpt[0]));
-  Value warp1 = urem(warp12, i32_val(wpt[1]));
+  Value warp0 = b.urem(warp, b.i32_val(wpt[0]));
+  Value warp12 = b.udiv(warp, b.i32_val(wpt[0]));
+  Value warp1 = b.urem(warp12, b.i32_val(wpt[1]));
 
   // warp offset
-  Value offWarpM = mul(warp0, i32_val(spw[0]));
-  Value offWarpN = mul(warp1, i32_val(spw[1]));
+  Value offWarpM = b.mul(warp0, b.i32_val(spw[0]));
+  Value offWarpN = b.mul(warp1, b.i32_val(spw[1]));
   // quad offset
-  Value offQuadM = mul(udiv(and_(lane, _16), _4), _fpw0);
-  Value offQuadN = mul(udiv(and_(lane, _16), _4), _fpw1);
+  Value offQuadM = b.mul(b.udiv(b.and_(lane, _16), _4), _fpw0);
+  Value offQuadN = b.mul(b.udiv(b.and_(lane, _16), _4), _fpw1);
   // pair offset
-  Value offPairM = udiv(urem(lane, _16), _4);
-  offPairM = urem(offPairM, _fpw0);
-  offPairM = mul(offPairM, _4);
-  Value offPairN = udiv(urem(lane, _16), _4);
-  offPairN = udiv(offPairN, _fpw0);
-  offPairN = urem(offPairN, _fpw1);
-  offPairN = mul(offPairN, _4);
-  offPairM = mul(offPairM, i32_val(rep[0] / 2));
-  offQuadM = mul(offQuadM, i32_val(rep[0] / 2));
-  offPairN = mul(offPairN, i32_val(rep[1] / 2));
-  offQuadN = mul(offQuadN, i32_val(rep[1] / 2));
+  Value offPairM = b.udiv(b.urem(lane, _16), _4);
+  offPairM = b.urem(offPairM, _fpw0);
+  offPairM = b.mul(offPairM, _4);
+  Value offPairN = b.udiv(b.urem(lane, _16), _4);
+  offPairN = b.udiv(offPairN, _fpw0);
+  offPairN = b.urem(offPairN, _fpw1);
+  offPairN = b.mul(offPairN, _4);
+  offPairM = b.mul(offPairM, b.i32_val(rep[0] / 2));
+  offQuadM = b.mul(offQuadM, b.i32_val(rep[0] / 2));
+  offPairN = b.mul(offPairN, b.i32_val(rep[1] / 2));
+  offQuadN = b.mul(offQuadN, b.i32_val(rep[1] / 2));
   // quad pair offset
-  Value offLaneM = add(offPairM, offQuadM);
-  Value offLaneN = add(offPairN, offQuadN);
+  Value offLaneM = b.add(offPairM, offQuadM);
+  Value offLaneN = b.add(offPairN, offQuadN);
   // a, b offset
-  Value offsetAM = add(offWarpM, offLaneM);
-  Value offsetBN = add(offWarpN, offLaneN);
+  Value offsetAM = b.add(offWarpM, offLaneM);
+  Value offsetBN = b.add(offWarpN, offLaneN);
   // m indices
-  Value offsetCM = add(and_(lane, _1), offsetAM);
+  Value offsetCM = b.add(b.and_(lane, _1), offsetAM);
   // n indices
-  Value offsetCN = add((and_(lane, _2)), (add(offWarpN, offPairN)));
+  Value offsetCN = b.add((b.and_(lane, _2)), (b.add(offWarpN, offPairN)));
   return {offsetCM, offsetCN};
 }
 
@@ -687,14 +840,15 @@ emitBaseIndexWithinCTAForMmaLayoutV2V3(Location loc, RewriterBase &rewriter,
   auto warpOrder = triton::gpu::getWarpOrder(mmaLayout);
   ArrayRef<unsigned int> instrShape = mmaLayout.getInstrShape();
   SmallVector<Value> warpsPerCTA;
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   for (unsigned i = 0; i < rank; ++i)
-    warpsPerCTA.push_back(i32_val(_warpsPerCTA[i]));
+    warpsPerCTA.push_back(b.i32_val(_warpsPerCTA[i]));
   auto shapePerCTA = getShapePerCTA(mmaLayout, shape);
 
   Value threadId = getThreadId(rewriter, loc);
-  Value warpSize = i32_val(32);
-  Value laneId = urem(threadId, warpSize);
-  Value warpId = udiv(threadId, warpSize);
+  Value warpSize = b.i32_val(32);
+  Value laneId = b.urem(threadId, warpSize);
+  Value warpId = b.udiv(threadId, warpSize);
 
   uint32_t repM =
       (_warpsPerCTA[rank - 2] * instrShape[rank - 2]) / shapePerCTA[rank - 2];
@@ -715,11 +869,11 @@ emitBaseIndexWithinCTAForMmaLayoutV2V3(Location loc, RewriterBase &rewriter,
 
   SmallVector<Value> multiDimWarpId(rank);
   multiDimWarpId = delinearize(rewriter, loc, warpId, _warpsPerCTA, warpOrder);
-  Value warpIdM = urem(multiDimWarpId[rank - 2], i32_val(warpsM));
-  Value warpIdN = urem(multiDimWarpId[rank - 1], i32_val(warpsN));
+  Value warpIdM = b.urem(multiDimWarpId[rank - 2], b.i32_val(warpsM));
+  Value warpIdN = b.urem(multiDimWarpId[rank - 1], b.i32_val(warpsN));
 
-  Value offWarpM = mul(warpIdM, i32_val(instrShape[rank - 2]));
-  Value offWarpN = mul(warpIdN, i32_val(instrShape[rank - 1]));
+  Value offWarpM = b.mul(warpIdM, b.i32_val(instrShape[rank - 2]));
+  Value offWarpN = b.mul(warpIdN, b.i32_val(instrShape[rank - 1]));
 
   SmallVector<Value> multiDimBase(rank);
   if (rank == 3)
@@ -731,10 +885,10 @@ emitBaseIndexWithinCTAForMmaLayoutV2V3(Location loc, RewriterBase &rewriter,
   // we rely on the caller to check.  Worst case we crash, which is better than
   // silently producing bad code.
   if (warpsM != 0)
-    multiDimBase[rank - 2] = add(udiv(laneId, i32_val(4)), offWarpM);
+    multiDimBase[rank - 2] = b.add(b.udiv(laneId, b.i32_val(4)), offWarpM);
   if (warpsN != 0)
     multiDimBase[rank - 1] =
-        add(mul(i32_val(2), urem(laneId, i32_val(4))), offWarpN);
+        b.add(b.mul(b.i32_val(2), b.urem(laneId, b.i32_val(4))), offWarpN);
 
   return multiDimBase;
 }
@@ -771,55 +925,56 @@ emitBaseIndexForMfmaLayout(Location loc, RewriterBase &rewriter,
   assert(rank == 2 || rank == 3);
   auto _warpsPerCTA = mfmaLayout.getWarpsPerCTA();
   SmallVector<Value> warpsPerCTA;
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   for (unsigned i = 0; i < rank; ++i)
-    warpsPerCTA.push_back(i32_val(_warpsPerCTA[i]));
+    warpsPerCTA.push_back(b.i32_val(_warpsPerCTA[i]));
   unsigned mDim = mfmaLayout.getMDim();
   unsigned nDim = mfmaLayout.getNDim();
   assert((mDim == nDim && (mDim == 32 || mDim == 16 || mDim == 4)) ||
          (mDim == 64 && nDim == 4) || (mDim == 4 && nDim == 64));
 
   Value threadId = getThreadId(rewriter, loc);
-  Value warpSize = i32_val(triton::gpu::getWarpSize(mfmaLayout));
+  Value warpSize = b.i32_val(triton::gpu::getWarpSize(mfmaLayout));
   Value effectiveWarpSize = warpSize;
   if (mDim == 4 && nDim == 4) {
     const int uniqueValuesPerWarp = 4;
-    effectiveWarpSize = i32_val(uniqueValuesPerWarp);
+    effectiveWarpSize = b.i32_val(uniqueValuesPerWarp);
   }
-  Value laneId = urem(threadId, effectiveWarpSize);
-  Value warpId = udiv(threadId, warpSize);
+  Value laneId = b.urem(threadId, effectiveWarpSize);
+  Value warpId = b.udiv(threadId, warpSize);
   SmallVector<Value> multiDimWarpId =
       delinearize(rewriter, loc, warpId, _warpsPerCTA,
                   triton::gpu::getWarpOrder(mfmaLayout));
   if (shape[rank - 2] >= mDim) {
     assert(shape[rank - 2] % mDim == 0);
     multiDimWarpId[rank - 2] =
-        urem(multiDimWarpId[rank - 2],
-             i32_val(ceil<unsigned>(shape[rank - 2], mDim)));
+        b.urem(multiDimWarpId[rank - 2],
+               b.i32_val(ceil<unsigned>(shape[rank - 2], mDim)));
   }
   if (shape[rank - 1] >= nDim) {
     assert(shape[rank - 1] % nDim == 0);
     multiDimWarpId[rank - 1] =
-        urem(multiDimWarpId[rank - 1],
-             i32_val(ceil<unsigned>(shape[rank - 1], nDim)));
+        b.urem(multiDimWarpId[rank - 1],
+               b.i32_val(ceil<unsigned>(shape[rank - 1], nDim)));
   }
-  Value offWarp0 = mul(multiDimWarpId[rank - 2], i32_val(mDim));
-  Value offWarp1 = mul(multiDimWarpId[rank - 1], i32_val(nDim));
+  Value offWarp0 = b.mul(multiDimWarpId[rank - 2], b.i32_val(mDim));
+  Value offWarp1 = b.mul(multiDimWarpId[rank - 1], b.i32_val(nDim));
 
   SmallVector<Value> multiDimBase(rank);
   if (mfmaLayout.getIsTransposed()) {
     multiDimBase[rank - 1] =
-        add(mul(i32_val(4), udiv(laneId, i32_val(mDim))), offWarp1);
-    multiDimBase[rank - 2] = add(urem(laneId, i32_val(mDim)), offWarp0);
+        b.add(b.mul(b.i32_val(4), b.udiv(laneId, b.i32_val(mDim))), offWarp1);
+    multiDimBase[rank - 2] = b.add(b.urem(laneId, b.i32_val(mDim)), offWarp0);
   } else {
     multiDimBase[rank - 2] =
-        add(mul(i32_val(4), udiv(laneId, i32_val(nDim))), offWarp0);
-    multiDimBase[rank - 1] = add(urem(laneId, i32_val(nDim)), offWarp1);
+        b.add(b.mul(b.i32_val(4), b.udiv(laneId, b.i32_val(nDim))), offWarp0);
+    multiDimBase[rank - 1] = b.add(b.urem(laneId, b.i32_val(nDim)), offWarp1);
   }
   // TODO(Lixun): It is assumed when rank = 3, warpsPerCTA is set to
   // {numWarps, 1, 1}. We need to generalize the offset computation.
   if (rank == 3) {
     assert(_warpsPerCTA[1] == 1 && _warpsPerCTA[2] == 1);
-    multiDimBase[0] = urem(warpId, i32_val(shape[0]));
+    multiDimBase[0] = b.urem(warpId, b.i32_val(shape[0]));
   }
   return multiDimBase;
 }
@@ -929,55 +1084,56 @@ emitBaseIndexForWmmaLayout(Location loc, RewriterBase &rewriter,
   auto rank = _warpsPerCTA.size();
   assert(rank == 2 || rank == 3);
   SmallVector<Value> warpsPerCTA;
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   for (unsigned i = 0; i < rank; ++i)
-    warpsPerCTA.push_back(i32_val(_warpsPerCTA[i]));
+    warpsPerCTA.push_back(b.i32_val(_warpsPerCTA[i]));
   auto mnkDim = AMDWmmaEncodingAttr::getMNKDimPerInstr();
 
   Value threadId = getThreadId(rewriter, loc);
-  Value warpSize = i32_val(triton::gpu::getWarpSize(wmmaLayout));
+  Value warpSize = b.i32_val(triton::gpu::getWarpSize(wmmaLayout));
   Value laneId =
-      urem(threadId, i32_val(triton::gpu::getWarpSize(wmmaLayout) / 2));
-  Value threadIdPerWarp = urem(threadId, warpSize);
+      b.urem(threadId, b.i32_val(triton::gpu::getWarpSize(wmmaLayout) / 2));
+  Value threadIdPerWarp = b.urem(threadId, warpSize);
 
-  Value warpId = udiv(threadId, warpSize);
+  Value warpId = b.udiv(threadId, warpSize);
   SmallVector<Value> multiDimWarpId =
       delinearize(rewriter, loc, warpId, _warpsPerCTA,
                   triton::gpu::getWarpOrder(wmmaLayout));
   if (shape[rank - 2] >= mnkDim[0]) {
     assert(shape[rank - 2] % mnkDim[0] == 0);
     multiDimWarpId[rank - 2] =
-        urem(multiDimWarpId[rank - 2],
-             i32_val(ceil<unsigned>(shape[rank - 2], mnkDim[0])));
+        b.urem(multiDimWarpId[rank - 2],
+               b.i32_val(ceil<unsigned>(shape[rank - 2], mnkDim[0])));
   }
   if (shape[rank - 1] >= mnkDim[1]) {
     assert(shape[rank - 1] % mnkDim[1] == 0);
     multiDimWarpId[rank - 1] =
-        urem(multiDimWarpId[rank - 1],
-             i32_val(ceil<unsigned>(shape[rank - 1], mnkDim[1])));
+        b.urem(multiDimWarpId[rank - 1],
+               b.i32_val(ceil<unsigned>(shape[rank - 1], mnkDim[1])));
   }
-  Value offWarp0 = mul(multiDimWarpId[rank - 2], i32_val(mnkDim[0]));
-  Value offWarp1 = mul(multiDimWarpId[rank - 1], i32_val(mnkDim[1]));
+  Value offWarp0 = b.mul(multiDimWarpId[rank - 2], b.i32_val(mnkDim[0]));
+  Value offWarp1 = b.mul(multiDimWarpId[rank - 1], b.i32_val(mnkDim[1]));
 
   SmallVector<Value> multiDimBase(rank);
 
   auto ver = wmmaLayout.getVersion();
   if (ver == 1) {
     multiDimBase[rank - 2] =
-        add(udiv(threadIdPerWarp, i32_val(mnkDim[2])), offWarp0);
+        b.add(b.udiv(threadIdPerWarp, b.i32_val(mnkDim[2])), offWarp0);
   } else {
     assert(ver == 2);
     multiDimBase[rank - 2] =
-        add(mul(udiv(threadIdPerWarp, i32_val(mnkDim[2])),
-                i32_val(wmmaLayout.getSizePerThread()[rank - 2])),
-            offWarp0);
+        b.add(b.mul(b.udiv(threadIdPerWarp, b.i32_val(mnkDim[2])),
+                    b.i32_val(wmmaLayout.getSizePerThread()[rank - 2])),
+              offWarp0);
   }
-  multiDimBase[rank - 1] = add(laneId, offWarp1);
+  multiDimBase[rank - 1] = b.add(laneId, offWarp1);
 
   // TODO: It is assumed when rank = 3, warpsPerCTA is set to
   // {numWarps, 1, 1}. We need to generalize the offset computation.
   if (rank == 3) {
     assert(_warpsPerCTA[1] == 1 && _warpsPerCTA[2] == 1);
-    multiDimBase[0] = urem(warpId, i32_val(shape[0]));
+    multiDimBase[0] = b.urem(warpId, b.i32_val(shape[0]));
   }
   return multiDimBase;
 }
@@ -1084,16 +1240,18 @@ inline SmallVector<Value> emitCTAOffsetForLayout(Location loc,
   SmallVector<Value> multiDimClusterCTAId =
       delinearize(rewriter, loc, clusterCTAId, CTAsPerCGA, CTAOrder);
 
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   // CTA Wrapping
   for (unsigned i = 0; i < rank; ++i) {
     // This wrapping rule must be consistent with getShapePerCTA
     unsigned splitNum = std::min<unsigned>(shape[i], CTASplitNum[i]);
-    multiDimClusterCTAId[i] = urem(multiDimClusterCTAId[i], i32_val(splitNum));
+    multiDimClusterCTAId[i] =
+        b.urem(multiDimClusterCTAId[i], b.i32_val(splitNum));
   }
 
   SmallVector<Value> CTAOffset(rank);
   for (unsigned i = 0; i < rank; ++i)
-    CTAOffset[i] = mul(multiDimClusterCTAId[i], i32_val(shapePerCTA[i]));
+    CTAOffset[i] = b.mul(multiDimClusterCTAId[i], b.i32_val(shapePerCTA[i]));
 
   return CTAOffset;
 }
@@ -1134,6 +1292,7 @@ emitBaseIndexForLayoutImpl(Location loc, RewriterBase &rewriter,
   } else {
     llvm_unreachable("unsupported emitBaseIndexForLayout");
   }
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   if (withCTAOffset) {
     auto CTAOffset =
         emitCTAOffsetForLayout(loc, rewriter, target, layout, shape);
@@ -1144,7 +1303,7 @@ emitBaseIndexForLayoutImpl(Location loc, RewriterBase &rewriter,
       // off.
       if (!result[k])
         continue;
-      result[k] = add(result[k], CTAOffset[k]);
+      result[k] = b.add(result[k], CTAOffset[k]);
     }
   }
   return result;
@@ -1229,9 +1388,10 @@ inline DenseMap<unsigned, Value> getSwizzledSharedPtrs(
   // then (x + y) XOR z = 0byyyyxxxx XOR 0b00000zzzz = (x XOR z) + y
   // This means that we can use some immediate offsets for shared memory
   // operations.
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto dstPtrTy = smemObj.base.getType();
   auto dstOffset = dot(rewriter, loc, offsetVals, smemObj.strides);
-  Value dstPtrBase = gep(dstPtrTy, resElemTy, smemObj.base, dstOffset);
+  Value dstPtrBase = b.gep(dstPtrTy, resElemTy, smemObj.base, dstOffset);
 
   auto srcEncoding = srcTy.getEncoding();
   auto srcShape = srcTy.getShape();
@@ -1264,7 +1424,7 @@ inline DenseMap<unsigned, Value> getSwizzledSharedPtrs(
   }
   unsigned numElemsPerSwizzlingRow =
       swizzlingByteWidth * 8 / resElemTy.getIntOrFloatBitWidth();
-  Value numElemsPerSwizzlingRowVal = i32_val(numElemsPerSwizzlingRow);
+  Value numElemsPerSwizzlingRowVal = b.i32_val(numElemsPerSwizzlingRow);
   unsigned leadingDimOffset;
   if (outOrder.size() >= 2) {
     leadingDimOffset = numElemsPerSwizzlingRow * srcShapePerCTA[outOrder[1]];
@@ -1272,20 +1432,20 @@ inline DenseMap<unsigned, Value> getSwizzledSharedPtrs(
     leadingDimOffset = numElemsPerSwizzlingRow;
   }
 
-  Value leadingDimOffsetVal = i32_val(leadingDimOffset);
+  Value leadingDimOffsetVal = b.i32_val(leadingDimOffset);
   // Return values
   DenseMap<unsigned, Value> ret;
   // cache for non-immediate offsets
   DenseMap<unsigned, Value> cacheCol, cacheRow;
   unsigned minVec = std::min(outVec, inVec);
-  Value strideRow = outOrder.size() >= 2 ? srcStrides[outOrder[1]] : i32_val(0);
+  Value strideRow = outOrder.size() >= 2 ? srcStrides[outOrder[1]] : b.i32_val(0);
   Value strideCol = srcStrides[outOrder[0]];
   LDBG("getSwizzledSharedPtrs: perPhase = "
        << perPhase << " maxPhase = " << maxPhase << " minVec = " << minVec
        << " inVec = " << inVec << " outVec = " << outVec << " strideRow "
        << strideRow << " strideCol " << strideCol);
   for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {
-    Value offset = i32_val(0);
+    Value offset = b.i32_val(0);
     // Extract multi dimensional index for current element
     auto idx = srcIndices[elemIdx];
     Value idxCol = idx[outOrder[0]]; // contiguous dimension
@@ -1293,19 +1453,19 @@ inline DenseMap<unsigned, Value> getSwizzledSharedPtrs(
     if (outOrder.size() >= 2) {
       idxRow = idx[outOrder[1]]; // discontiguous dimension
     } else {
-      idxRow = i32_val(0);
+      idxRow = b.i32_val(0);
     }
     // compute phase = (row // perPhase) % maxPhase
-    Value phase = urem(udiv(idxRow, i32_val(perPhase)), i32_val(maxPhase));
+    Value phase = b.urem(b.udiv(idxRow, b.i32_val(perPhase)), b.i32_val(maxPhase));
     // extract dynamic/static offset for immediate offsetting
     unsigned immedateOffCol = 0;
     unsigned immedateOffRow = 0;
     if (leadingDimOffset) {
       // hopper
       offset =
-          mul(udiv(idxCol, numElemsPerSwizzlingRowVal), leadingDimOffsetVal);
+          b.mul(b.udiv(idxCol, numElemsPerSwizzlingRowVal), leadingDimOffsetVal);
       // Shrink by swizzling blocks
-      idxCol = urem(idxCol, numElemsPerSwizzlingRowVal);
+      idxCol = b.urem(idxCol, numElemsPerSwizzlingRowVal);
       strideRow = numElemsPerSwizzlingRowVal;
     }
     if (auto add = idxCol.getDefiningOp<LLVM::AddOp>()) {
@@ -1329,33 +1489,33 @@ inline DenseMap<unsigned, Value> getSwizzledSharedPtrs(
       }
     }
     // row offset is simply row index
-    Value rowOff = mul(idxRow, strideRow);
+    Value rowOff = b.mul(idxRow, strideRow);
     // because swizzling happens at a granularity of outVec, we need to
     // decompose the offset into a swizzled factor and a non-swizzled
     // (ordered) factor: colOffSwizzled = ((col // outVec) ^ phase) * outVec
     // colOffOrdered = (col % outVec) // minVec * minVec
-    Value colOffSwizzled = xor_(udiv(idxCol, i32_val(outVec)), phase);
-    colOffSwizzled = mul(colOffSwizzled, i32_val(outVec));
-    Value colOffOrdered = urem(idxCol, i32_val(outVec));
-    colOffOrdered = udiv(colOffOrdered, i32_val(minVec));
-    colOffOrdered = mul(colOffOrdered, i32_val(minVec));
-    Value colOff = add(colOffSwizzled, colOffOrdered);
+    Value colOffSwizzled = b.xor_(b.udiv(idxCol, b.i32_val(outVec)), phase);
+    colOffSwizzled = b.mul(colOffSwizzled, b.i32_val(outVec));
+    Value colOffOrdered = b.urem(idxCol, b.i32_val(outVec));
+    colOffOrdered = b.udiv(colOffOrdered, b.i32_val(minVec));
+    colOffOrdered = b.mul(colOffOrdered, b.i32_val(minVec));
+    Value colOff = b.add(colOffSwizzled, colOffOrdered);
 
     // compute non-immediate offset
     if (outOrder.size() == 3)
-      offset = add(offset, mul(idx[outOrder[2]], srcStrides[outOrder[2]]));
-    offset = add(offset, add(rowOff, mul(colOff, strideCol)));
-    Value currPtr = gep(dstPtrTy, resElemTy, dstPtrBase, offset);
+      offset = b.add(offset, b.mul(idx[outOrder[2]], srcStrides[outOrder[2]]));
+    offset = b.add(offset, b.add(rowOff, b.mul(colOff, strideCol)));
+    Value currPtr = b.gep(dstPtrTy, resElemTy, dstPtrBase, offset);
     // compute immediate offset
     Value immediateOff;
     if (outOrder.size() >= 2) {
       immediateOff =
-          add(mul(i32_val(immedateOffRow), strideRow), i32_val(immedateOffCol));
+          b.add(b.mul(b.i32_val(immedateOffRow), strideRow), b.i32_val(immedateOffCol));
     } else {
-      immediateOff = i32_val(immedateOffCol);
+      immediateOff = b.i32_val(immedateOffCol);
     }
 
-    ret[elemIdx] = gep(dstPtrTy, resElemTy, currPtr, immediateOff);
+    ret[elemIdx] = b.gep(dstPtrTy, resElemTy, currPtr, immediateOff);
   }
   return ret;
 }
@@ -1375,6 +1535,7 @@ void storeDistributedToShared(MemDescType dstTy, RankedTensorType srcTy,
 inline Value getStructFromSharedMemoryObject(Location loc,
                                              const SharedMemoryObject &smemObj,
                                              RewriterBase &rewriter) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto elems = smemObj.getElems();
   auto types = smemObj.getTypes();
   auto structTy =
@@ -1383,7 +1544,7 @@ inline Value getStructFromSharedMemoryObject(Location loc,
   Value llvmStruct = rewriter.create<LLVM::UndefOp>(loc, structTy);
   for (const auto &v : llvm::enumerate(elems)) {
     assert(v.value() && "can not insert null values");
-    llvmStruct = insert_val(structTy, llvmStruct, v.value(), v.index());
+    llvmStruct = b.insert_val(structTy, llvmStruct, v.value(), v.index());
   }
   return llvmStruct;
 }
@@ -1398,9 +1559,10 @@ inline SmallVector<Value> unpackLLElements(Location loc, Value llvmStruct,
   ArrayRef<Type> types =
       cast<LLVM::LLVMStructType>(llvmStruct.getType()).getBody();
   SmallVector<Value> results(types.size());
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   for (unsigned i = 0; i < types.size(); ++i) {
     Type type = types[i];
-    results[i] = extract_val(type, llvmStruct, i);
+    results[i] = b.extract_val(type, llvmStruct, i);
   }
   return results;
 }
@@ -1423,6 +1585,7 @@ inline Value packLLElements(Location loc,
                    << resultVals.size();
   }
   Value llvmStruct = rewriter.create<LLVM::UndefOp>(loc, structType);
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   for (const auto &v : llvm::enumerate(resultVals)) {
     if (!v.value()) {
       emitError(loc)
@@ -1436,7 +1599,7 @@ inline Value packLLElements(Location loc,
                      << elementTypes[v.index()] << " but got "
                      << v.value().getType();
     }
-    llvmStruct = insert_val(structType, llvmStruct, v.value(), v.index());
+    llvmStruct = b.insert_val(structType, llvmStruct, v.value(), v.index());
   }
   return llvmStruct;
 }
@@ -1449,10 +1612,11 @@ inline SmallVector<Value> unpackLLVector(Location loc, Value llvmVec,
       isa<LLVM::LLVMPointerType>(llvmVec.getType()))
     return {llvmVec};
 
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   SmallVector<Value> results;
   for (int i = 0; i < cast<VectorType>(llvmVec.getType()).getNumElements();
        i++) {
-    results.push_back(extract_element(llvmVec, i32_val(i)));
+    results.push_back(b.extract_element(llvmVec, b.i32_val(i)));
   }
   return results;
 }
@@ -1461,9 +1625,10 @@ inline Value packLLVector(Location loc, ValueRange vals,
                           RewriterBase &rewriter) {
   assert(vals.size() > 0);
   auto vecType = vec_ty(vals[0].getType(), vals.size());
-  Value vec = undef(vecType);
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  Value vec = b.undef(vecType);
   for (int i = 0; i < vals.size(); i++) {
-    vec = insert_element(vec, vals[i], i32_val(i));
+    vec = b.insert_element(vec, vals[i], b.i32_val(i));
   }
   return vec;
 }
diff --git a/lib/Conversion/TritonGPUToLLVM/AssertOpToLLVM.cpp b/lib/Conversion/TritonGPUToLLVM/AssertOpToLLVM.cpp
index 508eb25f9..12f31d56d 100644
--- a/lib/Conversion/TritonGPUToLLVM/AssertOpToLLVM.cpp
+++ b/lib/Conversion/TritonGPUToLLVM/AssertOpToLLVM.cpp
@@ -18,17 +18,18 @@ struct AssertOpConversion : public ConvertOpToLLVMPattern<triton::AssertOp> {
   matchAndRewrite(triton::AssertOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
     auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto ctx = rewriter.getContext();
     auto typeConverter = getTypeConverter();
     auto elems = unpackLLElements(loc, adaptor.getCondition(), rewriter);
     auto elemTy = elems[0].getType();
-    Value condition = int_val(elemTy.getIntOrFloatBitWidth(), 0);
+    Value condition = b.int_val(elemTy.getIntOrFloatBitWidth(), 0);
     for (auto elem : elems) {
       if (elemTy.isSignedInteger() || elemTy.isSignlessInteger()) {
-        condition =
-            or_(condition,
-                icmp_eq(elem, rewriter.create<LLVM::ConstantOp>(
-                                  loc, elemTy, rewriter.getZeroAttr(elemTy))));
+        condition = b.or_(
+            condition,
+            b.icmp_eq(elem, rewriter.create<LLVM::ConstantOp>(
+                                loc, elemTy, rewriter.getZeroAttr(elemTy))));
       } else {
         assert(false && "Unsupported type for assert");
         return failure();
@@ -41,7 +42,7 @@ struct AssertOpConversion : public ConvertOpToLLVMPattern<triton::AssertOp> {
       // tensor in those two operations may have different layout we need to
       // make sure all the threads are done executing the assert before going to
       // the next op.
-      barrier();
+      b.barrier();
     }
     rewriter.eraseOp(op);
     return success();
diff --git a/lib/Conversion/TritonGPUToLLVM/ControlFlowOpToLLVM.cpp b/lib/Conversion/TritonGPUToLLVM/ControlFlowOpToLLVM.cpp
index 8d5a63eb1..1e6199363 100644
--- a/lib/Conversion/TritonGPUToLLVM/ControlFlowOpToLLVM.cpp
+++ b/lib/Conversion/TritonGPUToLLVM/ControlFlowOpToLLVM.cpp
@@ -13,6 +13,8 @@ struct ReturnOpConversion : public ConvertOpToLLVMPattern<triton::ReturnOp> {
   matchAndRewrite(triton::ReturnOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
     auto funcOp = op->getParentOfType<LLVM::LLVMFuncOp>();
+    auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     if (funcOp->hasAttr("nvvm.kernel")) {
       // A GPU kernel
       if (op.getNumOperands() > 0) {
@@ -34,10 +36,9 @@ struct ReturnOpConversion : public ConvertOpToLLVMPattern<triton::ReturnOp> {
             funcOp.getResultTypes());
         Value packedResults =
             rewriter.create<LLVM::UndefOp>(op.getLoc(), packedResultsTy);
-        auto loc = op.getLoc();
         for (auto it : llvm::enumerate(adaptor.getOperands())) {
-          packedResults = insert_val(packedResultsTy, packedResults, it.value(),
-                                     it.index());
+          packedResults = b.insert_val(packedResultsTy, packedResults,
+                                       it.value(), it.index());
         }
         newOp = rewriter.create<LLVM::ReturnOp>(op.getLoc(), packedResults);
       }
diff --git a/lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp b/lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp
index a18b2cbc3..f151eff03 100644
--- a/lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp
+++ b/lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp
@@ -69,6 +69,7 @@ private:
                       ArrayRef<unsigned> origRepShape,
                       ArrayRef<unsigned> outOrd, SmallVector<Value> &vals,
                       Value smemBase) const {
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);
     auto layout = type.getEncoding();
     auto rank = type.getRank();
@@ -116,29 +117,29 @@ private:
         Value offset = linearize(rewriter, loc, multiDimOffsetWrapped,
                                  paddedRepShape, outOrd);
         auto elemPtrTy = smemBase.getType();
-        Value ptr = gep(elemPtrTy, llvmElemTy, smemBase, offset);
+        Value ptr = b.gep(elemPtrTy, llvmElemTy, smemBase, offset);
         auto vecTy = vec_ty(llvmElemTy, vec);
         if (stNotRd) {
-          Value valVec = undef(vecTy);
+          Value valVec = b.undef(vecTy);
           for (unsigned v = 0; v < vec; ++v) {
             auto currVal = vals[elemId + linearCTAId * accumSizePerThread + v];
             if (isInt1)
-              currVal = zext(llvmElemTy, currVal);
+              currVal = b.zext(llvmElemTy, currVal);
             else if (isPtr)
-              currVal = ptrtoint(llvmElemTy, currVal);
-            valVec = insert_element(vecTy, valVec, currVal, i32_val(v));
+              currVal = b.ptrtoint(llvmElemTy, currVal);
+            valVec = b.insert_element(vecTy, valVec, currVal, b.i32_val(v));
           }
-          store(valVec, ptr);
+          b.store(valVec, ptr);
         } else {
-          Value valVec = load(vecTy, ptr);
+          Value valVec = b.load(vecTy, ptr);
           for (unsigned v = 0; v < vec; ++v) {
-            Value currVal = extract_element(llvmElemTy, valVec, i32_val(v));
+            Value currVal = b.extract_element(llvmElemTy, valVec, b.i32_val(v));
             if (isInt1)
-              currVal = icmp_ne(currVal,
-                                rewriter.create<LLVM::ConstantOp>(
-                                    loc, i8_ty, rewriter.getI8IntegerAttr(0)));
+              currVal = b.icmp_ne(
+                  currVal, rewriter.create<LLVM::ConstantOp>(
+                               loc, i8_ty, rewriter.getI8IntegerAttr(0)));
             else if (isPtr)
-              currVal = inttoptr(llvmElemTyOrig, currVal);
+              currVal = b.inttoptr(llvmElemTyOrig, currVal);
             vals[elemId + linearCTAId * accumSizePerThread + v] = currVal;
           }
         }
@@ -152,6 +153,7 @@ private:
                                 ConversionPatternRewriter &rewriter,
                                 const TargetInfoBase &targetInfo) const {
     auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto typeConverter = getTypeConverter();
     RankedTensorType srcTy = op.getSrc().getType();
     RankedTensorType dstTy = op.getType();
@@ -211,12 +213,12 @@ private:
       auto multiDimRepId =
           getMultiDimIndex<unsigned>(repId, numReplicates, outOrd);
       if (repId != 0) {
-        barrier();
+        b.barrier();
       }
       processReplica(loc, rewriter, /*stNotRd*/ true, srcTy, inNumCTAsEachRep,
                      multiDimRepId, inVec, paddedRepShape, origRepShape, outOrd,
                      vals, smemBase);
-      barrier();
+      b.barrier();
       processReplica(loc, rewriter, /*stNotRd*/ false, dstTy, outNumCTAsEachRep,
                      multiDimRepId, outVec, paddedRepShape, origRepShape,
                      outOrd, outVals, smemBase);
@@ -357,6 +359,7 @@ struct ConvertLayoutOpUsingLinearLayoutsConversion
                                     ConversionPatternRewriter &rewriter) const {
     MLIRContext *ctx = op.getContext();
     auto loc = op.getLoc();
+    auto tb = TritonLLVMOpBuilder(loc, rewriter);
     auto srcTy = op.getSrc().getType();
     auto dstTy = op.getType();
 
@@ -423,9 +426,9 @@ struct ConvertLayoutOpUsingLinearLayoutsConversion
     // Munge input values
     for (const auto &it : llvm::enumerate(inVals)) {
       if (isSubByteInt) {
-        inVals[it.index()] = zext(llvmElemTy, it.value());
+        inVals[it.index()] = tb.zext(llvmElemTy, it.value());
       } else if (isPtr) {
-        inVals[it.index()] = ptrtoint(llvmElemTy, it.value());
+        inVals[it.index()] = tb.ptrtoint(llvmElemTy, it.value());
       }
     }
 
@@ -441,9 +444,9 @@ struct ConvertLayoutOpUsingLinearLayoutsConversion
     // Unmunge output values
     for (const auto &it : llvm::enumerate(outVals)) {
       if (isSubByteInt) {
-        outVals[it.index()] = trunc(llvmElemTyOrig, it.value());
+        outVals[it.index()] = tb.trunc(llvmElemTyOrig, it.value());
       } else if (isPtr) {
-        outVals[it.index()] = inttoptr(llvmElemTyOrig, it.value());
+        outVals[it.index()] = tb.inttoptr(llvmElemTyOrig, it.value());
       }
     }
 
@@ -452,8 +455,8 @@ struct ConvertLayoutOpUsingLinearLayoutsConversion
     // In this case, we need to pack the outputs into i32
     if (isa<DotOperandEncodingAttr>(dstTy.getEncoding())) {
       auto concat = [&](Value a, Value b) {
-        return or_(zext(i32_ty, bitcast(a, i16_ty)),
-                   shl(zext(i32_ty, bitcast(b, i16_ty)), i32_val(16)));
+        return tb.or_(tb.zext(i32_ty, tb.bitcast(a, i16_ty)),
+                      tb.shl(tb.zext(i32_ty, tb.bitcast(b, i16_ty)), tb.i32_val(16)));
       };
 
       SmallVector<Value> outVals32(outVals.size() / 2);
@@ -476,6 +479,7 @@ struct ConvertLayoutOpUsingLinearLayoutsConversion
                           ConversionPatternRewriter &rewriter) const {
     MLIRContext *ctx = op.getContext();
     auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
 
     StringAttr kRegister = str_attr("register");
     StringAttr kLane = str_attr("lane");
@@ -485,9 +489,9 @@ struct ConvertLayoutOpUsingLinearLayoutsConversion
     StringAttr kIteration = str_attr("iteration");
 
     Value threadId = getThreadId(rewriter, loc);
-    Value threadsPerWarp = i32_val(srcLayout.getInDimSize(kLane));
-    Value laneId = urem(threadId, threadsPerWarp);
-    Value warpId = udiv(threadId, threadsPerWarp);
+    Value threadsPerWarp = b.i32_val(srcLayout.getInDimSize(kLane));
+    Value laneId = b.urem(threadId, threadsPerWarp);
+    Value warpId = b.udiv(threadId, threadsPerWarp);
 
     auto scratchConfig =
         getScratchConfigForCvt(op.getSrc().getType(), op.getType());
@@ -575,37 +579,38 @@ struct ConvertLayoutOpUsingLinearLayoutsConversion
                                 {kWarp, 0},
                                 {kBlock, 0}})[0]
                         .second;
-      Value offset = xor_(regBase, i32_val(regIdx));
+      Value offset = b.xor_(regBase, b.i32_val(regIdx));
       if (paddedSize > 0) {
         assert(llvm::isPowerOf2_32(paddedStride));
         assert(llvm::isPowerOf2_32(paddedSize));
         auto rshiftVal = llvm::Log2_32(paddedStride);
         auto lshiftVal = llvm::Log2_32(paddedSize);
-        offset = add(shl(lshr(offset, i32_val(rshiftVal)), i32_val(lshiftVal)),
-                     offset);
+        offset = b.add(
+            b.shl(b.lshr(offset, b.i32_val(rshiftVal)), b.i32_val(lshiftVal)),
+            offset);
       }
-      auto vecAddr = gep(sharedPtrTy, elemTy, smemBase, offset);
+      auto vecAddr = b.gep(sharedPtrTy, elemTy, smemBase, offset);
       vecAddr.setInbounds(true);
       return vecAddr;
     };
 
     auto storeBase = applyLinearLayout(loc, rewriter, *shmemStoreLayout,
-                                       {{kRegister, i32_val(0)},
+                                       {{kRegister, b.i32_val(0)},
                                         {kLane, laneId},
                                         {kWarp, warpId},
-                                        {kBlock, i32_val(0)}})[0]
+                                        {kBlock, b.i32_val(0)}})[0]
                          .second;
     auto loadBase = applyLinearLayout(loc, rewriter, shmemLoadLayout,
-                                      {{kRegister, i32_val(0)},
+                                      {{kRegister, b.i32_val(0)},
                                        {kLane, laneId},
                                        {kWarp, warpId},
-                                       {kBlock, i32_val(0)}})[0]
+                                       {kBlock, b.i32_val(0)}})[0]
                         .second;
     // register idx -> Value
     llvm::MapVector<int, Value> outVals;
     for (int i = 0; i < iterations; i++) {
       if (i != 0)
-        barrier();
+        b.barrier();
 
       auto &inRegs = inRegsForIter[i];
       auto &outRegs = outRegsForIter[i];
@@ -625,11 +630,11 @@ struct ConvertLayoutOpUsingLinearLayoutsConversion
           targetInfo.storeMatrixShared(rewriter, loc, vecAddr, valsVec);
         } else {
           targetInfo.storeDShared(rewriter, loc, vecAddr, std::nullopt, valsVec,
-                                  /*pred=*/true_val());
+                                  /*pred=*/b.true_val());
         }
       }
 
-      barrier();
+      b.barrier();
 
       for (int j = 0; j < outSize / iterations; j += scratchConfig.outVec) {
         auto outRegSlice = outRegs[j];
@@ -637,7 +642,7 @@ struct ConvertLayoutOpUsingLinearLayoutsConversion
         Value valsVec =
             targetInfo.loadDShared(rewriter, loc, vecAddr, std::nullopt,
                                    vec_ty(elemTy, scratchConfig.outVec),
-                                   /*pred=*/true_val());
+                                   /*pred=*/b.true_val());
         for (Value v : unpackLLVector(loc, valsVec, rewriter))
           outVals[outRegSlice++] = v;
       }
diff --git a/lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp b/lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp
index be2e6f584..85fc705b4 100644
--- a/lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp
+++ b/lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp
@@ -18,16 +18,17 @@ SmallVector<Value>
 getThreadIds(Value threadId, ArrayRef<unsigned int> shapePerCTATile,
              ArrayRef<unsigned int> sizePerThread, ArrayRef<unsigned int> order,
              ConversionPatternRewriter &rewriter, Location loc) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   int dim = order.size();
   SmallVector<Value> threadIds(dim);
   for (unsigned k = 0; k < dim - 1; k++) {
-    Value dimK = i32_val(shapePerCTATile[order[k]] / sizePerThread[order[k]]);
-    Value rem = urem(threadId, dimK);
-    threadId = udiv(threadId, dimK);
+    Value dimK = b.i32_val(shapePerCTATile[order[k]] / sizePerThread[order[k]]);
+    Value rem = b.urem(threadId, dimK);
+    threadId = b.udiv(threadId, dimK);
     threadIds[order[k]] = rem;
   }
-  Value dimK = i32_val(shapePerCTATile[order[dim - 1]]);
-  threadIds[order[dim - 1]] = urem(threadId, dimK);
+  Value dimK = b.i32_val(shapePerCTATile[order[dim - 1]]);
+  threadIds[order[dim - 1]] = b.urem(threadId, dimK);
   return threadIds;
 }
 
@@ -91,6 +92,7 @@ ValueTable getValueTableFromStruct(Value val, int K, int n0, int shapePerCTA,
 Value loadAFMA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,
                Location loc, const LLVMTypeConverter *typeConverter,
                ConversionPatternRewriter &rewriter) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto aTensorTy = cast<MemDescType>(A.getType());
   auto aLayout = cast<SharedEncodingAttr>(aTensorTy.getEncoding());
   auto aShapePerCTA = getShapePerCTA(aTensorTy);
@@ -114,27 +116,26 @@ Value loadAFMA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,
   auto shapePerCTATile = getShapePerCTATile(dLayout);
   auto sizePerThread = getSizePerThread(dLayout);
 
-  Value _0 = i32_val(0);
-
-  Value mContig = i32_val(sizePerThread[order[1]]);
+  Value _0 = b.i32_val(0);
 
+  Value mContig = b.i32_val(sizePerThread[order[1]]);
   // threadId in blocked layout
   auto threadIds = getThreadIds(thread, shapePerCTATile, sizePerThread, order,
                                 rewriter, loc);
   Value threadIdM = threadIds[0];
 
-  Value offA0 = isARow ? _0 : mul(threadIdM, mContig);
-  Value offA1 = isARow ? mul(threadIdM, mContig) : _0;
+  Value offA0 = isARow ? _0 : b.mul(threadIdM, mContig);
+  Value offA1 = isARow ? b.mul(threadIdM, mContig) : _0;
   SmallVector<Value> aOff(aNumPtr);
   for (int i = 0; i < aNumPtr; ++i) {
-    aOff[i] = add(mul(offA0, strideA0), mul(offA1, strideA1));
+    aOff[i] = b.add(b.mul(offA0, strideA0), b.mul(offA1, strideA1));
   }
   auto elemTy = typeConverter->convertType(aTensorTy.getElementType());
 
   Type ptrTy = aSmem.base.getType();
   SmallVector<Value> aPtrs(aNumPtr);
   for (int i = 0; i < aNumPtr; ++i)
-    aPtrs[i] = gep(ptrTy, elemTy, aSmem.base, aOff[i]);
+    aPtrs[i] = b.gep(ptrTy, elemTy, aSmem.base, aOff[i]);
 
   SmallVector<Value> vas;
 
@@ -145,9 +146,9 @@ Value loadAFMA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,
     for (unsigned m = 0; m < M; m += mShapePerCTATile)
       for (unsigned mm = 0; mm < mSizePerThread; ++mm) {
         Value offset =
-            add(mul(i32_val(m + mm), strideAM), mul(i32_val(k), strideAK));
-        Value pa = gep(ptrTy, elemTy, aPtrs[0], offset);
-        Value va = load(elemTy, pa);
+            b.add(b.mul(b.i32_val(m + mm), strideAM), b.mul(b.i32_val(k), strideAK));
+        Value pa = b.gep(ptrTy, elemTy, aPtrs[0], offset);
+        Value va = b.load(elemTy, pa);
         vas.emplace_back(va);
       }
 
@@ -157,6 +158,7 @@ Value loadAFMA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,
 Value loadBFMA(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,
                Location loc, const LLVMTypeConverter *typeConverter,
                ConversionPatternRewriter &rewriter) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto bTensorTy = cast<MemDescType>(B.getType());
   auto bLayout = cast<SharedEncodingAttr>(bTensorTy.getEncoding());
   auto bShapePerCTA = getShapePerCTA(bTensorTy);
@@ -180,27 +182,27 @@ Value loadBFMA(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,
   auto shapePerCTATile = getShapePerCTATile(dLayout);
   auto sizePerThread = getSizePerThread(dLayout);
 
-  Value _0 = i32_val(0);
+  Value _0 = b.i32_val(0);
 
-  Value nContig = i32_val(sizePerThread[order[0]]);
+  Value nContig = b.i32_val(sizePerThread[order[0]]);
 
   // threadId in blocked layout
   auto threadIds = getThreadIds(thread, shapePerCTATile, sizePerThread, order,
                                 rewriter, loc);
   Value threadIdN = threadIds[1];
 
-  Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;
-  Value offB1 = isBRow ? _0 : mul(threadIdN, nContig);
+  Value offB0 = isBRow ? b.mul(threadIdN, nContig) : _0;
+  Value offB1 = isBRow ? _0 : b.mul(threadIdN, nContig);
   SmallVector<Value> bOff(bNumPtr);
   for (int i = 0; i < bNumPtr; ++i) {
-    bOff[i] = add(mul(offB0, strideB0), mul(offB1, strideB1));
+    bOff[i] = b.add(b.mul(offB0, strideB0), b.mul(offB1, strideB1));
   }
   auto elemTy = typeConverter->convertType(bTensorTy.getElementType());
 
   Type ptrTy = bSmem.base.getType();
   SmallVector<Value> bPtrs(bNumPtr);
   for (int i = 0; i < bNumPtr; ++i)
-    bPtrs[i] = gep(ptrTy, elemTy, bSmem.base, bOff[i]);
+    bPtrs[i] = b.gep(ptrTy, elemTy, bSmem.base, bOff[i]);
 
   SmallVector<Value> vbs;
 
@@ -211,9 +213,9 @@ Value loadBFMA(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,
     for (unsigned n = 0; n < N; n += nShapePerCTATile)
       for (unsigned nn = 0; nn < nSizePerThread; ++nn) {
         Value offset =
-            add(mul(i32_val(n + nn), strideBN), mul(i32_val(k), strideBK));
-        Value pb = gep(ptrTy, elemTy, bPtrs[0], offset);
-        Value vb = load(elemTy, pb);
+            b.add(b.mul(b.i32_val(n + nn), strideBN), b.mul(b.i32_val(k), strideBK));
+        Value pb = b.gep(ptrTy, elemTy, bPtrs[0], offset);
+        Value vb = b.load(elemTy, pb);
         vbs.emplace_back(vb);
       }
 
diff --git a/lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp b/lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp
index 8ee166866..6261eac82 100644
--- a/lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp
+++ b/lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp
@@ -106,6 +106,7 @@ SmallVector<Value> reorderValues(const SmallVector<Value> &values, Type inType,
 SmallVector<Value> unpackI32(const SmallVector<Value> &inValues, Type srcTy,
                              ConversionPatternRewriter &rewriter, Location loc,
                              const LLVMTypeConverter *typeConverter) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto tensorTy = dyn_cast<RankedTensorType>(srcTy);
   if (!tensorTy)
     return inValues;
@@ -117,9 +118,9 @@ SmallVector<Value> unpackI32(const SmallVector<Value> &inValues, Type srcTy,
     // cast i32 to appropriate eltType vector and extract elements
     auto eltType = typeConverter->convertType(tensorTy.getElementType());
     auto vecType = vec_ty(eltType, 32 / eltType.getIntOrFloatBitWidth());
-    auto vec = bitcast(v, vecType);
+    auto vec = b.bitcast(v, vecType);
     for (int i = 0; i < 32 / eltType.getIntOrFloatBitWidth(); i++) {
-      outValues.push_back(extract_element(vec, i32_val(i)));
+      outValues.push_back(b.extract_element(vec, b.i32_val(i)));
     }
   }
   return outValues;
@@ -128,6 +129,7 @@ SmallVector<Value> unpackI32(const SmallVector<Value> &inValues, Type srcTy,
 SmallVector<Value> packI32(const SmallVector<Value> &inValues, Type srcTy,
                            ConversionPatternRewriter &rewriter, Location loc,
                            const LLVMTypeConverter *typeConverter) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto tensorTy = dyn_cast<RankedTensorType>(srcTy);
   if (!tensorTy)
     return inValues;
@@ -139,11 +141,11 @@ SmallVector<Value> packI32(const SmallVector<Value> &inValues, Type srcTy,
   int vecWidth = 32 / eltType.getIntOrFloatBitWidth();
   auto vecType = vec_ty(eltType, vecWidth);
   for (int i = 0; i < inValues.size(); i += vecWidth) {
-    Value vec = undef(vecType);
+    Value vec = b.undef(vecType);
     for (int j = 0; j < vecWidth; j++) {
-      vec = insert_element(vec, inValues[i + j], i32_val(j));
+      vec = b.insert_element(vec, inValues[i + j], b.i32_val(j));
     }
-    outValues.push_back(bitcast(vec, i32_ty));
+    outValues.push_back(b.bitcast(vec, i32_ty));
   }
   return outValues;
 }
@@ -181,6 +183,7 @@ struct AddPtrOpConversion : public ConvertOpToLLVMPattern<AddPtrOp> {
   matchAndRewrite(AddPtrOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
     Location loc = op->getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto resultTy = op.getType();
     auto typeConverter = getTypeConverter();
     auto resultTensorTy = dyn_cast<RankedTensorType>(resultTy);
@@ -193,7 +196,7 @@ struct AddPtrOpConversion : public ConvertOpToLLVMPattern<AddPtrOp> {
       auto offsets = unpackLLElements(loc, adaptor.getOffset(), rewriter);
       SmallVector<Value> resultVals(elems);
       for (unsigned i = 0; i < elems; ++i) {
-        resultVals[i] = gep(ptrTy, elemTy, ptrs[i], offsets[i]);
+        resultVals[i] = b.gep(ptrTy, elemTy, ptrs[i], offsets[i]);
       }
       Value view =
           packLLElements(loc, typeConverter, resultVals, rewriter, resultTy);
@@ -203,8 +206,8 @@ struct AddPtrOpConversion : public ConvertOpToLLVMPattern<AddPtrOp> {
       auto resultPtrTy = typeConverter->convertType(resultTy);
       auto resultElemTy = typeConverter->convertType(
           cast<PointerType>(resultTy).getPointeeType());
-      Value result =
-          gep(resultPtrTy, resultElemTy, adaptor.getPtr(), adaptor.getOffset());
+      Value result = b.gep(resultPtrTy, resultElemTy, adaptor.getPtr(),
+                           adaptor.getOffset());
       rewriter.replaceOp(op, result);
     }
     return success();
@@ -388,6 +391,7 @@ struct ElementwiseInlineAsmOpConversion
                                   MultipleOperandsRange operands,
                                   ConversionPatternRewriter &rewriter,
                                   Location loc) const {
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     SmallVector<Value> packedOperands;
     unsigned numPackedElements = op.getPackedElement();
     for (int i = 0, e = op.getNumOperands(); i < e; i++) {
@@ -403,9 +407,9 @@ struct ElementwiseInlineAsmOpConversion
         }
         Type t =
             vec_ty(getTypeConverter()->convertType(elemTy), numElementPerReg);
-        Value packed = undef(t);
+        Value packed = b.undef(t);
         for (int k = 0; k < numElementPerReg; k++) {
-          packed = insert_element(packed, operands[j + k][i], i32_val(k));
+          packed = b.insert_element(packed, operands[j + k][i], b.i32_val(k));
         }
         packedOperands.push_back(packed);
       }
@@ -418,6 +422,7 @@ struct ElementwiseInlineAsmOpConversion
                 ConversionPatternRewriter &rewriter,
                 MultipleOperandsRange operands, Location loc) const {
     auto ctx = op->getContext();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
 
     if (operands.size() % op.getPackedElement() != 0)
       llvm::report_fatal_error("Inline asm op has more packed elements than "
@@ -472,13 +477,13 @@ struct ElementwiseInlineAsmOpConversion
         Value val;
         if (asmRetTypes.size() > 1) {
           val =
-              extract_val(asmResults, i * op.getPackedElement() + structIdx++);
+              b.extract_val(asmResults, i * op.getPackedElement() + structIdx++);
         } else {
           val = asmResults;
         }
         if (auto vectorTy = dyn_cast<VectorType>(val.getType())) {
           for (int k = 0; k < vectorTy.getNumElements(); k++) {
-            ret[i].push_back(extract_element(val, i32_val(k)));
+            ret[i].push_back(b.extract_element(val, b.i32_val(k)));
           }
           j += vectorTy.getNumElements() - 1;
         } else {
@@ -493,6 +498,7 @@ struct ElementwiseInlineAsmOpConversion
   matchAndRewrite(ElementwiseInlineAsmOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
     Location loc = op->getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
 
     // Layout is unpackedOperands[operand][elem].
     SmallVector<SmallVector<Value>> unpackedOperands;
@@ -518,7 +524,7 @@ struct ElementwiseInlineAsmOpConversion
           op.getPackedElement() - numElemsPerThread % op.getPackedElement();
       for (auto &operands : unpackedOperands) {
         for (int i = 0; i < numPaddedValue; i++) {
-          operands.push_back(undef(operands[0].getType()));
+          operands.push_back(b.undef(operands[0].getType()));
         }
       }
     }
@@ -596,6 +602,7 @@ struct AbsFOpConversion
                                    ConversionPatternRewriter &rewriter,
                                    Type elemTy, MultipleOperandsRange operands,
                                    Location loc) const {
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     if (llvm::isa<IntegerType>(elemTy)) {
       // Mask out the sign bit
       auto num_bits =
@@ -604,7 +611,7 @@ struct AbsFOpConversion
       auto mask = (1u << (num_bits - 1u)) - 1u;
       auto maskAttr = rewriter.getIntegerAttr(elemTy, mask);
       auto maskConst = rewriter.create<LLVM::ConstantOp>(loc, maskAttr);
-      return {and_(operands[0][0], maskConst)};
+      return {b.and_(operands[0][0], maskConst)};
     }
 
     return {rewriter.create<LLVM::FAbsOp>(loc, elemTy, operands[0][0])};
diff --git a/lib/Conversion/TritonGPUToLLVM/HistogramOpToLLVM.cpp b/lib/Conversion/TritonGPUToLLVM/HistogramOpToLLVM.cpp
index ed4837fc1..ac350a149 100644
--- a/lib/Conversion/TritonGPUToLLVM/HistogramOpToLLVM.cpp
+++ b/lib/Conversion/TritonGPUToLLVM/HistogramOpToLLVM.cpp
@@ -18,9 +18,10 @@ static SmallVector<Value> computeWarpLevelHistogram(
     Location loc, RankedTensorType srcType, SmallVector<Value> &srcValues,
     int numBins, int numThreadPerWarp, Value threadId,
     ConversionPatternRewriter &rewriter, const TargetInfoBase &targetInfo) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   assert(numBins % numThreadPerWarp == 0 &&
          "numBins must be divisible by numThreadPerWarp");
-  Value zero = i32_val(0);
+  Value zero = b.i32_val(0);
   int numBits = log2Int(numBins);
   int numBitsLaneId = log2Int(numThreadPerWarp);
   unsigned numElementsPerThreads = triton::gpu::getTotalElemsPerThread(srcType);
@@ -34,25 +35,26 @@ static SmallVector<Value> computeWarpLevelHistogram(
     Value value = srcValues[i];
     SmallVector<Value> ballotBits;
     for (int j = 0; j < numBits; ++j) {
-      Value bitSet = and_(value, i32_val(1 << j));
-      Value cmp = icmp_ne(bitSet, zero);
+      Value bitSet = b.and_(value, b.i32_val(1 << j));
+      Value cmp = b.icmp_ne(bitSet, zero);
       Value bit =
           targetInfo.ballot(rewriter, loc, int_ty(numThreadPerWarp), cmp);
       ballotBits.push_back(bit);
     }
     uint64_t fullMaskValue =
         numThreadPerWarp == 32 ? 0xFFFFFFFF : 0xFFFFFFFFFFFFFFFF;
-    Value fullMask = int_val(numThreadPerWarp, fullMaskValue);
+    Value fullMask = b.int_val(numThreadPerWarp, fullMaskValue);
     Value mask = fullMask;
     // If not all threads have unique data, mask out the redundant ones.
     if (numThreadWithUniqueData < numThreadPerWarp) {
-      mask = int_val(numThreadPerWarp, (1ULL << numThreadWithUniqueData) - 1);
+      mask = b.int_val(numThreadPerWarp, (1ULL << numThreadWithUniqueData) - 1);
     }
     for (int i = 0; i < numBitsLaneId; i++) {
-      Value updateMask = select(icmp_ne(and_(threadId, i32_val(1 << i)), zero),
-                                int_val(numThreadPerWarp, 0), fullMask);
-      mask =
-          and_(mask, xor_(ballotBits[i + numBits - numBitsLaneId], updateMask));
+      Value updateMask =
+          b.select(b.icmp_ne(b.and_(threadId, b.i32_val(1 << i)), zero),
+                   b.int_val(numThreadPerWarp, 0), fullMask);
+      mask = b.and_(
+          mask, b.xor_(ballotBits[i + numBits - numBitsLaneId], updateMask));
     }
     // at this point, 'mask' tells you which elements are in a bin owned by this
     // thread.
@@ -60,16 +62,16 @@ static SmallVector<Value> computeWarpLevelHistogram(
       Value binMask = mask;
       for (int j = 0; j < numBits - numBitsLaneId; j++) {
         Value updateMask =
-            int_val(numThreadPerWarp, ((k & (1 << j)) ? 0 : fullMaskValue));
-        binMask = and_(binMask, xor_(ballotBits[j], updateMask));
+            b.int_val(numThreadPerWarp, ((k & (1 << j)) ? 0 : fullMaskValue));
+        binMask = b.and_(binMask, b.xor_(ballotBits[j], updateMask));
       }
       // at this point, 'bin_mask' tells you which elements are in the kth bin
       // owned by this thread.
       Value bitCount = rewriter.create<LLVM::CtPopOp>(
           loc, int_ty(numThreadPerWarp), binMask);
       if (numThreadPerWarp > 32)
-        bitCount = trunc(i32_ty, bitCount);
-      warpLevelHistogram[k] = add(warpLevelHistogram[k], bitCount);
+        bitCount = b.trunc(i32_ty, bitCount);
+      warpLevelHistogram[k] = b.add(warpLevelHistogram[k], bitCount);
     }
   }
   return warpLevelHistogram;
@@ -86,22 +88,24 @@ static SmallVector<Value> computeCrossWarpHistogram(
     Value baseSharedMemPtr, const SmallVector<Value> &warpLevelHistogram,
     int numBins, int numThreadPerWarp, const SmallVector<Value> &indices,
     Value threadId, int numWarps) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   SmallVector<Value> histogramValues;
   unsigned numWarpsWithUniqueData =
       mlir::triton::gpu::getWarpsPerCTAWithUniqueData(srcType.getEncoding(),
                                                       srcType.getShape())[0];
-  Value laneId = and_(threadId, i32_val(numThreadPerWarp - 1));
+  Value laneId = b.and_(threadId, b.i32_val(numThreadPerWarp - 1));
   // Initialize the shared memory with zeros.
   int64_t numElementPerThread =
       ceil<int64_t>(numBins, numThreadPerWarp * numWarps);
   for (int i = 0; i < numElementPerThread; ++i) {
-    Value offset = add(threadId, i32_val((i * numWarps * numThreadPerWarp)));
-    offset = urem(offset, i32_val(numBins));
+    Value offset =
+        b.add(threadId, b.i32_val((i * numWarps * numThreadPerWarp)));
+    offset = b.urem(offset, b.i32_val(numBins));
     Value sharedMemPtr =
-        gep(baseSharedMemPtr.getType(), i32_ty, baseSharedMemPtr, offset);
-    store(i32_val(0), sharedMemPtr);
+        b.gep(baseSharedMemPtr.getType(), i32_ty, baseSharedMemPtr, offset);
+    b.store(b.i32_val(0), sharedMemPtr);
   }
-  barrier();
+  b.barrier();
   Block *afterAtomics = nullptr;
   // If some warps have replicated data we need to skip those warps when
   // accumulating.
@@ -111,30 +115,30 @@ static SmallVector<Value> computeCrossWarpHistogram(
         rewriter.splitBlock(currentBlock, rewriter.getInsertionPoint());
     Block *atomicBlock = rewriter.createBlock(afterAtomics);
     rewriter.setInsertionPointToEnd(currentBlock);
-    Value cond =
-        icmp_ult(threadId, i32_val(numWarpsWithUniqueData * numThreadPerWarp));
+    Value cond = b.icmp_ult(
+        threadId, b.i32_val(numWarpsWithUniqueData * numThreadPerWarp));
     rewriter.create<LLVM::CondBrOp>(loc, cond, atomicBlock, afterAtomics);
     rewriter.setInsertionPointToStart(atomicBlock);
   }
   // Apply atomic add to update the histogram in shared memory.
   for (int i = 0; i < warpLevelHistogram.size(); ++i) {
     Value warpLevelHistogramValue = warpLevelHistogram[i];
-    Value offset =
-        add(mul(laneId, i32_val(warpLevelHistogram.size())), i32_val(i));
+    Value offset = b.add(b.mul(laneId, b.i32_val(warpLevelHistogram.size())),
+                         b.i32_val(i));
     Value sharedMemPtr =
-        gep(baseSharedMemPtr.getType(), i32_ty, baseSharedMemPtr, offset);
+        b.gep(baseSharedMemPtr.getType(), i32_ty, baseSharedMemPtr, offset);
     atomicAdd(sharedMemPtr, warpLevelHistogramValue, loc, rewriter);
   }
   if (afterAtomics) {
     rewriter.create<LLVM::BrOp>(loc, afterAtomics);
     rewriter.setInsertionPointToStart(afterAtomics);
   }
-  barrier();
+  b.barrier();
   // load the histogram to register with the right layout.
   for (Value index : indices) {
     Value sharedMemPtr =
-        gep(baseSharedMemPtr.getType(), i32_ty, baseSharedMemPtr, index);
-    Value val = load(i32_ty, sharedMemPtr);
+        b.gep(baseSharedMemPtr.getType(), i32_ty, baseSharedMemPtr, index);
+    Value val = b.load(i32_ty, sharedMemPtr);
     histogramValues.push_back(val);
   }
   return histogramValues;
diff --git a/lib/Conversion/TritonGPUToLLVM/MakeRangeOpToLLVM.cpp b/lib/Conversion/TritonGPUToLLVM/MakeRangeOpToLLVM.cpp
index 43120c791..8060b4431 100644
--- a/lib/Conversion/TritonGPUToLLVM/MakeRangeOpToLLVM.cpp
+++ b/lib/Conversion/TritonGPUToLLVM/MakeRangeOpToLLVM.cpp
@@ -18,6 +18,7 @@ struct MakeRangeOpConversion
   matchAndRewrite(triton::MakeRangeOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
     Location loc = op->getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     RankedTensorType ty = op.getType();
     auto shape = ty.getShape();
     auto layout = ty.getEncoding();
@@ -32,7 +33,7 @@ struct MakeRangeOpConversion
     // expand dims + broadcast. very weird behavior otherwise potentially.
     for (const auto &multiDim : llvm::enumerate(idxs)) {
       assert(multiDim.value().size() == 1);
-      retVals[multiDim.index()] = add(multiDim.value()[0], start);
+      retVals[multiDim.index()] = b.add(multiDim.value()[0], start);
     }
     auto typeConverter = getTypeConverter();
     Value result = packLLElements(loc, typeConverter, retVals, rewriter, ty);
diff --git a/lib/Conversion/TritonGPUToLLVM/MemoryOpToLLVM.cpp b/lib/Conversion/TritonGPUToLLVM/MemoryOpToLLVM.cpp
index 1a0c115a9..127998fca 100644
--- a/lib/Conversion/TritonGPUToLLVM/MemoryOpToLLVM.cpp
+++ b/lib/Conversion/TritonGPUToLLVM/MemoryOpToLLVM.cpp
@@ -167,6 +167,7 @@ private:
                            const LLVMTypeConverter *typeConverter,
                            ConversionPatternRewriter &rewriter) const {
     auto loc = op.getLoc();
+    auto tb = TritonLLVMOpBuilder(loc, rewriter);
     auto srcTy = op.getSrc().getType();
     auto dstTy = op.getResult().getType();
     auto dstShape = dstTy.getShape();
@@ -192,10 +193,10 @@ private:
         if (parent.isAmpere()) {
           if (elemLlvmTy.isInteger(8)) {
             auto concat = [&](Value a1, Value a2, Value a3, Value a4) {
-              return or_(
-                  or_(zext(i32_ty, a1), shl(zext(i32_ty, a2), i32_val(8))),
-                  or_(shl(zext(i32_ty, a3), i32_val(16)),
-                      shl(zext(i32_ty, a4), i32_val(24))));
+              return tb.or_(
+                  tb.or_(tb.zext(i32_ty, a1), tb.shl(tb.zext(i32_ty, a2), tb.i32_val(8))),
+                  tb.or_(tb.shl(tb.zext(i32_ty, a3), tb.i32_val(16)),
+                         tb.shl(tb.zext(i32_ty, a4), tb.i32_val(24))));
             };
             SmallVector<Value> outVals32(outVals.size() / 4);
             for (int i = 0; i < outVals32.size(); ++i) {
@@ -206,8 +207,8 @@ private:
           } else {
             assert(elemLlvmTy.isBF16() && "Unexpected element type");
             auto concat = [&](Value a, Value b) {
-              return or_(zext(i32_ty, bitcast(a, i16_ty)),
-                         shl(zext(i32_ty, bitcast(b, i16_ty)), i32_val(16)));
+              return tb.or_(tb.zext(i32_ty, tb.bitcast(a, i16_ty)),
+                            tb.shl(tb.zext(i32_ty, tb.bitcast(b, i16_ty)), tb.i32_val(16)));
             };
 
             SmallVector<Value> outVals32(outVals.size() / 2);
diff --git a/lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp b/lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp
index 966f6d31c..6a55635fe 100644
--- a/lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp
+++ b/lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp
@@ -110,7 +110,8 @@ private:
 
   void sync(ConversionPatternRewriter &rewriter, Location loc,
             triton::ReduceOp op) const {
-    barrier();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
+    b.barrier();
   }
 
   // Reduce along op axis for elements that are in the same thread. The
@@ -254,11 +255,12 @@ private:
       ConversionPatternRewriter &rewriter) const {
     triton::ReduceOp op = helper.getOperation();
     Location loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     Value threadId = getThreadId(rewriter, loc);
     auto srcLayout = helper.getSrcLayout();
-    Value warpSize = i32_val(triton::gpu::getWarpSize(srcLayout));
-    Value warpId = udiv(threadId, warpSize);
-    Value laneId = urem(threadId, warpSize);
+    Value warpSize = b.i32_val(triton::gpu::getWarpSize(srcLayout));
+    Value warpId = b.udiv(threadId, warpSize);
+    Value laneId = b.urem(threadId, warpSize);
     auto srcShape = helper.getSrcShape();
     unsigned axis = op.getAxis();
     auto smemShape = helper.getScratchRepShape();
@@ -269,8 +271,8 @@ private:
     SmallVector<Value> multiDimLaneId =
         delinearize(rewriter, loc, laneId, threadsPerWarp, order);
     Value laneIdAxis = multiDimLaneId[axis];
-    Value zero = i32_val(0);
-    Value laneZero = icmp_eq(laneIdAxis, zero);
+    Value zero = b.i32_val(0);
+    Value laneZero = b.icmp_eq(laneIdAxis, zero);
 
     SmallVector<Value> multiDimWarpId =
         getMultiDimWarpId(helper, warpId, loc, rewriter);
@@ -288,7 +290,7 @@ private:
       for (unsigned i = 0; i < op.getNumOperands(); ++i) {
         auto elemTy = getElementType(op, i);
         Value writePtr =
-            gep(smemBases[i].getType(), elemTy, smemBases[i], writeOffset);
+            b.gep(smemBases[i].getType(), elemTy, smemBases[i], writeOffset);
         targetInfo.storeShared(rewriter, loc, writePtr, acc[i], laneZero);
       }
     }
@@ -305,25 +307,26 @@ private:
     unsigned elems = product<unsigned>(smemShape);
     unsigned sizeInterWarps = helper.getInterWarpSizeWithUniqueData();
     Location loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
 
     Value threadId = getThreadId(rewriter, loc);
-    Value warpSize = i32_val(triton::gpu::getWarpSize(srcLayout));
-    Value laneId = urem(threadId, warpSize);
-    Value zero = i32_val(0);
+    Value warpSize = b.i32_val(triton::gpu::getWarpSize(srcLayout));
+    Value laneId = b.urem(threadId, warpSize);
+    Value zero = b.i32_val(0);
 
     auto mod = op.getOperation()->getParentOfType<ModuleOp>();
     unsigned numThreads =
         product<unsigned>(triton::gpu::getWarpsPerCTA(srcLayout)) *
         triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);
     unsigned elemsPerThread = std::max<unsigned>(elems / numThreads, 1);
-    Value threadIsNeeded = icmp_slt(threadId, i32_val(elems));
+    Value threadIsNeeded = b.icmp_slt(threadId, b.i32_val(elems));
     Value readOffset = threadId;
     for (unsigned round = 0; round < elemsPerThread; ++round) {
       SmallVector<Value> acc(op.getNumOperands());
       for (unsigned i = 0; i < op.getNumOperands(); ++i) {
         auto elemTy = getElementType(op, i);
         Value readPtr =
-            gep(smemBases[i].getType(), elemTy, smemBases[i], readOffset);
+            b.gep(smemBases[i].getType(), elemTy, smemBases[i], readOffset);
         acc[i] = targetInfo.loadShared(rewriter, loc, readPtr, elemTy,
                                        threadIsNeeded);
       }
@@ -335,20 +338,20 @@ private:
       for (unsigned i = 0; i < op.getNumOperands(); ++i) {
         auto elemTy = getElementType(op, i);
         writePtrs[i] =
-            gep(smemBases[i].getType(), elemTy, smemBases[i], writeOffset);
+            b.gep(smemBases[i].getType(), elemTy, smemBases[i], writeOffset);
       }
 
-      Value laneIdModSizeInterWarps = urem(laneId, i32_val(sizeInterWarps));
+      Value laneIdModSizeInterWarps = b.urem(laneId, b.i32_val(sizeInterWarps));
       Value laneIdModSizeInterWarpsIsZero =
-          icmp_eq(laneIdModSizeInterWarps, zero);
-      Value pred = and_(threadIsNeeded, laneIdModSizeInterWarpsIsZero);
+          b.icmp_eq(laneIdModSizeInterWarps, zero);
+      Value pred = b.and_(threadIsNeeded, laneIdModSizeInterWarpsIsZero);
 
       for (unsigned i = 0; i < op.getNumOperands(); ++i) {
         targetInfo.storeShared(rewriter, loc, writePtrs[i], acc[i], pred);
       }
 
       if (round != elemsPerThread - 1) {
-        readOffset = add(readOffset, i32_val(numThreads));
+        readOffset = b.add(readOffset, b.i32_val(numThreads));
       }
     }
   }
@@ -361,6 +364,7 @@ private:
                                   ConversionPatternRewriter &rewriter) const {
     triton::ReduceOp op = helper.getOperation();
     Location loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto srcLayout = helper.getSrcLayout();
     auto axis = op.getAxis();
     auto smemOrder = helper.getOrderWithAxisAtBeginning();
@@ -381,7 +385,7 @@ private:
         SmallVector<Value> resultVals(resultElems);
         for (size_t j = 0; j < resultElems; ++j) {
           SmallVector<Value> readIdx = resultIndices[j];
-          readIdx.insert(readIdx.begin() + op.getAxis(), i32_val(0));
+          readIdx.insert(readIdx.begin() + op.getAxis(), b.i32_val(0));
           for (size_t resultIdx = 0, resultDim = resultShape.size();
                resultIdx < resultDim; ++resultIdx) {
             auto smemIdx = resultIdx < op.getAxis() ? resultIdx : resultIdx + 1;
@@ -391,21 +395,21 @@ private:
               // elements is accumulated in smem. Modulo smemShape effectively
               // replicates srcShape elements to src sizePerThread.
               readIdx[smemIdx] =
-                  urem(readIdx[smemIdx], i32_val(smemShape[smemIdx]));
+                  b.urem(readIdx[smemIdx], b.i32_val(smemShape[smemIdx]));
             }
           }
           Value readOffset =
               linearize(rewriter, loc, readIdx, smemShape, smemOrder);
           Value readPtr =
-              gep(smemBases[i].getType(), elemTy, smemBases[i], readOffset);
-          resultVals[j] = load(elemTy, readPtr);
+              b.gep(smemBases[i].getType(), elemTy, smemBases[i], readOffset);
+          resultVals[j] = b.load(elemTy, readPtr);
         }
 
         results[i] = packLLElements(loc, getTypeConverter(), resultVals,
                                     rewriter, resultTy);
       } else {
         // 0d-tensor -> scalar
-        results[i] = load(elemTy, smemBases[i]);
+        results[i] = b.load(elemTy, smemBases[i]);
       }
     }
     rewriter.replaceOp(op, results);
diff --git a/lib/Conversion/TritonGPUToLLVM/ReduceScanCommon.h b/lib/Conversion/TritonGPUToLLVM/ReduceScanCommon.h
index a35d52776..bd68b78b2 100644
--- a/lib/Conversion/TritonGPUToLLVM/ReduceScanCommon.h
+++ b/lib/Conversion/TritonGPUToLLVM/ReduceScanCommon.h
@@ -140,6 +140,7 @@ public:
                                   ConversionPatternRewriter &rewriter,
                                   const TargetInfoBase &targetInfo) const {
     auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     // indices will store the index of the op operands in descending order
     // of their bitwidths
     std::vector<unsigned> indices(op.getNumOperands());
@@ -156,8 +157,8 @@ public:
     indexToBase[indices[0]] = basePtr;
     for (unsigned i = 1; i < op.getNumOperands(); ++i) {
       indexToBase[indices[i]] =
-          gep(basePtr.getType(), getElementType(op, indices[i - 1]),
-              indexToBase[indices[i - 1]], i32_val(elems));
+          b.gep(basePtr.getType(), getElementType(op, indices[i - 1]),
+                indexToBase[indices[i - 1]], b.i32_val(elems));
     }
     // smemBases[k] is the base pointer for the k-th operand
     SmallVector<Value> smemBases(op.getNumOperands());
diff --git a/lib/Conversion/TritonGPUToLLVM/ScanOpToLLVM.cpp b/lib/Conversion/TritonGPUToLLVM/ScanOpToLLVM.cpp
index 969b227c8..46b4f9bc8 100644
--- a/lib/Conversion/TritonGPUToLLVM/ScanOpToLLVM.cpp
+++ b/lib/Conversion/TritonGPUToLLVM/ScanOpToLLVM.cpp
@@ -53,6 +53,7 @@ static void warpScan(SmallVector<SmallVector<Value>> &srcValues,
                      const TargetInfoBase &targetInfo,
                      ScanLoweringHelper &helper, Value laneIdAxis) {
   Location loc = helper.getLoc();
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThread();
   unsigned elementStride = helper.getAxisElementStride();
   unsigned threadStride = helper.getAxisThreadStride();
@@ -69,11 +70,11 @@ static void warpScan(SmallVector<SmallVector<Value>> &srcValues,
       for (unsigned j = 0; j < acc.size(); ++j) {
         shfl[j] = targetInfo.shuffleUp(rewriter, loc, acc[j], i * threadStride);
       }
-      Value mask = icmp_sge(laneIdAxis, i32_val(i));
+      Value mask = b.icmp_sge(laneIdAxis, b.i32_val(i));
       SmallVector<Value> tempAcc =
           accumulate(helper, rewriter, shfl, acc, mask);
       for (unsigned j = 0; j < acc.size(); ++j) {
-        acc[j] = select(mask, tempAcc[j], acc[j]);
+        acc[j] = b.select(mask, tempAcc[j], acc[j]);
       }
     }
     srcValues[srcIndex] = std::move(acc);
@@ -94,6 +95,7 @@ static void storeWarpAccumulator(SmallVector<SmallVector<Value>> &srcValues,
                                  Value parallelLaneId,
                                  const TargetInfoBase &targetInfo) {
   Location loc = helper.getLoc();
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThread();
   unsigned scanDim = helper.getAxisNumThreadsPerWarpWithUniqueData();
   unsigned numParallelLane = helper.getNonAxisNumThreadsPerCTA();
@@ -107,12 +109,13 @@ static void storeWarpAccumulator(SmallVector<SmallVector<Value>> &srcValues,
     if (elementIdx != scanElementsPerThreads - 1)
       continue;
     auto lastElement = srcValues[srcIndex];
-    Value mask = icmp_eq(laneId, i32_val(scanDim - 1));
-    Value index = add(parallelLaneId, mul(warpId, i32_val(numParallelLane)));
-    index = add(index, i32_val(chunkId * numParallelLane * axisNumWarps));
+    Value mask = b.icmp_eq(laneId, b.i32_val(scanDim - 1));
+    Value index =
+        b.add(parallelLaneId, b.mul(warpId, b.i32_val(numParallelLane)));
+    index = b.add(index, b.i32_val(chunkId * numParallelLane * axisNumWarps));
     for (unsigned i = 0; i < lastElement.size(); ++i) {
       Value writePtr =
-          gep(smemBases[i].getType(), smemTypes[i], smemBases[i], index);
+          b.gep(smemBases[i].getType(), smemTypes[i], smemBases[i], index);
       targetInfo.storeShared(rewriter, loc, writePtr, lastElement[i], mask);
     }
     chunkId++;
@@ -132,15 +135,16 @@ static void AddPartialReduce(SmallVector<SmallVector<Value>> &srcValues,
                              ArrayRef<Type> smemTypes, Value warpId,
                              Value laneIdAxis, Value parallelLaneId) {
   Location loc = helper.getLoc();
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   unsigned numParallelLane = helper.getNonAxisNumThreadsPerCTA();
   unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThread();
   unsigned parallelElementsPerThread = helper.getNonAxisNumElementsPerThread();
   unsigned elementStride = helper.getAxisElementStride();
   unsigned threadStride = helper.getAxisThreadStride();
   unsigned axisNumWarps = helper.getAxisNumWarpsWithUniqueData();
-  Value maskNotFirstWarp = icmp_ne(warpId, i32_val(0));
-  Value maskNotFirstLane = icmp_ne(laneIdAxis, i32_val(0));
-  Value maskNotFirstThread = or_(maskNotFirstWarp, maskNotFirstLane);
+  Value maskNotFirstWarp = b.icmp_ne(warpId, b.i32_val(0));
+  Value maskNotFirstLane = b.icmp_ne(laneIdAxis, b.i32_val(0));
+  Value maskNotFirstThread = b.or_(maskNotFirstWarp, maskNotFirstLane);
   struct Accumulator {
     SmallVector<Value> acc;
     SmallVector<Value> maskedAcc;
@@ -171,13 +175,14 @@ static void AddPartialReduce(SmallVector<SmallVector<Value>> &srcValues,
     Accumulator &accumulator = accumulators[accumulatorIndex];
     unsigned axisBlockId = (blockId / blockStride) % numScanBlocks;
     for (unsigned i = 0; i < axisNumWarps; ++i) {
-      Value index = add(parallelLaneId, i32_val(numParallelLane *
-                                                (i + chunkId * axisNumWarps)));
+      Value index =
+          b.add(parallelLaneId,
+                b.i32_val(numParallelLane * (i + chunkId * axisNumWarps)));
       SmallVector<Value> partialReduce(helper.getNumOperands());
       for (unsigned j = 0; j < helper.getNumOperands(); ++j) {
         auto elemTy = smemTypes[j];
-        Value ptr = gep(smemBases[j].getType(), elemTy, smemBases[j], index);
-        partialReduce[j] = load(elemTy, ptr);
+        Value ptr = b.gep(smemBases[j].getType(), elemTy, smemBases[j], index);
+        partialReduce[j] = b.load(elemTy, ptr);
       }
 
       if (accumulator.acc.size() == 0) {
@@ -185,12 +190,12 @@ static void AddPartialReduce(SmallVector<SmallVector<Value>> &srcValues,
         accumulator.maskedAcc = partialReduce;
         continue;
       }
-      Value mask = icmp_sge(warpId, i32_val(i + 1));
+      Value mask = b.icmp_sge(warpId, b.i32_val(i + 1));
       accumulator.acc =
           accumulate(helper, rewriter, accumulator.acc, partialReduce);
       for (unsigned j = 0; j < helper.getNumOperands(); ++j) {
         accumulator.maskedAcc[j] =
-            select(mask, accumulator.acc[j], accumulator.maskedAcc[j]);
+            b.select(mask, accumulator.acc[j], accumulator.maskedAcc[j]);
       }
     }
 
@@ -202,7 +207,7 @@ static void AddPartialReduce(SmallVector<SmallVector<Value>> &srcValues,
       // accumulate.
       auto val = srcValues[srcIndex];
       for (unsigned i = 0; i < helper.getNumOperands(); ++i) {
-        temp[i] = select(maskNotFirstWarp, temp[i], val[i]);
+        temp[i] = b.select(maskNotFirstWarp, temp[i], val[i]);
       }
     }
     srcValues[srcIndex] = temp;
@@ -210,7 +215,8 @@ static void AddPartialReduce(SmallVector<SmallVector<Value>> &srcValues,
     SmallVector<Value> lastElement(helper.getNumOperands());
     for (unsigned i = 0; i < helper.getNumOperands(); ++i) {
       auto elem = targetInfo.shuffleUp(rewriter, loc, temp[i], threadStride);
-      lastElement[i] = select(maskNotFirstLane, elem, accumulator.maskedAcc[i]);
+      lastElement[i] =
+          b.select(maskNotFirstLane, elem, accumulator.maskedAcc[i]);
     }
     for (unsigned i = 1; i < scanElementsPerThreads; ++i) {
       pred = axisBlockId == 0 ? maskNotFirstThread : Value{};
@@ -220,8 +226,8 @@ static void AddPartialReduce(SmallVector<SmallVector<Value>> &srcValues,
         // For the first warp and first chunk we don't have anything to
         // accumulate.
         for (unsigned j = 0; j < helper.getNumOperands(); ++j) {
-          laneValue[j] = select(maskNotFirstThread, laneValue[j],
-                                srcValues[srcIndex - i * elementStride][j]);
+          laneValue[j] = b.select(maskNotFirstThread, laneValue[j],
+                                  srcValues[srcIndex - i * elementStride][j]);
         }
       }
       srcValues[srcIndex - i * elementStride] = std::move(laneValue);
@@ -239,6 +245,7 @@ static void AddPartialReduceOneWarp(SmallVector<SmallVector<Value>> &srcValues,
                                     ScanLoweringHelper &helper, Value warpId,
                                     Value laneIdAxis, Value laneIdLast) {
   Location loc = helper.getLoc();
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThread();
   unsigned parallelElementsPerThread = helper.getNonAxisNumElementsPerThread();
   unsigned elementStride = helper.getAxisElementStride();
@@ -246,9 +253,9 @@ static void AddPartialReduceOneWarp(SmallVector<SmallVector<Value>> &srcValues,
   unsigned axisNumWarps = helper.getAxisNumWarpsWithUniqueData();
   unsigned numParallelLane = helper.getNonAxisNumThreadsPerCTA();
   unsigned scanDim = helper.getAxisNumThreadsPerWarpWithUniqueData();
-  Value maskFirstWarp = icmp_eq(warpId, i32_val(0));
-  Value maskFirstLane = icmp_eq(laneIdAxis, i32_val(0));
-  Value maskFirstThread = and_(maskFirstWarp, maskFirstLane);
+  Value maskFirstWarp = b.icmp_eq(warpId, b.i32_val(0));
+  Value maskFirstLane = b.icmp_eq(laneIdAxis, b.i32_val(0));
+  Value maskFirstThread = b.and_(maskFirstWarp, maskFirstLane);
   unsigned numScanBlocks = helper.getAxisNumBlocks();
   unsigned numParallelBlocks = helper.getNonAxisNumBlocks();
   assert(numScanBlocks * numParallelBlocks * parallelElementsPerThread *
@@ -282,7 +289,8 @@ static void AddPartialReduceOneWarp(SmallVector<SmallVector<Value>> &srcValues,
       for (unsigned i = 0; i < helper.getNumOperands(); ++i) {
         lastElement[i] = targetInfo.shuffleUp(
             rewriter, loc, srcValues[srcIndex][i], threadStride);
-        lastElement[i] = select(maskFirstLane, accumulator[i], lastElement[i]);
+        lastElement[i] =
+            b.select(maskFirstLane, accumulator[i], lastElement[i]);
         if (numScanBlocks > 1)
           // Update accumulator with the value from the last lane.
           accumulator[i] = targetInfo.shuffleIdx(
@@ -298,9 +306,9 @@ static void AddPartialReduceOneWarp(SmallVector<SmallVector<Value>> &srcValues,
         for (unsigned j = 0; j < helper.getNumOperands(); ++j) {
           // For the first warp and first chunk we don't have anything to
           // accumulate.
-          laneValue[j] =
-              select(maskFirstThread,
-                     srcValues[srcIndex - i * elementStride][j], laneValue[j]);
+          laneValue[j] = b.select(maskFirstThread,
+                                  srcValues[srcIndex - i * elementStride][j],
+                                  laneValue[j]);
         }
       }
       srcValues[srcIndex - i * elementStride] = std::move(laneValue);
@@ -384,6 +392,7 @@ ScanOpConversion::getDelinearizedIds(ConversionPatternRewriter &rewriter,
                                      ScanLoweringHelper &helper, Value laneId,
                                      Value warpId) const {
   auto loc = helper.getLoc();
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   unsigned axis = helper.getAxis();
   auto srcEncoding = helper.getEncoding();
 
@@ -399,17 +408,17 @@ ScanOpConversion::getDelinearizedIds(ConversionPatternRewriter &rewriter,
   Value laneIdAxis = multiDimLaneId[axis];
   Value warpIdAxis = multiDimWarpId[axis];
 
-  multiDimLaneId[axis] = i32_val(0);
+  multiDimLaneId[axis] = b.i32_val(0);
   threadsPerWarp[axis] = 1;
   Value laneIdParallel =
       linearize(rewriter, loc, multiDimLaneId, threadsPerWarp, order);
-  multiDimWarpId[axis] = i32_val(0);
+  multiDimWarpId[axis] = b.i32_val(0);
   warpsPerCTA[axis] = 1;
   Value warpIdParallel =
       linearize(rewriter, loc, multiDimWarpId, warpsPerCTA, warpOrder);
-  Value flatIdParallel =
-      add(laneIdParallel,
-          mul(warpIdParallel, i32_val(helper.getNonAxisNumThreadsPerWarp())));
+  Value flatIdParallel = b.add(
+      laneIdParallel,
+      b.mul(warpIdParallel, b.i32_val(helper.getNonAxisNumThreadsPerWarp())));
   return std::make_tuple(laneIdAxis, warpIdAxis, flatIdParallel);
 }
 
@@ -460,20 +469,21 @@ ScanOpConversion::emitFastScan(triton::ScanOp op, triton::ScanOpAdaptor adaptor,
                                const TargetInfoBase &targetInfo) const {
   ScanLoweringHelper helper(op);
   auto loc = helper.getLoc();
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   if (!helper.isSupported())
     return failure();
 
   Value threadId = getThreadId(rewriter, loc);
   auto mod = op->getParentOfType<ModuleOp>();
   unsigned iWarpSize = triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);
-  Value warpSize = i32_val(iWarpSize);
-  Value warpId = udiv(threadId, warpSize);
-  Value laneId = urem(threadId, warpSize);
+  Value warpSize = b.i32_val(iWarpSize);
+  Value warpId = b.udiv(threadId, warpSize);
+  Value laneId = b.urem(threadId, warpSize);
 
   auto [laneIdAxis, warpIdAxis, flatIdParallel] =
       getDelinearizedIds(rewriter, helper, laneId, warpId);
   auto axisNumWarps = helper.getAxisNumWarpsWithUniqueData();
-  warpIdAxis = urem(warpIdAxis, i32_val(axisNumWarps));
+  warpIdAxis = b.urem(warpIdAxis, b.i32_val(axisNumWarps));
   auto srcValues =
       unpackInputs(loc, op, adaptor, rewriter, *getTypeConverter());
 
@@ -485,7 +495,7 @@ ScanOpConversion::emitFastScan(triton::ScanOp op, triton::ScanOpAdaptor adaptor,
   // having to add a lot of the complex cross warp code (if rev switch
   // first/last etc). Reverse first seems more maintainable.)
   if (op.getReverse()) {
-    warpIdAxis = sub(i32_val(axisNumWarps - 1), warpIdAxis);
+    warpIdAxis = b.sub(b.i32_val(axisNumWarps - 1), warpIdAxis);
     srcValues =
         flipSrcValues(loc, op, rewriter, targetInfo, srcValues, iWarpSize);
   }
@@ -510,7 +520,7 @@ ScanOpConversion::emitFastScan(triton::ScanOp op, triton::ScanOpAdaptor adaptor,
     // Store the partial reducing for each warp into shared memory.
     storeWarpAccumulator(srcValues, rewriter, helper, laneIdAxis, warpIdAxis,
                          smemBases, smemTypes, flatIdParallel, targetInfo);
-    barrier();
+    b.barrier();
     // Read back the partial reduction of each warp and accumulate them based on
     // warpId. Then update each chunk of contiguous elements by adding the
     // accumulated value from the previous lane.
@@ -521,7 +531,7 @@ ScanOpConversion::emitFastScan(triton::ScanOp op, triton::ScanOpAdaptor adaptor,
     // the axis.
     unsigned scanDim = helper.getAxisNumThreadsPerWarpWithUniqueData();
     auto multiDimLaneId = getMultiDimLaneId(rewriter, helper, laneId);
-    multiDimLaneId[helper.getAxis()] = i32_val(scanDim - 1);
+    multiDimLaneId[helper.getAxis()] = b.i32_val(scanDim - 1);
     auto threadsPerWarp = triton::gpu::getThreadsPerWarp(helper.getEncoding());
     auto laneIdLast = linearize(rewriter, loc, multiDimLaneId, threadsPerWarp,
                                 triton::gpu::getOrder(helper.getEncoding()));
diff --git a/lib/Conversion/TritonGPUToLLVM/Utility.cpp b/lib/Conversion/TritonGPUToLLVM/Utility.cpp
index e857dd36f..f0eeb85a8 100644
--- a/lib/Conversion/TritonGPUToLLVM/Utility.cpp
+++ b/lib/Conversion/TritonGPUToLLVM/Utility.cpp
@@ -15,16 +15,17 @@ getMNCoords(Value thread, Location loc, RewriterBase &rewriter,
             ArrayRef<unsigned int> wpt, const NvidiaMmaEncodingAttr &mmaLayout,
             ArrayRef<int64_t> shape, bool isARow, bool isBRow, bool isAVec4,
             bool isBVec4) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   static constexpr std::array<int, 3> fpw{{2, 2, 1}};
 
   auto *ctx = thread.getContext();
-  Value _1 = i32_val(1);
-  Value _2 = i32_val(2);
-  Value _4 = i32_val(4);
-  Value _16 = i32_val(16);
-  Value _32 = i32_val(32);
-  Value _fpw0 = i32_val(fpw[0]);
-  Value _fpw1 = i32_val(fpw[1]);
+  Value _1 = b.i32_val(1);
+  Value _2 = b.i32_val(2);
+  Value _4 = b.i32_val(4);
+  Value _16 = b.i32_val(16);
+  Value _32 = b.i32_val(32);
+  Value _fpw0 = b.i32_val(fpw[0]);
+  Value _fpw1 = b.i32_val(fpw[1]);
 
   // A info
   auto aRep = mmaLayout.getMMAv1Rep(0);
@@ -37,57 +38,57 @@ getMNCoords(Value thread, Location loc, RewriterBase &rewriter,
   SmallVector<int, 2> spw({aSpw[0], bSpw[1]});
   SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});
 
-  Value lane = urem(thread, _32);
-  Value warp = udiv(thread, _32);
+  Value lane = b.urem(thread, _32);
+  Value warp = b.udiv(thread, _32);
 
-  Value warp0 = urem(warp, i32_val(wpt[0]));
-  Value warp12 = udiv(warp, i32_val(wpt[0]));
-  Value warp1 = urem(warp12, i32_val(wpt[1]));
+  Value warp0 = b.urem(warp, b.i32_val(wpt[0]));
+  Value warp12 = b.udiv(warp, b.i32_val(wpt[0]));
+  Value warp1 = b.urem(warp12, b.i32_val(wpt[1]));
 
   // warp offset
-  Value offWarpM = mul(warp0, i32_val(spw[0]));
-  Value offWarpN = mul(warp1, i32_val(spw[1]));
+  Value offWarpM = b.mul(warp0, b.i32_val(spw[0]));
+  Value offWarpN = b.mul(warp1, b.i32_val(spw[1]));
   // quad offset
-  Value offQuadM = mul(udiv(and_(lane, _16), _4), _fpw0);
-  Value offQuadN = mul(udiv(and_(lane, _16), _4), _fpw1);
+  Value offQuadM = b.mul(b.udiv(b.and_(lane, _16), _4), _fpw0);
+  Value offQuadN = b.mul(b.udiv(b.and_(lane, _16), _4), _fpw1);
   // pair offset
-  Value offPairM = udiv(urem(lane, _16), _4);
-  offPairM = urem(offPairM, _fpw0);
-  offPairM = mul(offPairM, _4);
-  Value offPairN = udiv(urem(lane, _16), _4);
-  offPairN = udiv(offPairN, _fpw0);
-  offPairN = urem(offPairN, _fpw1);
-  offPairN = mul(offPairN, _4);
+  Value offPairM = b.udiv(b.urem(lane, _16), _4);
+  offPairM = b.urem(offPairM, _fpw0);
+  offPairM = b.mul(offPairM, _4);
+  Value offPairN = b.udiv(b.urem(lane, _16), _4);
+  offPairN = b.udiv(offPairN, _fpw0);
+  offPairN = b.urem(offPairN, _fpw1);
+  offPairN = b.mul(offPairN, _4);
 
   // sclare
-  offPairM = mul(offPairM, i32_val(rep[0] / 2));
-  offQuadM = mul(offQuadM, i32_val(rep[0] / 2));
-  offPairN = mul(offPairN, i32_val(rep[1] / 2));
-  offQuadN = mul(offQuadN, i32_val(rep[1] / 2));
+  offPairM = b.mul(offPairM, b.i32_val(rep[0] / 2));
+  offQuadM = b.mul(offQuadM, b.i32_val(rep[0] / 2));
+  offPairN = b.mul(offPairN, b.i32_val(rep[1] / 2));
+  offQuadN = b.mul(offQuadN, b.i32_val(rep[1] / 2));
 
   // quad pair offset
-  Value offLaneM = add(offPairM, offQuadM);
-  Value offLaneN = add(offPairN, offQuadN);
+  Value offLaneM = b.add(offPairM, offQuadM);
+  Value offLaneN = b.add(offPairN, offQuadN);
   // a, b offset
-  Value offsetAM = add(offWarpM, offLaneM);
-  Value offsetBN = add(offWarpN, offLaneN);
+  Value offsetAM = b.add(offWarpM, offLaneM);
+  Value offsetBN = b.add(offWarpN, offLaneN);
   // m indices
-  Value offsetCM = add(and_(lane, _1), offsetAM);
+  Value offsetCM = b.add(b.and_(lane, _1), offsetAM);
   SmallVector<Value> idxM;
   for (unsigned m = 0; m < shape[0]; m += shapePerCTA[0])
     for (unsigned mm = 0; mm < rep[0]; ++mm)
-      idxM.push_back(add(offsetCM, i32_val(m + mm * 2)));
+      idxM.push_back(b.add(offsetCM, b.i32_val(m + mm * 2)));
 
   // n indices
-  Value offsetCN = add((and_(lane, _2)), (add(offWarpN, offPairN)));
+  Value offsetCN = b.add((b.and_(lane, _2)), (b.add(offWarpN, offPairN)));
   SmallVector<Value> idxN;
   for (int n = 0; n < shape[1]; n += shapePerCTA[1]) {
     for (int nn = 0; nn < rep[1]; ++nn) {
-      idxN.push_back(add(
-          offsetCN, i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1])));
+      idxN.push_back(b.add(
+          offsetCN, b.i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1])));
       idxN.push_back(
-          add(offsetCN,
-              i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1] + 1)));
+          b.add(offsetCN,
+                b.i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1] + 1)));
     }
   }
 
@@ -147,6 +148,7 @@ SmallVector<std::pair<StringAttr, Value>>
 applyLinearLayout(Location loc, RewriterBase &rewriter,
                   const LinearLayout &layout,
                   ArrayRef<std::pair<StringAttr, Value>> indices) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   assert(layout.getNumInDims() == indices.size());
   for (auto [inDimName, idx] : indices) {
     assert(layout.hasInDim(inDimName) && "Invalid inDimName");
@@ -173,13 +175,13 @@ applyLinearLayout(Location loc, RewriterBase &rewriter,
   SmallVector<int32_t> constantComponent =
       llvm::to_vector(llvm::make_second_range(layout.apply(constantIns)));
 
-  Value zero = i32_val(0);
+  Value zero = b.i32_val(0);
   SmallVector<std::pair<StringAttr, Value>> outIndices;
   for (auto [i, outDimName] : llvm::enumerate(layout.getOutDimNames())) {
     if (constantComponent[i] == 0)
       outIndices.push_back({outDimName, zero});
     else
-      outIndices.push_back({outDimName, i32_val(constantComponent[i])});
+      outIndices.push_back({outDimName, b.i32_val(constantComponent[i])});
   }
 
   for (auto [inDimName, idx] : indices) {
@@ -189,13 +191,13 @@ applyLinearLayout(Location loc, RewriterBase &rewriter,
 
     int nBits = layout.getInDimSizeLog2(inDimName);
     for (int i = 0; i < nBits; i++) {
-      Value bit = and_(idx, i32_val(1 << i));
-      Value bit_is_zero = icmp_eq(bit, zero);
+      Value bit = b.and_(idx, b.i32_val(1 << i));
+      Value bit_is_zero = b.icmp_eq(bit, zero);
       for (auto &[outDimName, outIdx] : outIndices) {
         int32_t basis = layout.getBasis(inDimName, i, outDimName);
         if (basis == 0)
           continue;
-        outIdx = xor_(outIdx, select(bit_is_zero, zero, i32_val(basis)));
+        outIdx = b.xor_(outIdx, b.select(bit_is_zero, zero, b.i32_val(basis)));
       }
     }
   }
@@ -206,6 +208,7 @@ applyLinearLayout(Location loc, RewriterBase &rewriter,
 SmallVector<SmallVector<Value>>
 emitIndices(Location loc, RewriterBase &rewriter, const TargetInfoBase &target,
             Attribute layout, RankedTensorType type, bool withCTAOffset) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   MLIRContext *ctx = rewriter.getContext();
   auto shape = type.getShape();
 
@@ -221,11 +224,11 @@ emitIndices(Location loc, RewriterBase &rewriter, const TargetInfoBase &target,
   StringAttr kBlock = str_attr("block");
 
   Value threadId = getThreadId(rewriter, loc);
-  Value threadsPerWarp = i32_val(ll->getInDimSize(kLane));
-  Value laneId = urem(threadId, threadsPerWarp);
-  Value warpId = udiv(threadId, threadsPerWarp);
+  Value threadsPerWarp = b.i32_val(ll->getInDimSize(kLane));
+  Value laneId = b.urem(threadId, threadsPerWarp);
+  Value warpId = b.udiv(threadId, threadsPerWarp);
   Value blockId =
-      withCTAOffset ? target.getClusterCTAId(rewriter, loc) : i32_val(0);
+      withCTAOffset ? target.getClusterCTAId(rewriter, loc) : b.i32_val(0);
   unsigned rank = shape.size();
   SmallVector<SmallVector<Value>> ret;
   // Linear layout function is split in two parts below:
@@ -238,7 +241,7 @@ emitIndices(Location loc, RewriterBase &rewriter, const TargetInfoBase &target,
   // This approach produces code with lower register pressure and
   // less computations, compared to fused L(r,t,w,b) method.
   auto idxsBase = applyLinearLayout(loc, rewriter, *ll,
-                                    {{kRegister, i32_val(0)},
+                                    {{kRegister, b.i32_val(0)},
                                      {kLane, laneId},
                                      {kWarp, warpId},
                                      {kBlock, blockId}});
@@ -250,7 +253,7 @@ emitIndices(Location loc, RewriterBase &rewriter, const TargetInfoBase &target,
       auto dimName = idxBase.first;
       assert(dimName == idxReg.first &&
              "dim names of block+warp+thread and register idx should be equal");
-      auto idx = xor_(idxBase.second, i32_val(idxReg.second));
+      auto idx = b.xor_(idxBase.second, b.i32_val(idxReg.second));
       idxs.emplace_back(dimName, idx);
     }
     assert(idxs.size() == rank);
@@ -270,6 +273,7 @@ bool emitTransferBetweenRegistersAndShared(
     const TargetInfoBase &target,
     std::function<void(VectorType, Value /*shmemAddr*/)> perVectorCallback) {
   MLIRContext *ctx = rewriter.getContext();
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
 
   auto shape = registerTy.getShape();
   int rank = shape.size();
@@ -343,14 +347,14 @@ bool emitTransferBetweenRegistersAndShared(
                maxVecElems.value_or(std::numeric_limits<int>::max()));
 
   Value threadId = getThreadId(rewriter, loc);
-  Value threadsPerWarp = i32_val(regToSharedLayout.getInDimSize(kLane));
-  Value laneId = urem(threadId, threadsPerWarp);
-  Value warpId = udiv(threadId, threadsPerWarp);
+  Value threadsPerWarp = b.i32_val(regToSharedLayout.getInDimSize(kLane));
+  Value laneId = b.urem(threadId, threadsPerWarp);
+  Value warpId = b.udiv(threadId, threadsPerWarp);
 
   int numElems = regToSharedLayout.getInDimSize(kRegister);
   auto vecTy = vec_ty(elemLlvmTy, vecElems);
   auto ptrTy = shmemBase.getType();
-  Value zero = i32_val(0);
+  Value zero = b.i32_val(0);
   SmallVector<Value> ret;
   for (int i = 0; i < numElems / vecElems; i++) {
     // Get the address to load/store.  The multi-dim address is (offsetX1, ...,
@@ -359,7 +363,7 @@ bool emitTransferBetweenRegistersAndShared(
     auto multiDimShmemOffset =
         llvm::to_vector(llvm::drop_end(llvm::make_second_range(
             applyLinearLayout(loc, rewriter, regToSharedLayout,
-                              {{kRegister, i32_val(i * vecElems)},
+                              {{kRegister, b.i32_val(i * vecElems)},
                                {kLane, laneId},
                                {kWarp, warpId},
                                {kBlock, zero}}))));
@@ -368,7 +372,7 @@ bool emitTransferBetweenRegistersAndShared(
     // multi-dimensional offsets in regToSharedLayout.
     Value shmemOffset = dot(rewriter, loc, multiDimShmemOffset,
                             applyPermutation(shmemStrides, sharedOrder));
-    auto vecAddr = gep(ptrTy, elemLlvmTy, shmemBase, shmemOffset);
+    auto vecAddr = b.gep(ptrTy, elemLlvmTy, shmemBase, shmemOffset);
     vecAddr.setInbounds(true);
 
     perVectorCallback(vecTy, vecAddr);
@@ -381,17 +385,18 @@ SmallVector<Value> loadSharedToDistributed(RankedTensorType dstTy,
                                            SharedMemoryObject smemObj,
                                            Location loc, RewriterBase &rewriter,
                                            const TargetInfoBase &target) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   SmallVector<Value> ret;
   bool success = emitTransferBetweenRegistersAndShared(
       dstTy, srcTy, elemLlvmTy, /*maxVecElems=*/std::nullopt, smemObj.getBase(),
       smemObj.getStrides(), loc, rewriter, target,
       [&](VectorType vecTy, Value vecAddr) {
-        auto vecVal = load(vecTy, vecAddr);
+        auto vecVal = b.load(vecTy, vecAddr);
         vecVal.setAlignment(vecTy.getNumElements() *
                             elemLlvmTy.getIntOrFloatBitWidth() / 8);
 
         for (int v = 0; v < vecTy.getNumElements(); v++) {
-          ret.push_back(extract_element(elemLlvmTy, vecVal, i32_val(v)));
+          ret.push_back(b.extract_element(elemLlvmTy, vecVal, b.i32_val(v)));
         }
       });
   if (!success)
@@ -405,17 +410,18 @@ void storeDistributedToShared(MemDescType dstTy, RankedTensorType srcTy,
                               Value smemBase, ArrayRef<Value> dstStrides,
                               Location loc, RewriterBase &rewriter,
                               const TargetInfoBase &target) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   bool success = emitTransferBetweenRegistersAndShared(
       srcTy, dstTy, elemLlvmTy, /*maxVecElems=*/std::nullopt, smemBase,
       dstStrides, loc, rewriter, target, [&](VectorType vecTy, Value vecAddr) {
         ArrayRef<Value> vals = srcVals.take_front(vecTy.getNumElements());
         srcVals = srcVals.drop_front(vecTy.getNumElements());
 
-        Value vec = undef(vecTy);
+        Value vec = b.undef(vecTy);
         for (int i = 0; i < vals.size(); i++) {
-          vec = insert_element(vec, vals[i], i32_val(i));
+          vec = b.insert_element(vec, vals[i], b.i32_val(i));
         }
-        store(vec, vecAddr)
+        b.store(vec, vecAddr)
             .setAlignment(vecTy.getNumElements() *
                           elemLlvmTy.getIntOrFloatBitWidth() / 8);
       });
@@ -551,12 +557,13 @@ SharedMemoryObject getSharedMemoryObjectFromStruct(Location loc,
                                                    Value llvmStruct,
                                                    Type elemTy,
                                                    RewriterBase &rewriter) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   ArrayRef<Type> types =
       cast<LLVM::LLVMStructType>(llvmStruct.getType()).getBody();
   SmallVector<Value> elems(types.size());
   for (unsigned i = 0; i < types.size(); ++i) {
     Type type = types[i];
-    elems[i] = extract_val(type, llvmStruct, i);
+    elems[i] = b.extract_val(type, llvmStruct, i);
   }
 
   auto rank = (elems.size() - 1) / 2;
@@ -570,11 +577,12 @@ SmallVector<Value> getStridesFromShapeAndOrder(ArrayRef<int64_t> shape,
                                                ArrayRef<unsigned> order,
                                                Location loc,
                                                RewriterBase &rewriter) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto rank = shape.size();
   SmallVector<Value> strides(rank);
   int64_t stride = 1;
   for (auto idx : order) {
-    strides[idx] = i32_val(stride);
+    strides[idx] = b.i32_val(stride);
     stride *= shape[idx];
   }
   return strides;
@@ -606,13 +614,14 @@ SmallVector<Value> delinearize(RewriterBase &rewriter, Location loc,
 
 SmallVector<Value> delinearize(RewriterBase &rewriter, Location loc,
                                unsigned linear, ArrayRef<unsigned> shape) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   unsigned rank = shape.size();
   assert(rank > 0);
   SmallVector<Value> multiDim(rank);
   unsigned remained = linear;
   for (auto &&en : llvm::enumerate(shape)) {
     unsigned dimSize = en.value();
-    multiDim[en.index()] = i32_val(remained % dimSize);
+    multiDim[en.index()] = b.i32_val(remained % dimSize);
     remained = remained / dimSize;
   }
   return multiDim;
@@ -620,14 +629,15 @@ SmallVector<Value> delinearize(RewriterBase &rewriter, Location loc,
 
 SmallVector<Value> delinearize(RewriterBase &rewriter, Location loc,
                                Value linear, ArrayRef<unsigned> shape) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   unsigned rank = shape.size();
   assert(rank > 0);
   SmallVector<Value> multiDim(rank);
   Value remained = linear;
   for (auto &&en : llvm::enumerate(shape)) {
-    Value dimSize = i32_val(en.value());
-    multiDim[en.index()] = urem(remained, dimSize);
-    remained = udiv(remained, dimSize);
+    Value dimSize = b.i32_val(en.value());
+    multiDim[en.index()] = b.urem(remained, dimSize);
+    remained = b.udiv(remained, dimSize);
   }
   return multiDim;
 }
@@ -640,14 +650,15 @@ Value linearize(RewriterBase &rewriter, Location loc, ArrayRef<Value> multiDim,
 
 Value linearize(RewriterBase &rewriter, Location loc, ArrayRef<Value> multiDim,
                 ArrayRef<unsigned> shape) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto rank = multiDim.size();
-  Value linear = i32_val(0);
+  Value linear = b.i32_val(0);
   if (rank > 0) {
     linear = multiDim.back();
     for (auto [dim, dimShape] :
          llvm::reverse(llvm::zip(multiDim.drop_back(), shape.drop_back()))) {
-      Value dimSize = i32_val(dimShape);
-      linear = add(mul(linear, dimSize), dim);
+      Value dimSize = b.i32_val(dimShape);
+      linear = b.add(b.mul(linear, dimSize), dim);
     }
   }
   return linear;
@@ -655,6 +666,7 @@ Value linearize(RewriterBase &rewriter, Location loc, ArrayRef<Value> multiDim,
 
 Value addStringToModule(Location loc, RewriterBase &rewriter, StringRef key,
                         StringRef content) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto moduleOp = rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();
   auto ctx = moduleOp.getContext();
   unsigned stringNumber = 0;
@@ -678,12 +690,12 @@ Value addStringToModule(Location loc, RewriterBase &rewriter, StringRef key,
         rewriter.getStringAttr(contentStr));
   }
 
-  Value zero = i32_val(0);
+  Value zero = b.i32_val(0);
   Type globalPtrType = LLVM::LLVMPointerType::get(ctx, global.getAddrSpace());
   Value globalPtr = rewriter.create<LLVM::AddressOfOp>(
       UnknownLoc::get(ctx), globalPtrType, global.getSymName());
   Value stringStart =
-      gep(ptr_ty(ctx), i8_ty, globalPtr, SmallVector<Value>({zero}));
+      b.gep(ptr_ty(ctx), i8_ty, globalPtr, SmallVector<Value>({zero}));
   return stringStart;
 }
 
@@ -693,6 +705,7 @@ SmallVector<Value> getMultiDimOffset(Attribute layout, Location loc,
                                      unsigned elemId, RankedTensorType type,
                                      ArrayRef<unsigned> multiDimCTAInRepId,
                                      ArrayRef<unsigned> shapePerCTATile) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto shape = type.getShape();
   unsigned rank = shape.size();
   if (auto blockedLayout = dyn_cast<BlockedEncodingAttr>(layout)) {
@@ -703,9 +716,9 @@ SmallVector<Value> getMultiDimOffset(Attribute layout, Location loc,
         elemId, getSizePerThread(layout), getOrder(layout));
     for (unsigned d = 0; d < rank; ++d) {
       multiDimOffset[d] =
-          add(multiDimOffsetFirstElem[d],
-              i32_val(multiDimCTAInRepId[d] * shapePerCTATile[d] +
-                      multiDimElemId[d]));
+          b.add(multiDimOffsetFirstElem[d],
+                b.i32_val(multiDimCTAInRepId[d] * shapePerCTATile[d] +
+                          multiDimElemId[d]));
     }
     return multiDimOffset;
   }
@@ -745,40 +758,42 @@ SmallVector<Value> getMultiDimOffset(Attribute layout, Location loc,
     SmallVector<Value> mmaColIdx(2);
     SmallVector<Value> mmaRowIdx(2);
     Value threadId = getThreadId(rewriter, loc);
-    Value warpSize = i32_val(32);
-    Value laneId = urem(threadId, warpSize);
-    Value warpId = udiv(threadId, warpSize);
+    Value warpSize = b.i32_val(32);
+    Value laneId = b.urem(threadId, warpSize);
+    Value warpId = b.udiv(threadId, warpSize);
     // TODO: fix the bug in MMAEncodingAttr document
     SmallVector<Value> multiDimWarpId(2);
     auto warpsPerCTA = mmaLayout.getWarpsPerCTA();
     auto warpOrder = triton::gpu::getWarpOrder(mmaLayout);
     multiDimWarpId = delinearize(rewriter, loc, warpId, warpsPerCTA, warpOrder);
-    Value _1 = i32_val(1);
-    Value _2 = i32_val(2);
-    Value _4 = i32_val(4);
-    Value _8 = i32_val(8);
-    Value _16 = i32_val(16);
+    Value _1 = b.i32_val(1);
+    Value _2 = b.i32_val(2);
+    Value _4 = b.i32_val(4);
+    Value _8 = b.i32_val(8);
+    Value _16 = b.i32_val(16);
     if (mmaLayout.isAmpere() || mmaLayout.isHopper()) {
-      multiDimWarpId[rank - 1] = urem(
-          multiDimWarpId[rank - 1],
-          i32_val(ceil<unsigned>(shapePerCTA[rank - 1], instrShape[rank - 1])));
-      multiDimWarpId[rank - 2] = urem(
-          multiDimWarpId[rank - 2],
-          i32_val(ceil<unsigned>(shapePerCTA[rank - 2], instrShape[rank - 2])));
-
-      Value mmaGrpId = udiv(laneId, _4);
-      Value mmaGrpIdP8 = add(mmaGrpId, _8);
-      Value mmaThreadIdInGrp = urem(laneId, _4);
-      Value mmaThreadIdInGrpM2 = mul(mmaThreadIdInGrp, _2);
-      Value mmaThreadIdInGrpM2P1 = add(mmaThreadIdInGrpM2, _1);
+      multiDimWarpId[rank - 1] =
+          b.urem(multiDimWarpId[rank - 1],
+                 b.i32_val(ceil<unsigned>(shapePerCTA[rank - 1],
+                                          instrShape[rank - 1])));
+      multiDimWarpId[rank - 2] =
+          b.urem(multiDimWarpId[rank - 2],
+                 b.i32_val(ceil<unsigned>(shapePerCTA[rank - 2],
+                                          instrShape[rank - 2])));
+
+      Value mmaGrpId = b.udiv(laneId, _4);
+      Value mmaGrpIdP8 = b.add(mmaGrpId, _8);
+      Value mmaThreadIdInGrp = b.urem(laneId, _4);
+      Value mmaThreadIdInGrpM2 = b.mul(mmaThreadIdInGrp, _2);
+      Value mmaThreadIdInGrpM2P1 = b.add(mmaThreadIdInGrpM2, _1);
       Value rowWarpOffset =
-          mul(multiDimWarpId[rank - 2], i32_val(instrShape[rank - 2]));
-      mmaRowIdx[0] = add(mmaGrpId, rowWarpOffset);
-      mmaRowIdx[1] = add(mmaGrpIdP8, rowWarpOffset);
+          b.mul(multiDimWarpId[rank - 2], b.i32_val(instrShape[rank - 2]));
+      mmaRowIdx[0] = b.add(mmaGrpId, rowWarpOffset);
+      mmaRowIdx[1] = b.add(mmaGrpIdP8, rowWarpOffset);
       Value colWarpOffset =
-          mul(multiDimWarpId[rank - 1], i32_val(instrShape[rank - 1]));
-      mmaColIdx[0] = add(mmaThreadIdInGrpM2, colWarpOffset);
-      mmaColIdx[1] = add(mmaThreadIdInGrpM2P1, colWarpOffset);
+          b.mul(multiDimWarpId[rank - 1], b.i32_val(instrShape[rank - 1]));
+      mmaColIdx[0] = b.add(mmaThreadIdInGrpM2, colWarpOffset);
+      mmaColIdx[1] = b.add(mmaThreadIdInGrpM2P1, colWarpOffset);
     } else if (mmaLayout.isVolta()) {
       // Volta doesn't follow the pattern here.
     } else {
@@ -791,24 +806,26 @@ SmallVector<Value> getMultiDimOffset(Attribute layout, Location loc,
       unsigned nGrpId = elemId / 4;
       multiDimOffset[0] = elemIdRem4 < 2 ? mmaRowIdx[0] : mmaRowIdx[1];
       multiDimOffset[1] = elemIdRem4 % 2 == 0 ? mmaColIdx[0] : mmaColIdx[1];
-      multiDimOffset[1] = add(multiDimOffset[1], i32_val(8 * nGrpId));
-      multiDimOffset[0] = add(multiDimOffset[0], i32_val(multiDimCTAInRepId[0] *
-                                                         shapePerCTATile[0]));
-      multiDimOffset[1] = add(multiDimOffset[1], i32_val(multiDimCTAInRepId[1] *
-                                                         shapePerCTATile[1]));
+      multiDimOffset[1] = b.add(multiDimOffset[1], b.i32_val(8 * nGrpId));
+      multiDimOffset[0] =
+          b.add(multiDimOffset[0],
+                b.i32_val(multiDimCTAInRepId[0] * shapePerCTATile[0]));
+      multiDimOffset[1] =
+          b.add(multiDimOffset[1],
+                b.i32_val(multiDimCTAInRepId[1] * shapePerCTATile[1]));
     } else if (mmaLayout.isAmpere()) {
       if (rank == 3)
         multiDimOffset[0] =
-            add(multiDimWarpId[0],
-                i32_val(multiDimCTAInRepId[0] * shapePerCTATile[0]));
+            b.add(multiDimWarpId[0],
+                  b.i32_val(multiDimCTAInRepId[0] * shapePerCTATile[0]));
       multiDimOffset[rank - 2] = elemId < 2 ? mmaRowIdx[0] : mmaRowIdx[1];
       multiDimOffset[rank - 1] = elemId % 2 == 0 ? mmaColIdx[0] : mmaColIdx[1];
       multiDimOffset[rank - 2] =
-          add(multiDimOffset[rank - 2], i32_val(multiDimCTAInRepId[rank - 2] *
-                                                shapePerCTATile[rank - 2]));
+          b.add(multiDimOffset[rank - 2], b.i32_val(multiDimCTAInRepId[rank - 2] *
+                                                    shapePerCTATile[rank - 2]));
       multiDimOffset[rank - 1] =
-          add(multiDimOffset[rank - 1], i32_val(multiDimCTAInRepId[rank - 1] *
-                                                shapePerCTATile[rank - 1]));
+          b.add(multiDimOffset[rank - 1], b.i32_val(multiDimCTAInRepId[rank - 1] *
+                                                    shapePerCTATile[rank - 1]));
     } else if (mmaLayout.isVolta()) {
       auto [isARow, isBRow, isAVec4, isBVec4, _] =
           mmaLayout.decodeVoltaLayoutStates();
@@ -834,8 +851,8 @@ SmallVector<Value> getMultiDimOffset(Attribute layout, Location loc,
       emitWmmaOffsetForCTA(wmmaLayout, offsets, 0, multiDimCTAInRepId[0],
                            multiDimCTAInRepId[1]);
     }
-    multiDimOffset[0] = add(multiDimBase[0], i32_val(offsets[elemId][0]));
-    multiDimOffset[1] = add(multiDimBase[1], i32_val(offsets[elemId][1]));
+    multiDimOffset[0] = b.add(multiDimBase[0], b.i32_val(offsets[elemId][0]));
+    multiDimOffset[1] = b.add(multiDimBase[1], b.i32_val(offsets[elemId][1]));
     return multiDimOffset;
   }
   llvm_unreachable("unexpected layout in getMultiDimOffset");
@@ -845,11 +862,12 @@ SmallVector<Value> getWrappedMultiDimOffset(
     RewriterBase &rewriter, Location loc, ArrayRef<Value> multiDimOffset,
     ArrayRef<unsigned> shape, SmallVector<unsigned> shapePerCTATile,
     SmallVector<int64_t> shapePerCTA) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   unsigned rank = shape.size();
   SmallVector<Value> multiDimOffsetWrapped(rank);
   for (unsigned d = 0; d < rank; ++d) {
     if (shapePerCTATile[d] > shapePerCTA[d])
-      multiDimOffsetWrapped[d] = urem(multiDimOffset[d], i32_val(shape[d]));
+      multiDimOffsetWrapped[d] = b.urem(multiDimOffset[d], b.i32_val(shape[d]));
     else
       multiDimOffsetWrapped[d] = multiDimOffset[d];
   }
diff --git a/lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp b/lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp
index 297a94e85..5708626fe 100644
--- a/lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp
+++ b/lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp
@@ -20,6 +20,7 @@ struct SplatOpConversion : public ConvertOpToLLVMPattern<triton::SplatOp> {
                                   const LLVMTypeConverter *typeConverter,
                                   ConversionPatternRewriter &rewriter,
                                   Location loc) {
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto tensorTy = cast<RankedTensorType>(resType);
     // Check the converted type for the tensor as depending on the encoding the
     // converter may pick different element types.
@@ -35,13 +36,13 @@ struct SplatOpConversion : public ConvertOpToLLVMPattern<triton::SplatOp> {
       unsigned ratio = srcBitWidth / cstBitWidth;
       Type intTy = IntegerType::get(elemType.getContext(), cstBitWidth);
       VectorType vecType = VectorType::get(ratio, intTy);
-      Value intCst = bitcast(constVal, intTy);
-      Value vec = undef(vecType);
+      Value intCst = b.bitcast(constVal, intTy);
+      Value vec = b.undef(vecType);
       for (unsigned i = 0; i < ratio; ++i)
-        vec = insert_element(vecType, vec, intCst, int_val(32, i));
+        vec = b.insert_element(vecType, vec, intCst, b.int_val(32, i));
       constVal = vec;
     }
-    auto llSrc = bitcast(constVal, srcType);
+    auto llSrc = b.bitcast(constVal, srcType);
     size_t elemsPerThread = getTotalElemsPerThread(tensorTy);
     llvm::SmallVector<Value> elems(elemsPerThread, llSrc);
     return packLLElements(loc, typeConverter, elems, rewriter, resType);
@@ -369,6 +370,7 @@ struct MemDescSubviewOpConversion
                   ConversionPatternRewriter &rewriter) const override {
     // %dst = extract_slice %src[%offsets]
     Location loc = op->getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto srcTy = op.getSrc().getType();
     auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());
 
@@ -389,7 +391,7 @@ struct MemDescSubviewOpConversion
     auto offset = dot(rewriter, loc, opOffsetVals, smemObj.strides);
     auto elemPtrTy = smemObj.base.getType();
     smemObj =
-        SharedMemoryObject(gep(elemPtrTy, llvmElemTy, smemObj.base, offset),
+        SharedMemoryObject(b.gep(elemPtrTy, llvmElemTy, smemObj.base, offset),
                            llvmElemTy, strides, offsetVals);
     auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);
     rewriter.replaceOp(op, retVal);
diff --git a/third_party/amd/lib/TritonAMDGPUToLLVM/BufferOpsEmitter.cpp b/third_party/amd/lib/TritonAMDGPUToLLVM/BufferOpsEmitter.cpp
index 37bdb8fe9..6a5bcb236 100644
--- a/third_party/amd/lib/TritonAMDGPUToLLVM/BufferOpsEmitter.cpp
+++ b/third_party/amd/lib/TritonAMDGPUToLLVM/BufferOpsEmitter.cpp
@@ -40,6 +40,7 @@ BufferEmitter::BufferEmitter(RewriterBase &rw, Location loc, TargetInfo ti)
     : rewriter(rw), loc(loc), targetInfo(ti) {}
 
 Value BufferEmitter::createResourceDescriptor(Value basePtr) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   // 1. Create the resource descriptor
   // bits 0-11: dst sel, ignored by these intrinsics
   // bits 12-14: data format (ignored, must be nonzero, 7=float)
@@ -64,10 +65,11 @@ Value BufferEmitter::createResourceDescriptor(Value basePtr) {
     uint32_t oob = 3;
     flags |= (oob << 28);
   }
-  Value stride = int_val(16, 0);
-  Value flagsConst = int_val(32, flags);
+  Value stride = b.int_val(16, 0);
+  Value flagsConst = b.int_val(32, flags);
+
   Type rsrcType = LLVM::LLVMPointerType::get(rewriter.getContext(), 8);
-  Value numRecordsByte = int_val(32, std::numeric_limits<int>::max() - 1);
+  Value numRecordsByte = b.int_val(32, std::numeric_limits<int>::max() - 1);
 
   Value resource = rewriter.createOrFold<ROCDL::MakeBufferRsrcOp>(
       loc, rsrcType, basePtr, stride, numRecordsByte, flagsConst);
@@ -76,23 +78,25 @@ Value BufferEmitter::createResourceDescriptor(Value basePtr) {
 
 Value BufferEmitter::emitLoad(Type type, Value rsrcDesc, Value offset,
                               Value pred, Value falseVal) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   SmallVector<Value, 6> args;
   fillCommonArgs(type, rsrcDesc, offset, pred, args);
   Type bufferType = getBufferOpType(type);
   Value data = rewriter.create<ROCDL::RawPtrBufferLoadOp>(
       loc, bufferType, args, ArrayRef<NamedAttribute>());
-  data = bitcast(data, type);
+  data = b.bitcast(data, type);
   if (!isZero(falseVal))
-    data = select(pred, data, falseVal);
+    data = b.select(pred, data, falseVal);
   return data;
 }
 
 void BufferEmitter::emitStore(Value rsrcDesc, Value offset, Value data,
                               Value pred) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   VectorType vecTy = cast<VectorType>(data.getType());
   Type bufferType = getBufferOpType(vecTy);
   if (vecTy != bufferType)
-    data = bitcast(data, bufferType);
+    data = b.bitcast(data, bufferType);
   SmallVector<Value, 6> args{data};
   fillCommonArgs(vecTy, rsrcDesc, offset, pred, args);
   rewriter.create<ROCDL::RawPtrBufferStoreOp>(loc, TypeRange{}, args,
@@ -144,7 +148,7 @@ Type BufferEmitter::getBufferOpType(Type type) {
 void BufferEmitter::fillCommonArgs(Type type, Value rsrcDesc,
                                    Value vOffsetElems, Value pred,
                                    SmallVector<Value> &args) {
-
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   // 1. Create the (masked) offset
   Type elementType = getElementTypeOrSelf(type);
   const int valueElemNBits = std::max(8u, elementType.getIntOrFloatBitWidth());
@@ -152,19 +156,19 @@ void BufferEmitter::fillCommonArgs(Type type, Value rsrcDesc,
   // Please note: the index passed is not in bytes, but in number of elements
   // In order to pass the index to the buffer operation, we need to convert in
   // bytes (i.e., we need to multiply by `elementByteWidth`)
-  Value vOffsetOutOfBunds = int_val(
+  Value vOffsetOutOfBunds = b.int_val(
       32, static_cast<int>(std::numeric_limits<int>::max() + int64_t(1)));
-  Value vOffsetBytes = mul(int_val(32, elementByteWidth), vOffsetElems);
-  Value maskedOffsetBytes = select(pred, vOffsetBytes, vOffsetOutOfBunds);
+  Value vOffsetBytes = b.mul(b.int_val(32, elementByteWidth), vOffsetElems);
+  Value maskedOffsetBytes = b.select(pred, vOffsetBytes, vOffsetOutOfBunds);
 
   // 2. Set the sgprOffset to 0
-  Value sgprOffset = int_val(32, 0);
+  Value sgprOffset = b.int_val(32, 0);
 
   // 3. Create the cache modifiers word
   // bit 0: GLC = 0 (atomics drop value, less coherency)
   // bits 1-2: SLC, DLC = 0 (similarly)
   // bit 3: swizzled (0 for raw)
-  Value cacheModifiers = int_val(32, 0);
+  Value cacheModifiers = b.int_val(32, 0);
 
   // 5. Add the arguments
   args.push_back(rsrcDesc);
diff --git a/third_party/amd/lib/TritonAMDGPUToLLVM/ConvertLayoutOpToLLVM.cpp b/third_party/amd/lib/TritonAMDGPUToLLVM/ConvertLayoutOpToLLVM.cpp
index b7ee4efc7..9a54e4a4a 100644
--- a/third_party/amd/lib/TritonAMDGPUToLLVM/ConvertLayoutOpToLLVM.cpp
+++ b/third_party/amd/lib/TritonAMDGPUToLLVM/ConvertLayoutOpToLLVM.cpp
@@ -61,6 +61,7 @@ private:
       ConversionPatternRewriter &rewriter,
       const DotOperandEncodingAttr &dotOperandLayout, bool isOuter) const {
     auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     Value src = op.getSrc();
     Value dst = op.getResult();
     auto llvmElemTy = typeConverter->convertType(
@@ -77,7 +78,7 @@ private:
                                     : SharedToDotOperandWMMA::convertLayout;
       res = sharedToDotConvert(dotOperandLayout.getOpIdx(), rewriter, loc, src,
                                dotOperandLayout, smemObj, typeConverter,
-                               tid_val());
+                               b.tid_val());
     } else {
       assert(false && "unsupported layout found");
     }
diff --git a/third_party/amd/lib/TritonAMDGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandHelper.cpp b/third_party/amd/lib/TritonAMDGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandHelper.cpp
index 03b7c56b7..29fc3da5d 100644
--- a/third_party/amd/lib/TritonAMDGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandHelper.cpp
+++ b/third_party/amd/lib/TritonAMDGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandHelper.cpp
@@ -9,11 +9,12 @@ Value getWarpIdInBlock(ConversionPatternRewriter &rewriter, Location loc,
                        Value warpId, const ArrayRef<unsigned int> &wpt,
                        int elemPerInstrNonK, int tensorSizeNonK, int nonKIdx,
                        const ArrayRef<unsigned int> &order) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   SmallVector<Value> multiDimWarpId =
       delinearize(rewriter, loc, warpId, wpt, order);
 
-  return urem(multiDimWarpId[nonKIdx],
-              i32_val(tensorSizeNonK / elemPerInstrNonK));
+  return b.urem(multiDimWarpId[nonKIdx],
+                b.i32_val(tensorSizeNonK / elemPerInstrNonK));
 }
 
 bool isSwizzled(SharedEncodingAttr layout) { return layout.getMaxPhase() != 1; }
@@ -21,6 +22,7 @@ bool isSwizzled(SharedEncodingAttr layout) { return layout.getMaxPhase() != 1; }
 std::pair<mlir::Value, mlir::Value>
 swizzleIndexes(ConversionPatternRewriter &rewriter, Location loc, Value row,
                Value col, SharedMemoryObject smemObj, SharedEncodingAttr attr) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   (void)smemObj; // unused in current pattern
   const auto &order = attr.getOrder();
   auto rank = order.size();
@@ -29,9 +31,9 @@ swizzleIndexes(ConversionPatternRewriter &rewriter, Location loc, Value row,
     // tensor is column-wise, so swapping col and row in computations
     std::swap(row, col);
   }
-  auto vec = i32_val(attr.getVec());
-  auto perPhase = i32_val(attr.getPerPhase());
-  auto maxPhase = i32_val(attr.getMaxPhase());
+  auto vec = b.i32_val(attr.getVec());
+  auto perPhase = b.i32_val(attr.getPerPhase());
+  auto maxPhase = b.i32_val(attr.getMaxPhase());
 
   // Original algorithm taken from getSwizzledSharedPtrs function
   // (TritonGPUToLLVMBase.h): Basic algorithm for row-major tensor is following:
@@ -40,10 +42,10 @@ swizzleIndexes(ConversionPatternRewriter &rewriter, Location loc, Value row,
   // colOffSwizzled = ((col // vec) ^ phase) * vec
   // colOffOrdered = col % vec
   // colOff = colOffSwizzled + colOffOrdered
-  auto phase = urem(udiv(row, perPhase), maxPhase);
-  auto colOffSwizzled = mul(xor_(udiv(col, vec), phase), vec);
-  auto colOffOrdered = urem(col, vec);
-  auto colOff = add(colOffSwizzled, colOffOrdered);
+  auto phase = b.urem(b.udiv(row, perPhase), maxPhase);
+  auto colOffSwizzled = b.mul(b.xor_(b.udiv(col, vec), phase), vec);
+  auto colOffOrdered = b.urem(col, vec);
+  auto colOff = b.add(colOffSwizzled, colOffOrdered);
 
   if (transposed)
     return {colOff, row};
@@ -54,24 +56,26 @@ swizzleIndexes(ConversionPatternRewriter &rewriter, Location loc, Value row,
 Value computeOffset(ConversionPatternRewriter &rewriter, Location loc,
                     Value row, Value col, SharedMemoryObject smemObj,
                     SharedEncodingAttr srcLayout) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto [swizzledRow, swizzledCol] =
       swizzleIndexes(rewriter, loc, row, col, smemObj, srcLayout);
   const auto &strides = smemObj.getStrides();
   auto rank = strides.size();
   assert(rank == 2 || rank == 3);
-  Value rowOffset = mul(swizzledRow, strides[rank - 2]);
-  Value colOffset = mul(swizzledCol, strides[rank - 1]);
-  return add(rowOffset, colOffset);
+  Value rowOffset = b.mul(swizzledRow, strides[rank - 2]);
+  Value colOffset = b.mul(swizzledCol, strides[rank - 1]);
+  return b.add(rowOffset, colOffset);
 }
 
 Value computeBasePtr(ConversionPatternRewriter &rewriter, Location loc,
                      const SharedMemoryObject &smemObj) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   Value base = smemObj.base;
   Type type = base.getType();
   Type elemType = smemObj.getBaseElemType();
   for (int i = 0; i < smemObj.strides.size(); ++i) {
-    Value offset = sub(i32_val(0), mul(smemObj.offsets[i], smemObj.strides[i]));
-    base = gep(type, elemType, base, offset);
+    Value offset = b.sub(b.i32_val(0), b.mul(smemObj.offsets[i], smemObj.strides[i]));
+    base = b.gep(type, elemType, base, offset);
   }
   return base;
 }
@@ -123,6 +127,7 @@ llvm::SmallVector<Value> computeOffsetsAType(
     ArrayRef<int64_t> reps, SharedMemoryObject smemObj,
     SharedEncodingAttr srcLayout, unsigned nonKDim, unsigned kDim) {
   SmallVector<Value> strides = smemObj.getStrides();
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   SmallVector<Value> offsets = smemObj.getOffsets();
   auto rank = offsets.size();
 
@@ -146,7 +151,7 @@ llvm::SmallVector<Value> computeOffsetsAType(
     for (int block = 0; block < numBlocks; ++block) {
       int blockNonKOffset = block * nonKDim * warpsPerBlock;
       for (int i = 0; i < blockSize; ++i) {
-        Value row = add(mapping[i][0], i32_val(blockNonKOffset));
+        Value row = b.add(mapping[i][0], b.i32_val(blockNonKOffset));
         Value col = mapping[i][1];
         aOffsets[block * blockSize + i] =
             computeOffset(rewriter, loc, row, col, smemObj, srcLayout);
@@ -163,9 +168,9 @@ llvm::SmallVector<Value> computeOffsetsAType(
     }
     for (int block = 0; block < numBlocks; ++block) {
       int blockNonKOffset = block * nonKDim * warpsPerBlock;
-      Value offAdjust = mul(i32_val(blockNonKOffset), strides[rank - 2]);
+      Value offAdjust = b.mul(b.i32_val(blockNonKOffset), strides[rank - 2]);
       for (int i = 0; i < blockSize; ++i)
-        aOffsets[block * blockSize + i] = add(offAdjust, inblockOffset[i]);
+        aOffsets[block * blockSize + i] = b.add(offAdjust, inblockOffset[i]);
     }
   }
   return aOffsets;
@@ -188,6 +193,7 @@ llvm::SmallVector<Value> computeOffsetsBType(
     Value warpId, Value laneId, int warpsPerBlock, int numOfElems,
     ArrayRef<int64_t> reps, SharedMemoryObject smemObj,
     SharedEncodingAttr srcLayout, unsigned nonKDim, unsigned kDim) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   // transpose reps and offsets, because operand B has layout equal to
   // transposed operand A layout
   // this unifies axis order, so non-K dim is 0, k dim is 1
@@ -220,7 +226,7 @@ llvm::SmallVector<Value> computeOffsetsBType(
         // swap row and col, because operand B layout is
         // a transposed operand A layout
         Value row = mapping[i][1];
-        Value col = add(mapping[i][0], i32_val(blockNonKOffset));
+        Value col = b.add(mapping[i][0], b.i32_val(blockNonKOffset));
         bOffsets[block * blockSize + i] =
             computeOffset(rewriter, loc, row, col, smemObj, srcLayout);
       }
@@ -238,9 +244,9 @@ llvm::SmallVector<Value> computeOffsetsBType(
     }
     for (int block = 0; block < numBlocks; ++block) {
       int blockNonKOffset = block * nonKDim * warpsPerBlock;
-      Value offAdjust = mul(i32_val(blockNonKOffset), tStrides[rank - 2]);
+      Value offAdjust = b.mul(b.i32_val(blockNonKOffset), tStrides[rank - 2]);
       for (int i = 0; i < mapping.size(); ++i)
-        bOffsets[block * blockSize + i] = add(offAdjust, inblockOffset[i]);
+        bOffsets[block * blockSize + i] = b.add(offAdjust, inblockOffset[i]);
     }
   }
   return bOffsets;
diff --git a/third_party/amd/lib/TritonAMDGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMFMA.cpp b/third_party/amd/lib/TritonAMDGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMFMA.cpp
index b832d985b..be0e75305 100644
--- a/third_party/amd/lib/TritonAMDGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMFMA.cpp
+++ b/third_party/amd/lib/TritonAMDGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMFMA.cpp
@@ -74,47 +74,48 @@ llvm::SmallVector<llvm::SmallVector<Value>> computeTensorElemMappingInBlock(
     const ArrayRef<int64_t> &elemsPerInstr, Value warpId, Value laneId,
     int numOfElems, ArrayRef<int64_t> reps, ArrayRef<Value> smemOffsets,
     int loadVecSize, unsigned iNonKDim, unsigned iKDim) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto numM = reps[1];
   auto numK = reps[2];
   const int loadsPerThread = numOfElems / loadVecSize;
   llvm::SmallVector<llvm::SmallVector<Value>> mapping(numK * loadsPerThread);
 
-  Value _0 = i32_val(0);
-  Value _32 = i32_val(32);
-  Value nonKDim = i32_val(iNonKDim);
-  Value warpVOffset = mul(warpId, i32_val(elemsPerInstr[0]));
+  Value _0 = b.i32_val(0);
+  Value _32 = b.i32_val(32);
+  Value nonKDim = b.i32_val(iNonKDim);
+  Value warpVOffset = b.mul(warpId, b.i32_val(elemsPerInstr[0]));
 
   auto rank = smemOffsets.size();
 
   for (int tile = 0; tile < numK; ++tile) {
     Value tileVOffset = _0;
-    Value tileHOffset = i32_val(tile * elemsPerInstr[1]);
+    Value tileHOffset = b.i32_val(tile * elemsPerInstr[1]);
 
-    Value laneVOffset = urem(laneId, nonKDim);
+    Value laneVOffset = b.urem(laneId, nonKDim);
     Value laneHOffset;
     if (iNonKDim == 32)
-      laneHOffset = select(icmp_uge(laneId, _32), i32_val(numOfElems), _0);
+      laneHOffset = b.select(b.icmp_uge(laneId, _32), b.i32_val(numOfElems), _0);
     else {
       // In this configuration warp contains 16 copies of same data
       if ((iKDim == 1 || iKDim == 4) && iNonKDim == 4) {
-        laneHOffset = i32_val(0);
+        laneHOffset = b.i32_val(0);
       } else {
         assert(iKDim * iNonKDim / numOfElems == 64 &&
                "seems no all threads in warp contain unique elements");
-        laneHOffset = mul(udiv(laneId, nonKDim), i32_val(numOfElems));
+        laneHOffset = b.mul(b.udiv(laneId, nonKDim), b.i32_val(numOfElems));
       }
     }
 
     for (int loadId = 0; loadId < loadsPerThread; ++loadId) {
       Value elemVOffset = _0;
-      Value elemHOffset = i32_val(loadId * loadVecSize);
+      Value elemHOffset = b.i32_val(loadId * loadVecSize);
 
-      Value sliceVOffset =
-          add(add(add(tileVOffset, laneVOffset), elemVOffset), warpVOffset);
-      Value sliceHOffset = add(add(tileHOffset, laneHOffset), elemHOffset);
+      Value sliceVOffset = b.add(
+          b.add(b.add(tileVOffset, laneVOffset), elemVOffset), warpVOffset);
+      Value sliceHOffset = b.add(b.add(tileHOffset, laneHOffset), elemHOffset);
 
-      Value row = add(sliceVOffset, smemOffsets[rank - 2]);
-      Value col = add(sliceHOffset, smemOffsets[rank - 1]);
+      Value row = b.add(sliceVOffset, smemOffsets[rank - 2]);
+      Value col = b.add(sliceHOffset, smemOffsets[rank - 1]);
 
       mapping[loadsPerThread * tile + loadId] = {row, col};
     }
@@ -142,6 +143,7 @@ fastPathComputeOffsets(ConversionPatternRewriter &rewriter, Location loc,
                        const ArrayRef<int64_t> &elemsPerInstr, Value warpId,
                        Value laneId, int warpsPerBlock, int numOfElems,
                        ArrayRef<int64_t> reps, Value cSwizzleOffset) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto numK = reps[1];
   auto numN = reps[2];
   SmallVector<Value> offsets(numK * numN * numOfElems);
@@ -149,14 +151,14 @@ fastPathComputeOffsets(ConversionPatternRewriter &rewriter, Location loc,
   auto iKDim = elemsPerInstr[0];
   auto iNonKDim = elemsPerInstr[1];
   int lineSize = warpsPerBlock * iNonKDim * numN;
-  Value _nonKDim = i32_val(iNonKDim);
-  Value warpOffset = mul(warpId, i32_val(iNonKDim));
-  Value colOffset = urem(laneId, _nonKDim);
+  Value _nonKDim = b.i32_val(iNonKDim);
+  Value warpOffset = b.mul(warpId, b.i32_val(iNonKDim));
+  Value colOffset = b.urem(laneId, _nonKDim);
 
   for (int block = 0; block < numN; ++block) {
-    Value blockOffset = i32_val(block * iNonKDim * warpsPerBlock);
+    Value blockOffset = b.i32_val(block * iNonKDim * warpsPerBlock);
     for (int tile = 0; tile < numK; ++tile) {
-      Value tileOffset = i32_val(tile * iKDim * lineSize);
+      Value tileOffset = b.i32_val(tile * iKDim * lineSize);
       for (int elem = 0; elem < numOfElems; ++elem) {
         // halfOffset is an offset related to wrapping of warp in the tile.
         // for example, mfma 32 case (mapping of tensor elements to lane ids in
@@ -172,14 +174,14 @@ fastPathComputeOffsets(ConversionPatternRewriter &rewriter, Location loc,
         // 32 33 34 35 ... 63
         Value halfOffset;
         if ((iKDim == 1 || iKDim == 4) && iNonKDim == 4)
-          halfOffset = i32_val(0);
+          halfOffset = b.i32_val(0);
         else
           halfOffset =
-              mul(udiv(laneId, _nonKDim), i32_val(numOfElems * lineSize));
-        Value rowOffset = add(i32_val(elem * lineSize), halfOffset);
-        Value elemOffset = add(rowOffset, colOffset);
-        Value offset =
-            add(add(add(warpOffset, blockOffset), tileOffset), elemOffset);
+              b.mul(b.udiv(laneId, _nonKDim), b.i32_val(numOfElems * lineSize));
+        Value rowOffset = b.add(b.i32_val(elem * lineSize), halfOffset);
+        Value elemOffset = b.add(rowOffset, colOffset);
+        Value offset = b.add(b.add(b.add(warpOffset, blockOffset), tileOffset),
+                             elemOffset);
         offsets[numK * numOfElems * block + numOfElems * tile + elem] = offset;
       }
     }
@@ -196,6 +198,7 @@ Value convertLayout(int opIdx, ConversionPatternRewriter &rewriter,
                     Location loc, Value tensor, DotOperandEncodingAttr encoding,
                     const SharedMemoryObject &smemObj,
                     const LLVMTypeConverter *typeConverter, Value thread) {
+  auto tb = TritonLLVMOpBuilder(loc, rewriter);
   assert((opIdx == 0 || opIdx == 1) && "unexpected operand idx");
   auto aTensorTy = cast<MemDescType>(tensor.getType());
   ArrayRef<int64_t> shape = aTensorTy.getShape();
@@ -250,9 +253,9 @@ Value convertLayout(int opIdx, ConversionPatternRewriter &rewriter,
 
   unsigned iWarpSize = triton::gpu::getWarpSize(mfmaLayout);
   assert(iWarpSize == 64);
-  Value warpSize = i32_val(iWarpSize);
-  Value linearWarpId = udiv(thread, warpSize);
-  Value lane = urem(thread, warpSize);
+  Value warpSize = tb.i32_val(iWarpSize);
+  Value linearWarpId = tb.udiv(thread, warpSize);
+  Value lane = tb.urem(thread, warpSize);
 
   Value spatialWarpId = AMD::getWarpIdInBlock(
       rewriter, loc, linearWarpId, warpsPerCTA, mfmaInstrNonK,
@@ -271,7 +274,7 @@ Value convertLayout(int opIdx, ConversionPatternRewriter &rewriter,
   int warpsPerBlockNonK = std::min(warpsPerCTA[nonKDimIdx], maxNumWarps);
   int warpsPerBatch =
       rank == 3 ? std::min<unsigned>(shape[0], warpsPerCTA[0]) : 1;
-  Value warpIdInBatch = urem(linearWarpId, i32_val(warpsPerBatch));
+  Value warpIdInBatch = tb.urem(linearWarpId, tb.i32_val(warpsPerBatch));
   elemTy = typeConverter->convertType(elemTy);
 
   SmallVector<Value> loadedValues;
@@ -338,11 +341,12 @@ Value convertLayout(int opIdx, ConversionPatternRewriter &rewriter,
 
   for (int b = 0; b < repB; ++b) {
     int operandSize = shape[rank - 1] * shape[rank - 2];
-    Value batchOffset = mul(i32_val(operandSize),
-                            add(warpIdInBatch, i32_val(b * warpsPerBatch)));
+    Value batchOffset =
+        tb.mul(tb.i32_val(operandSize),
+               tb.add(warpIdInBatch, tb.i32_val(b * warpsPerBatch)));
     for (int nonK = 0; nonK < numRepNonK; ++nonK) {
       int blockNonKOffset = nonK * mfmaInstrNonK * warpsPerBlockNonK;
-      Value warpBlockOffAdjust = i32_val(blockNonKOffset * shape[order[0]]);
+      Value warpBlockOffAdjust = tb.i32_val(blockNonKOffset * shape[order[0]]);
       for (int k = 0; k < numRepK; ++k) {
         auto vecTy = vec_ty(resElemTy, numOfElems);
         for (unsigned loadId = 0; loadId < loadsPerThread; ++loadId) {
@@ -350,12 +354,12 @@ Value convertLayout(int opIdx, ConversionPatternRewriter &rewriter,
           Value loadOffset;
           loadOffset = offsets[nonK * loadsPerThread * numRepK +
                                k * loadsPerThread + loadId];
-          loadOffset = add(loadOffset, batchOffset);
-          Value loadAddress = gep(smemPtrTy, elemTy, smemBase, loadOffset);
-          Value loadedValue = load(loadVecTy, loadAddress);
+          loadOffset = tb.add(loadOffset, batchOffset);
+          Value loadAddress = tb.gep(smemPtrTy, elemTy, smemBase, loadOffset);
+          Value loadedValue = tb.load(loadVecTy, loadAddress);
           for (int elemId = 0; elemId < elemsPerLoad; ++elemId) {
             Value elemVal =
-                extract_element(elemTy, loadedValue, i32_val(elemId));
+                tb.extract_element(elemTy, loadedValue, tb.i32_val(elemId));
             loadedValues.push_back(elemVal);
           }
         }
diff --git a/third_party/amd/lib/TritonAMDGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandWMMA.cpp b/third_party/amd/lib/TritonAMDGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandWMMA.cpp
index b60c86e1a..d14488491 100644
--- a/third_party/amd/lib/TritonAMDGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandWMMA.cpp
+++ b/third_party/amd/lib/TritonAMDGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandWMMA.cpp
@@ -71,26 +71,27 @@ computeTensorElemMappingInBlockWmma1(
     const ArrayRef<int64_t> &elemsPerInstr, Value warpId, Value laneId,
     int numOfElems, ArrayRef<int64_t> reps, ArrayRef<Value> smemOffsets,
     int loadVecSize, unsigned iNonKDim, [[maybe_unused]] unsigned iKDim) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   assert(reps.size() == 3);
   assert(elemsPerInstr.size() == 2);
   auto numK = reps[2];
   const int loadsPerThread = numOfElems / loadVecSize;
   llvm::SmallVector<llvm::SmallVector<Value>> mapping(numK * loadsPerThread);
 
-  Value elemsPerInstrV = i32_val(elemsPerInstr[0]);
-  Value warpVOffset = mul(warpId, elemsPerInstrV);
-  Value sliceVOffset = add(urem(laneId, elemsPerInstrV), warpVOffset);
+  Value elemsPerInstrV = b.i32_val(elemsPerInstr[0]);
+  Value warpVOffset = b.mul(warpId, elemsPerInstrV);
+  Value sliceVOffset = b.add(b.urem(laneId, elemsPerInstrV), warpVOffset);
   auto rank = smemOffsets.size();
-  Value row = add(sliceVOffset, smemOffsets[rank - 2]);
+  Value row = b.add(sliceVOffset, smemOffsets[rank - 2]);
 
   for (int tile = 0; tile < numK; ++tile) {
-    Value tileHOffset = i32_val(tile * elemsPerInstr[1]);
+    Value tileHOffset = b.i32_val(tile * elemsPerInstr[1]);
 
     for (int loadId = 0; loadId < loadsPerThread; ++loadId) {
-      Value elemHOffset = i32_val(loadId * loadVecSize);
-      Value sliceHOffset = add(tileHOffset, elemHOffset);
+      Value elemHOffset = b.i32_val(loadId * loadVecSize);
+      Value sliceHOffset = b.add(tileHOffset, elemHOffset);
 
-      Value col = add(sliceHOffset, smemOffsets[rank - 1]);
+      Value col = b.add(sliceHOffset, smemOffsets[rank - 1]);
       mapping[loadsPerThread * tile + loadId] = {row, col};
     }
   }
@@ -104,29 +105,30 @@ computeTensorElemMappingInBlockWmma2(
     const ArrayRef<int64_t> &elemsPerInstr, Value warpId, Value laneId,
     int numOfElems, ArrayRef<int64_t> reps, ArrayRef<Value> smemOffsets,
     int loadVecSize, unsigned iNonKDim, [[maybe_unused]] unsigned iKDim) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   assert(reps.size() == 3);
   assert(elemsPerInstr.size() == 2);
   auto numK = reps[2];
   const int loadsPerThread = numOfElems / loadVecSize;
   llvm::SmallVector<llvm::SmallVector<Value>> mapping(numK * loadsPerThread);
 
-  Value rowsPerInstr = i32_val(elemsPerInstr[0]);
-  Value colsPerInstr = i32_val(elemsPerInstr[1]);
-  Value elemsPerThread = i32_val(elemsPerInstr[1] / 2);
-  Value warpVOffset = mul(warpId, rowsPerInstr);
-  Value sliceVOffset = add(urem(laneId, rowsPerInstr), warpVOffset);
+  Value rowsPerInstr = b.i32_val(elemsPerInstr[0]);
+  Value colsPerInstr = b.i32_val(elemsPerInstr[1]);
+  Value elemsPerThread = b.i32_val(elemsPerInstr[1] / 2);
+  Value warpVOffset = b.mul(warpId, rowsPerInstr);
+  Value sliceVOffset = b.add(b.urem(laneId, rowsPerInstr), warpVOffset);
 
   auto rank = smemOffsets.size();
-  Value row = add(sliceVOffset, smemOffsets[rank - 2]);
-  Value laneHOffset = mul(udiv(laneId, colsPerInstr), elemsPerThread);
+  Value row = b.add(sliceVOffset, smemOffsets[rank - 2]);
+  Value laneHOffset = b.mul(b.udiv(laneId, colsPerInstr), elemsPerThread);
 
   for (int tile = 0; tile < numK; ++tile) {
-    Value tileHOffset = add(laneHOffset, i32_val(tile * elemsPerInstr[1]));
+    Value tileHOffset = b.add(laneHOffset, b.i32_val(tile * elemsPerInstr[1]));
     for (int loadId = 0; loadId < loadsPerThread; ++loadId) {
-      Value elemHOffset = i32_val(loadId * loadVecSize);
-      Value sliceHOffset = add(tileHOffset, elemHOffset);
+      Value elemHOffset = b.i32_val(loadId * loadVecSize);
+      Value sliceHOffset = b.add(tileHOffset, elemHOffset);
 
-      Value col = add(sliceHOffset, smemOffsets[rank - 1]);
+      Value col = b.add(sliceHOffset, smemOffsets[rank - 1]);
 
       mapping[loadsPerThread * tile + loadId] = {row, col};
     }
@@ -139,6 +141,7 @@ Value convertLayout(int opIdx, ConversionPatternRewriter &rewriter,
                     Location loc, Value tensor, DotOperandEncodingAttr encoding,
                     const SharedMemoryObject &smemObj,
                     const LLVMTypeConverter *typeConverter, Value thread) {
+  auto tb = TritonLLVMOpBuilder(loc, rewriter);
   assert((opIdx == 0 || opIdx == 1) && "unexpected operand idx");
   auto rank = smemObj.getStrides().size();
   int kDimIdx = opIdx == 0 ? rank - 1 : rank - 2;
@@ -172,18 +175,18 @@ Value convertLayout(int opIdx, ConversionPatternRewriter &rewriter,
 
   unsigned iWaveSize = triton::gpu::getWarpSize(wmmaLayout);
   assert(iWaveSize == 32);
-  Value waveSize = i32_val(iWaveSize);
-  Value linearWaveId = udiv(thread, waveSize);
+  Value waveSize = tb.i32_val(iWaveSize);
+  Value linearWaveId = tb.udiv(thread, waveSize);
 
   unsigned numElemsPerThreadPerRep =
       wmmaLayout.getSizePerThreadForOperand(kWidth, opIdx)[kDimIdx];
 
-  Value lane = urem(thread, waveSize);
+  Value lane = tb.urem(thread, waveSize);
   unsigned int maxNumWarps = shape[nonKDimIdx] / wmmaInstrNonK;
   int warpsPerBlockNonK = std::min(warpsPerCTA[nonKDimIdx], maxNumWarps);
   int warpsPerBatch =
       rank == 3 ? std::min<unsigned>(shape[0], warpsPerCTA[0]) : 1;
-  Value waveIdInBatch = urem(linearWaveId, i32_val(warpsPerBatch));
+  Value waveIdInBatch = tb.urem(linearWaveId, tb.i32_val(warpsPerBatch));
   elemTy = typeConverter->convertType(elemTy);
 
   SmallVector<Value> loadedValues;
@@ -214,22 +217,23 @@ Value convertLayout(int opIdx, ConversionPatternRewriter &rewriter,
   assert(numElemsPerThreadPerRep % loadsPerThread == 0);
   for (int b = 0; b < repB; ++b) {
     int operandSize = shape[rank - 1] * shape[rank - 2];
-    Value batchOffset = mul(i32_val(operandSize),
-                            add(waveIdInBatch, i32_val(b * warpsPerBatch)));
+    Value batchOffset =
+        tb.mul(tb.i32_val(operandSize),
+               tb.add(waveIdInBatch, tb.i32_val(b * warpsPerBatch)));
     for (int nonK = 0; nonK < numRepNonK; ++nonK) {
       for (int k = 0; k < numRepK; ++k) {
         auto vecTy = vec_ty(resElemTy, numElemsPerThreadPerRep);
-        Value valVec = undef(vecTy);
+        Value valVec = tb.undef(vecTy);
         for (unsigned loadId = 0; loadId < loadsPerThread; ++loadId) {
           auto loadVecTy = vec_ty(elemTy, elemsPerLoad);
           Value loadOffset = offsets[nonK * loadsPerThread * numRepK +
                                      k * loadsPerThread + loadId];
-          loadOffset = add(loadOffset, batchOffset);
-          Value loadAddress = gep(smemPtrTy, elemTy, smemBase, loadOffset);
-          Value loadedValue = load(loadVecTy, loadAddress);
+          loadOffset = tb.add(loadOffset, batchOffset);
+          Value loadAddress = tb.gep(smemPtrTy, elemTy, smemBase, loadOffset);
+          Value loadedValue = tb.load(loadVecTy, loadAddress);
           for (int elemId = 0; elemId < elemsPerLoad; ++elemId) {
             Value elemVal =
-                extract_element(elemTy, loadedValue, i32_val(elemId));
+                tb.extract_element(elemTy, loadedValue, tb.i32_val(elemId));
             loadedValues.push_back(elemVal);
           }
         }
diff --git a/third_party/amd/lib/TritonAMDGPUToLLVM/DotOpToLLVM/MFMA.cpp b/third_party/amd/lib/TritonAMDGPUToLLVM/DotOpToLLVM/MFMA.cpp
index 204d54894..29b9505ef 100644
--- a/third_party/amd/lib/TritonAMDGPUToLLVM/DotOpToLLVM/MFMA.cpp
+++ b/third_party/amd/lib/TritonAMDGPUToLLVM/DotOpToLLVM/MFMA.cpp
@@ -62,8 +62,9 @@ struct DotOpMFMAConversionHelper {
 
   Value generateMFMAOp(StringRef mfmaInsnName, Value valA, Value valB,
                        Value valC) const {
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto resType = valC.getType();
-    Value zeroFlag = i32_val(0);
+    Value zeroFlag = b.i32_val(0);
     OperationState loweredOp(loc, mfmaInsnName);
     loweredOp.addTypes(resType);
     loweredOp.addOperands({valA, valB, valC, zeroFlag, zeroFlag, zeroFlag});
@@ -94,6 +95,7 @@ struct DotOpMFMAConversionHelper {
 
   Value processSubBlocks(int numSubBlocks, Value acc, bool reduceSubBlocks,
                          bool zeroSubBlocks) const {
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     assert((numSubBlocks & (numSubBlocks - 1)) == 0 &&
            "numSubBlocks in not pow 2!");
     if (numSubBlocks == 1)
@@ -101,14 +103,14 @@ struct DotOpMFMAConversionHelper {
     constexpr int warpSize = 64;
     int subBlockSize = warpSize / numSubBlocks;
     Value laneId = getThreadId();
-    laneId = and_(laneId, i32_val(warpSize - 1));
+    laneId = b.and_(laneId, b.i32_val(warpSize - 1));
     auto vecTy = dyn_cast<VectorType>(acc.getType());
     auto elemType = vecTy.getElementType();
     assert(elemType.getIntOrFloatBitWidth() == 32);
     int numScalars = vecTy.getNumElements();
     std::vector<Value> accScalar(numScalars);
     for (int i = 0; i < numScalars; ++i)
-      accScalar[i] = extract_element(elemType, acc, i32_val(i));
+      accScalar[i] = b.extract_element(elemType, acc, b.i32_val(i));
 
     if (reduceSubBlocks) {
       while (subBlockSize < warpSize) {
@@ -116,9 +118,9 @@ struct DotOpMFMAConversionHelper {
           Value other_acc =
               shuffleXor(loc, rewriter, accScalar[i], subBlockSize);
           if (elemType.isInteger(32))
-            accScalar[i] = add(accScalar[i], other_acc);
+            accScalar[i] = b.add(accScalar[i], other_acc);
           else
-            accScalar[i] = fadd(accScalar[i], other_acc);
+            accScalar[i] = b.fadd(accScalar[i], other_acc);
         }
         subBlockSize *= 2;
       }
@@ -126,17 +128,18 @@ struct DotOpMFMAConversionHelper {
     if (zeroSubBlocks) {
       Value zero;
       if (elemType.isInteger(32))
-        zero = i32_val(0);
+        zero = b.i32_val(0);
       else
-        zero = f32_val(0.0);
-      auto cond = icmp_ult(laneId, i32_val(subBlockSize));
+        zero = b.f32_val(0.0);
+      auto cond = b.icmp_ult(laneId, b.i32_val(subBlockSize));
       for (int i = 0; i < numScalars; ++i)
-        accScalar[i] = select(cond, accScalar[i], zero);
+        accScalar[i] = b.select(cond, accScalar[i], zero);
     }
 
-    Value reducedAcc = undef(vecTy);
+    Value reducedAcc = b.undef(vecTy);
     for (int i = 0; i < numScalars; ++i)
-      reducedAcc = insert_element(vecTy, reducedAcc, accScalar[i], i32_val(i));
+      reducedAcc =
+          b.insert_element(vecTy, reducedAcc, accScalar[i], b.i32_val(i));
     return reducedAcc;
   }
 
@@ -164,6 +167,8 @@ struct DotOpMFMAConversionHelper {
 
   // Conduct the Dot conversion.
   LogicalResult convertDot(DotOp op, DotOpAdaptor adaptor) const {
+    auto tb = TritonLLVMOpBuilder(loc, rewriter);
+
     auto warpsPerCTA = mfmaLayout.getWarpsPerCTA();
     auto mDim = mfmaLayout.getMDim();
     auto nDim = mfmaLayout.getNDim();
@@ -230,13 +235,13 @@ struct DotOpMFMAConversionHelper {
     for (int b = 0; b < numRepB; ++b) {
       for (int m = 0; m < numRepM; ++m) {
         for (int n = 0; n < numRepN; ++n) {
-          Value acc = undef(vecTy);
+          Value acc = tb.undef(vecTy);
           for (unsigned v = 0; v < elemsPerVec; ++v) {
-            acc = insert_element(
+            acc = tb.insert_element(
                 vecTy, acc,
                 fc[b * numRepM * numRepN * elemsPerVec +
                    m * numRepN * elemsPerVec + n * elemsPerVec + v],
-                i32_val(v));
+                tb.i32_val(v));
           }
           acc = zeroAuxiliarBlocks(subBlocks, acc);
           for (int k = 0; k < numRepK; k++) {
@@ -252,7 +257,7 @@ struct DotOpMFMAConversionHelper {
           for (unsigned v = 0; v < elemsPerVec; ++v) {
             fc[b * numRepM * numRepN * elemsPerVec + m * numRepN * elemsPerVec +
                n * elemsPerVec + v] =
-                extract_element(dstElemTy, acc, i32_val(v));
+                tb.extract_element(dstElemTy, acc, tb.i32_val(v));
           }
         }
       }
@@ -273,28 +278,30 @@ struct DotOpMFMAConversionHelper {
    */
   SmallVector<Value> extractOperands(Value rawElems, int kWidth, int kBase,
                                      Type type) const {
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     int kpack = kWidth / kBase;
     SmallVector<Value> results;
     auto vecTy = vec_ty(type, kBase);
     if (type.isBF16())
       vecTy = vec_ty(i16_ty, kBase);
     for (int k = 0; k < kpack; ++k) {
-      Value vec = undef(vecTy);
+      Value vec = b.undef(vecTy);
       for (int elemId = 0; elemId < kBase; ++elemId) {
-        auto val = extract_element(type, rawElems, i32_val(elemId + k * kBase));
+        auto val =
+            b.extract_element(type, rawElems, b.i32_val(elemId + k * kBase));
         if (type.isBF16()) {
           // rocdl.mfma.f32.32x32x8bf16.1k calls for input of i16 type
-          auto cast = bitcast(val, i16_ty);
-          vec = insert_element(vecTy, vec, cast, i32_val(elemId));
+          auto cast = b.bitcast(val, i16_ty);
+          vec = b.insert_element(vecTy, vec, cast, b.i32_val(elemId));
         } else
-          vec = insert_element(vecTy, vec, val, i32_val(elemId));
+          vec = b.insert_element(vecTy, vec, val, b.i32_val(elemId));
       }
       if (type.getIntOrFloatBitWidth() == 8) {
         if (4 == kBase)
           // This is for int8 on pre- MI300 GPUs
-          results.push_back(bitcast(vec, i32_ty));
+          results.push_back(b.bitcast(vec, i32_ty));
         if (8 == kBase)
-          results.push_back(bitcast(vec, i64_ty));
+          results.push_back(b.bitcast(vec, i64_ty));
       } else
         results.push_back(vec);
     }
@@ -308,6 +315,7 @@ struct DotOpMFMAConversionHelper {
   SmallVector<ValueTable>
   getValuesFromDotOperandLayoutStruct(Value value, int batch, int n0, int n1,
                                       int kWidth, int kBase, Type type) const {
+    auto tb = TritonLLVMOpBuilder(loc, rewriter);
     auto elems = unpackLLElements(loc, value, rewriter);
     int kpack = kWidth / kBase;
     SmallVector<ValueTable> dotOpVals(kpack);
@@ -316,19 +324,19 @@ struct DotOpMFMAConversionHelper {
         for (int j = 0; j < n1; j++) {
           Type elemTy = typeConverter->convertType(type);
           Type ty = vec_ty(elemTy, kWidth);
-          Value rawElems = undef(ty);
+          Value rawElems = tb.undef(ty);
           for (int k = 0; k < kWidth; ++k) {
-            rawElems = insert_element(
+            rawElems = tb.insert_element(
                 ty, rawElems,
                 elems[kWidth * n1 * n0 * b + kWidth * n1 * i + kWidth * j + k],
-                i32_val(k));
+                tb.i32_val(k));
           }
 
           Value convertedElems;
           if (type.isF32()) {
             for (int k = 0; k < kpack; ++k)
               dotOpVals[k][{b, i, j}] =
-                  extract_element(type, rawElems, i32_val(k));
+                  tb.extract_element(type, rawElems, tb.i32_val(k));
           } else {
             SmallVector<Value> vals;
             if (type.getIntOrFloatBitWidth() == 8) {
diff --git a/third_party/amd/lib/TritonAMDGPUToLLVM/DotOpToLLVM/WMMA.cpp b/third_party/amd/lib/TritonAMDGPUToLLVM/DotOpToLLVM/WMMA.cpp
index 5a003f768..91f15a9a1 100644
--- a/third_party/amd/lib/TritonAMDGPUToLLVM/DotOpToLLVM/WMMA.cpp
+++ b/third_party/amd/lib/TritonAMDGPUToLLVM/DotOpToLLVM/WMMA.cpp
@@ -53,6 +53,7 @@ getValuesFromDotOperandLayoutStruct(ConversionPatternRewriter &rewriter,
                                     const LLVMTypeConverter *typeConverter,
                                     Value value, int batch, int n0, int n1,
                                     int kWidth, Type type, Location loc) {
+  auto tb = TritonLLVMOpBuilder(loc, rewriter);
   auto elems = unpackLLElements(loc, value, rewriter);
   ValueTable vals;
   for (int b = 0; b < batch; b++) {
@@ -60,21 +61,21 @@ getValuesFromDotOperandLayoutStruct(ConversionPatternRewriter &rewriter,
       for (int j = 0; j < n1; j++) {
         Type elemTy = typeConverter->convertType(type);
         Type ty = vec_ty(elemTy, kWidth);
-        Value rawElems = undef(ty);
+        Value rawElems = tb.undef(ty);
         for (int k = 0; k < kWidth; ++k) {
-          rawElems = insert_element(
+          rawElems = tb.insert_element(
               ty, rawElems,
               elems[n0 * n1 * kWidth * b + kWidth * (n1 * i + j) + k],
-              i32_val(k));
+              tb.i32_val(k));
         }
 
         Value convertedElems;
         if (type.isF16()) {
           convertedElems = rawElems;
         } else if (type.isBF16()) {
-          convertedElems = bitcast(rawElems, vec_ty(i16_ty, kWidth));
+          convertedElems = tb.bitcast(rawElems, vec_ty(i16_ty, kWidth));
         } else {
-          convertedElems = bitcast(
+          convertedElems = tb.bitcast(
               rawElems, vec_ty(i32_ty, kWidth * type.getIntOrFloatBitWidth() /
                                            i32_ty.getIntOrFloatBitWidth()));
         }
@@ -120,8 +121,9 @@ static WMMAInstrType getWMMAInstrTypeFromDot(DotOp op) {
 Value generateROCDLOp(ConversionPatternRewriter &rewriter, Location loc,
                       WMMAInstrType wmmaType, Value valA, Value valB,
                       Value valC, Type aElType, Type bElType) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto resType = valC.getType();
-  Value falseFlag = int_val(1, false);
+  Value falseFlag = b.int_val(1, false);
   switch (wmmaType) {
   case WMMAInstrType::FP32_FP16:
     return rewriter.create<ROCDL::wmma_f32_16x16x16_f16>(
@@ -138,14 +140,14 @@ Value generateROCDLOp(ConversionPatternRewriter &rewriter, Location loc,
   case WMMAInstrType::I32_I8:
     return rewriter.create<ROCDL::wmma_i32_16x16x16_iu8>(
         loc, TypeRange{resType},
-        ValueRange{int_val(1, !aElType.isUnsignedInteger()), valA,
-                   int_val(1, !bElType.isUnsignedInteger()), valB, valC,
+        ValueRange{b.int_val(1, !aElType.isUnsignedInteger()), valA,
+                   b.int_val(1, !bElType.isUnsignedInteger()), valB, valC,
                    falseFlag});
   case WMMAInstrType::I32_I4:
     return rewriter.create<ROCDL::wmma_i32_16x16x16_iu4>(
         loc, TypeRange{resType},
-        ValueRange{int_val(1, !aElType.isUnsignedInteger()), valA,
-                   int_val(1, !bElType.isUnsignedInteger()), valB, valC,
+        ValueRange{b.int_val(1, !aElType.isUnsignedInteger()), valA,
+                   b.int_val(1, !bElType.isUnsignedInteger()), valB, valC,
                    falseFlag});
   default:
     llvm::report_fatal_error("WMMA data type not supported");
@@ -204,21 +206,22 @@ Value generateWMMAIntrinsic(ConversionPatternRewriter &rewriter, Location loc,
                             WMMAInstrType wmmaType, Value valA, Value valB,
                             Value valC, Type aElType, Type bElType,
                             Type dElType) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto name = getWmmaIntrinsicName(aElType, bElType, dElType, valA.getType(),
                                    valC.getType());
   LLVM::FastmathFlagsAttr defaultFlags{};
   SmallVector<Value> operands;
   if (aElType.isInteger())
-    operands.push_back(int_val(1, !aElType.isUnsignedInteger()));
+    operands.push_back(b.int_val(1, !aElType.isUnsignedInteger()));
   operands.push_back(valA);
   if (bElType.isInteger())
-    operands.push_back(int_val(1, !bElType.isUnsignedInteger()));
+    operands.push_back(b.int_val(1, !bElType.isUnsignedInteger()));
   operands.push_back(valB);
   operands.push_back(valC);
   // Flag for using low bits in registers. Result could be already packed to
   // int32. Set low bits by default for now.
   if (32 / dElType.getIntOrFloatBitWidth() > 1 || dElType.isInteger(32)) {
-    operands.push_back(int_val(1, false));
+    operands.push_back(b.int_val(1, false));
   }
   auto wmmaIntrinsic = LLVM::createLLVMIntrinsicCallOp(
       rewriter, loc, name, valC.getType(), operands);
@@ -250,6 +253,7 @@ LogicalResult convertDot(DotOp op, DotOpAdaptor adaptor,
   auto wmmaInstrType = getWMMAInstrTypeFromDot(op);
 
   auto loc = op.getLoc();
+  auto tb = TritonLLVMOpBuilder(loc, rewriter);
   Value a = op.getA();
   Value b = op.getB();
   Value d = op.getD();
@@ -303,10 +307,10 @@ LogicalResult convertDot(DotOp op, DotOpAdaptor adaptor,
         auto nRepOffId = n * dElemsToStorePerThread;
         auto fcThreadOffIdx = batchOffIdx + mRepOffId + nRepOffId;
 
-        Value acc = undef(vecTy);
+        Value acc = tb.undef(vecTy);
         for (unsigned v = 0; v < dElemsToStorePerThread; ++v) {
-          acc = insert_element(vecTy, acc, fc[fcThreadOffIdx + v],
-                               i32_val(v * paddedOutputElemSize));
+          acc = tb.insert_element(vecTy, acc, fc[fcThreadOffIdx + v],
+                                  tb.i32_val(v * paddedOutputElemSize));
         }
         for (size_t k = 0; k < numRepK; k++) {
           acc = generateWMMAOp(rewriter, loc, wmmaInstrType, ha[{b, m, k}],
@@ -314,8 +318,8 @@ LogicalResult convertDot(DotOp op, DotOpAdaptor adaptor,
                                bTensorTy.getElementType(), dstElemTy, wmmaVer);
         }
         for (unsigned v = 0; v < dElemsToStorePerThread; ++v) {
-          fc[fcThreadOffIdx + v] = extract_element(
-              dstElemTy, acc, i32_val(v * paddedOutputElemSize));
+          fc[fcThreadOffIdx + v] = tb.extract_element(
+              dstElemTy, acc, tb.i32_val(v * paddedOutputElemSize));
         }
       }
     }
diff --git a/third_party/amd/lib/TritonAMDGPUToLLVM/ElementwiseOpToLLVM.cpp b/third_party/amd/lib/TritonAMDGPUToLLVM/ElementwiseOpToLLVM.cpp
index 47d5fbb35..313cc3365 100644
--- a/third_party/amd/lib/TritonAMDGPUToLLVM/ElementwiseOpToLLVM.cpp
+++ b/third_party/amd/lib/TritonAMDGPUToLLVM/ElementwiseOpToLLVM.cpp
@@ -32,55 +32,57 @@ namespace {
 static SmallVector<Value>
 Fp16_to_Fp8E5M2_RTNE(Location loc, ConversionPatternRewriter &rewriter,
                      const SmallVector<Value> &v) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto fp16x2VecTy = vec_ty(f16_ty, 2);
-  Value fp16x2Vec0 = undef(fp16x2VecTy);
-  Value fp16x2Vec1 = undef(fp16x2VecTy);
-  fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v[0], i32_val(0));
-  fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v[1], i32_val(1));
-  fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v[2], i32_val(0));
-  fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v[3], i32_val(1));
+  Value fp16x2Vec0 = b.undef(fp16x2VecTy);
+  Value fp16x2Vec1 = b.undef(fp16x2VecTy);
+  fp16x2Vec0 = b.insert_element(fp16x2VecTy, fp16x2Vec0, v[0], b.i32_val(0));
+  fp16x2Vec0 = b.insert_element(fp16x2VecTy, fp16x2Vec0, v[1], b.i32_val(1));
+  fp16x2Vec1 = b.insert_element(fp16x2VecTy, fp16x2Vec1, v[2], b.i32_val(0));
+  fp16x2Vec1 = b.insert_element(fp16x2VecTy, fp16x2Vec1, v[3], b.i32_val(1));
 
-  Value a0 = bitcast(fp16x2Vec0, i32_ty);
-  Value a1 = bitcast(fp16x2Vec1, i32_ty);
+  Value a0 = b.bitcast(fp16x2Vec0, i32_ty);
+  Value a1 = b.bitcast(fp16x2Vec1, i32_ty);
 
-  a0 = and_(i32_ty, a0, i32_val(0xfffefffe));
-  a1 = and_(i32_ty, a1, i32_val(0xfffefffe));
+  a0 = b.and_(i32_ty, a0, b.i32_val(0xfffefffe));
+  a1 = b.and_(i32_ty, a1, b.i32_val(0xfffefffe));
 
-  a0 = add(i32_ty, a0, i32_val(0x00800080));
-  a1 = add(i32_ty, a1, i32_val(0x00800080));
+  a0 = b.add(i32_ty, a0, b.i32_val(0x00800080));
+  a1 = b.add(i32_ty, a1, b.i32_val(0x00800080));
 
   auto fp8x4VecTy = vec_ty(i8_ty, 4);
-  a0 = bitcast(a0, fp8x4VecTy);
-  a1 = bitcast(a1, fp8x4VecTy);
+  a0 = b.bitcast(a0, fp8x4VecTy);
+  a1 = b.bitcast(a1, fp8x4VecTy);
 
-  return {extract_element(i8_ty, a0, i32_val(1)),
-          extract_element(i8_ty, a0, i32_val(3)),
-          extract_element(i8_ty, a1, i32_val(1)),
-          extract_element(i8_ty, a1, i32_val(3))};
+  return {b.extract_element(i8_ty, a0, b.i32_val(1)),
+          b.extract_element(i8_ty, a0, b.i32_val(3)),
+          b.extract_element(i8_ty, a1, b.i32_val(1)),
+          b.extract_element(i8_ty, a1, b.i32_val(3))};
 }
 
 static SmallVector<Value>
 Fp16_to_Fp8E5M2_RTZ(Location loc, ConversionPatternRewriter &rewriter,
                     const SmallVector<Value> &v) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto fp16x2VecTy = vec_ty(f16_ty, 2);
-  Value fp16x2Vec0 = undef(fp16x2VecTy);
-  Value fp16x2Vec1 = undef(fp16x2VecTy);
-  fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v[0], i32_val(0));
-  fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v[1], i32_val(1));
-  fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v[2], i32_val(0));
-  fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v[3], i32_val(1));
+  Value fp16x2Vec0 = b.undef(fp16x2VecTy);
+  Value fp16x2Vec1 = b.undef(fp16x2VecTy);
+  fp16x2Vec0 = b.insert_element(fp16x2VecTy, fp16x2Vec0, v[0], b.i32_val(0));
+  fp16x2Vec0 = b.insert_element(fp16x2VecTy, fp16x2Vec0, v[1], b.i32_val(1));
+  fp16x2Vec1 = b.insert_element(fp16x2VecTy, fp16x2Vec1, v[2], b.i32_val(0));
+  fp16x2Vec1 = b.insert_element(fp16x2VecTy, fp16x2Vec1, v[3], b.i32_val(1));
 
-  Value a0 = bitcast(fp16x2Vec0, i32_ty);
-  Value a1 = bitcast(fp16x2Vec1, i32_ty);
+  Value a0 = b.bitcast(fp16x2Vec0, i32_ty);
+  Value a1 = b.bitcast(fp16x2Vec1, i32_ty);
 
   auto fp8x4VecTy = vec_ty(i8_ty, 4);
-  a0 = bitcast(a0, fp8x4VecTy);
-  a1 = bitcast(a1, fp8x4VecTy);
+  a0 = b.bitcast(a0, fp8x4VecTy);
+  a1 = b.bitcast(a1, fp8x4VecTy);
 
-  return {extract_element(i8_ty, a0, i32_val(1)),
-          extract_element(i8_ty, a0, i32_val(3)),
-          extract_element(i8_ty, a1, i32_val(1)),
-          extract_element(i8_ty, a1, i32_val(3))};
+  return {b.extract_element(i8_ty, a0, b.i32_val(1)),
+          b.extract_element(i8_ty, a0, b.i32_val(3)),
+          b.extract_element(i8_ty, a1, b.i32_val(1)),
+          b.extract_element(i8_ty, a1, b.i32_val(3))};
 }
 
 static Value cvtFp16ToFp32(Location loc, ConversionPatternRewriter &rewriter,
@@ -117,14 +119,15 @@ static SmallVector<Value> cvtFp8ToFp32(Location loc,
                                        ConversionPatternRewriter &rewriter,
                                        Value v0, Value v1,
                                        const std::string &fp8_format) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   assert(fp8_format == "fp8" || fp8_format == "bf8");
   std::string ins_str = "v_cvt_pk_f32_" + fp8_format;
 
   auto fp8x4VecTy = vec_ty(i8_ty, 4);
-  Value fp8x4Vec = undef(fp8x4VecTy);
-  fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v0, i32_val(0));
-  fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v1, i32_val(1));
-  auto i32v = bitcast(fp8x4Vec, i32_ty);
+  Value fp8x4Vec = b.undef(fp8x4VecTy);
+  fp8x4Vec = b.insert_element(fp8x4VecTy, fp8x4Vec, v0, b.i32_val(0));
+  fp8x4Vec = b.insert_element(fp8x4VecTy, fp8x4Vec, v1, b.i32_val(1));
+  auto i32v = b.bitcast(fp8x4Vec, i32_ty);
 
   GCNBuilder builder1;
   auto &cvt = *builder1.create(ins_str);
@@ -133,11 +136,11 @@ static SmallVector<Value> cvtFp8ToFp32(Location loc,
   cvt(res, operand);
   auto i64v = builder1.launch(rewriter, loc, i64_ty, false);
   auto fp32x2VecTy = vec_ty(f32_ty, 2);
-  auto fp32x2Vec = bitcast(i64v, fp32x2VecTy);
+  auto fp32x2Vec = b.bitcast(i64v, fp32x2VecTy);
 
   SmallVector<Value> ret(2);
-  ret[0] = extract_element(f32_ty, fp32x2Vec, i32_val(0));
-  ret[1] = extract_element(f32_ty, fp32x2Vec, i32_val(1));
+  ret[0] = b.extract_element(f32_ty, fp32x2Vec, b.i32_val(0));
+  ret[1] = b.extract_element(f32_ty, fp32x2Vec, b.i32_val(1));
 
   return ret;
 }
@@ -147,6 +150,7 @@ static SmallVector<Value> cvtFp32ToFp8(Location loc,
                                        ConversionPatternRewriter &rewriter,
                                        Value v0, Value v1,
                                        const std::string &fp8_format) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   assert(fp8_format == "fp8" || fp8_format == "bf8");
   std::string ins_str = "v_cvt_pk_" + fp8_format + "_f32";
 
@@ -159,11 +163,11 @@ static SmallVector<Value> cvtFp32ToFp8(Location loc,
   auto fp8x4Vec = builder.launch(rewriter, loc, i32_ty, false);
 
   auto fp8x4VecTy = vec_ty(i8_ty, 4);
-  auto a1 = bitcast(fp8x4Vec, fp8x4VecTy);
+  auto a1 = b.bitcast(fp8x4Vec, fp8x4VecTy);
 
   SmallVector<Value> ret(2);
-  ret[0] = extract_element(i8_ty, a1, i32_val(0));
-  ret[1] = extract_element(i8_ty, a1, i32_val(1));
+  ret[0] = b.extract_element(i8_ty, a1, b.i32_val(0));
+  ret[1] = b.extract_element(i8_ty, a1, b.i32_val(1));
 
   return ret;
 }
@@ -226,29 +230,30 @@ Fp8E4M3FNUZ_to_Fp32(Location loc, ConversionPatternRewriter &rewriter,
 static Value Fp16_to_Fp8E5M2FNUZ_oneValue(Location loc,
                                           ConversionPatternRewriter &rewriter,
                                           Value v) {
-  auto vi16 = bitcast(v, i16_ty);
-  auto e = and_(i16_ty, vi16, int_val(16, 0x7C00));
-  auto sign = and_(i16_ty, vi16, int_val(16, 0x8000));
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  auto vi16 = b.bitcast(v, i16_ty);
+  auto e = b.and_(i16_ty, vi16, b.int_val(16, 0x7C00));
+  auto sign = b.and_(i16_ty, vi16, b.int_val(16, 0x8000));
 
   // normal value
-  auto a = and_(i16_ty, vi16, int_val(16, 0x7FFFF));
-  auto a1 = add(i16_ty, a, int_val(16, 0x0400));
-  auto o1 = or_(i16_ty, a1, sign);
+  auto a = b.and_(i16_ty, vi16, b.int_val(16, 0x7FFFF));
+  auto a1 = b.add(i16_ty, a, b.int_val(16, 0x0400));
+  auto o1 = b.or_(i16_ty, a1, sign);
 
   // subnormal value, e is 0
-  auto m = and_(i16_ty, vi16, int_val(16, 0x03FF));
-  auto m2 = shl(m, int_val(16, 1));
-  auto o2 = or_(i16_ty, sign, or_(i16_ty, int_val(16, 1), m2));
+  auto m = b.and_(i16_ty, vi16, b.int_val(16, 0x03FF));
+  auto m2 = b.shl(m, b.int_val(16, 1));
+  auto o2 = b.or_(i16_ty, sign, b.or_(i16_ty, b.int_val(16, 1), m2));
 
-  auto e_is_zero = icmp_eq(e, int_val(16, 0));
-  auto e_is_all1 = icmp_eq(e, int_val(16, 0x7C00));
+  auto e_is_zero = b.icmp_eq(e, b.int_val(16, 0));
+  auto e_is_all1 = b.icmp_eq(e, b.int_val(16, 0x7C00));
 
-  auto ot = select(e_is_zero, o2, o1);
-  auto o = select(e_is_all1, vi16, ot);
+  auto ot = b.select(e_is_zero, o2, o1);
+  auto o = b.select(e_is_all1, vi16, ot);
   auto fp8x2VecTy = vec_ty(i8_ty, 2);
-  auto res = bitcast(o, fp8x2VecTy);
+  auto res = b.bitcast(o, fp8x2VecTy);
 
-  return extract_element(i8_ty, res, i32_val(1));
+  return b.extract_element(i8_ty, res, b.i32_val(1));
 }
 
 static SmallVector<Value>
@@ -274,105 +279,109 @@ ConverterT Fp16_to_Fp8E5M2FNUZ(AMD::ISAFamily isaFamily) {
 static SmallVector<Value> Fp8E5M2_to_Fp16(Location loc,
                                           ConversionPatternRewriter &rewriter,
                                           const SmallVector<Value> &v) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto fp8x4VecTy = vec_ty(i8_ty, 4);
-  Value a0 = undef(fp8x4VecTy);
-  a0 = insert_element(fp8x4VecTy, a0, int_val(8, 0), i32_val(0));
-  a0 = insert_element(fp8x4VecTy, a0, v[0], i32_val(1));
-  a0 = insert_element(fp8x4VecTy, a0, int_val(8, 0), i32_val(2));
-  a0 = insert_element(fp8x4VecTy, a0, v[1], i32_val(3));
-  a0 = bitcast(a0, i32_ty);
-  Value a1 = undef(fp8x4VecTy);
-  a1 = insert_element(fp8x4VecTy, a1, int_val(8, 0), i32_val(0));
-  a1 = insert_element(fp8x4VecTy, a1, v[2], i32_val(1));
-  a1 = insert_element(fp8x4VecTy, a1, int_val(8, 0), i32_val(2));
-  a1 = insert_element(fp8x4VecTy, a1, v[3], i32_val(3));
-  a1 = bitcast(a1, i32_ty);
+  Value a0 = b.undef(fp8x4VecTy);
+  a0 = b.insert_element(fp8x4VecTy, a0, b.int_val(8, 0), b.i32_val(0));
+  a0 = b.insert_element(fp8x4VecTy, a0, v[0], b.i32_val(1));
+  a0 = b.insert_element(fp8x4VecTy, a0, b.int_val(8, 0), b.i32_val(2));
+  a0 = b.insert_element(fp8x4VecTy, a0, v[1], b.i32_val(3));
+  a0 = b.bitcast(a0, i32_ty);
+  Value a1 = b.undef(fp8x4VecTy);
+  a1 = b.insert_element(fp8x4VecTy, a1, b.int_val(8, 0), b.i32_val(0));
+  a1 = b.insert_element(fp8x4VecTy, a1, v[2], b.i32_val(1));
+  a1 = b.insert_element(fp8x4VecTy, a1, b.int_val(8, 0), b.i32_val(2));
+  a1 = b.insert_element(fp8x4VecTy, a1, v[3], b.i32_val(3));
+  a1 = b.bitcast(a1, i32_ty);
 
   auto fp16x2VecTy = vec_ty(f16_ty, 2);
-  auto fp16x2Vec0 = bitcast(a0, fp16x2VecTy);
-  auto fp16x2Vec1 = bitcast(a1, fp16x2VecTy);
+  auto fp16x2Vec0 = b.bitcast(a0, fp16x2VecTy);
+  auto fp16x2Vec1 = b.bitcast(a1, fp16x2VecTy);
 
-  return {extract_element(f16_ty, fp16x2Vec0, i32_val(0)),
-          extract_element(f16_ty, fp16x2Vec0, i32_val(1)),
-          extract_element(f16_ty, fp16x2Vec1, i32_val(0)),
-          extract_element(f16_ty, fp16x2Vec1, i32_val(1))};
+  return {b.extract_element(f16_ty, fp16x2Vec0, b.i32_val(0)),
+          b.extract_element(f16_ty, fp16x2Vec0, b.i32_val(1)),
+          b.extract_element(f16_ty, fp16x2Vec1, b.i32_val(0)),
+          b.extract_element(f16_ty, fp16x2Vec1, b.i32_val(1))};
 }
 
 static Value convertBf16ToFp32(Location loc,
                                ConversionPatternRewriter &rewriter,
                                const Value &v) {
-  auto as_int16 = bitcast(v, i16_ty);
-  auto as_int32 = zext(i32_ty, as_int16);
-  auto shifted = shl(i32_ty, as_int32, i32_val(16));
-  return bitcast(shifted, f32_ty);
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  auto as_int16 = b.bitcast(v, i16_ty);
+  auto as_int32 = b.zext(i32_ty, as_int16);
+  auto shifted = b.shl(i32_ty, as_int32, b.i32_val(16));
+  return b.bitcast(shifted, f32_ty);
 }
 
 static Value convertFp32ToBf16(Location loc,
                                ConversionPatternRewriter &rewriter,
                                const Value &v, const RoundingMode rounding) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   if (rounding == RoundingMode::RTZ) {
-    auto as_int32 = bitcast(v, i32_ty);
-    auto shifted = lshr(i32_ty, as_int32, i32_val(16));
-    auto truncated = trunc(i16_ty, shifted);
-    return bitcast(truncated, bf16_ty);
+    auto as_int32 = b.bitcast(v, i32_ty);
+    auto shifted = b.lshr(i32_ty, as_int32, b.i32_val(16));
+    auto truncated = b.trunc(i16_ty, shifted);
+    return b.bitcast(truncated, bf16_ty);
   }
   // Otherwise it is (rounding == RoundingMode::RTNE)
-  auto as_uint32 = bitcast(v, i32_ty);
+  auto as_uint32 = b.bitcast(v, i32_ty);
   auto check_exponent =
-      and_(i32_ty, xor_(i32_ty, as_uint32, i32_val(0xffffffff)),
-           i32_val(0x7f800000));
-  auto exponent_not_all1s = icmp_ne(check_exponent, i32_val(0));
-  auto exponent_all1s = icmp_eq(check_exponent, i32_val(0));
+      b.and_(i32_ty, b.xor_(i32_ty, as_uint32, b.i32_val(0xffffffff)),
+             b.i32_val(0x7f800000));
+  auto exponent_not_all1s = b.icmp_ne(check_exponent, b.i32_val(0));
+  auto exponent_all1s = b.icmp_eq(check_exponent, b.i32_val(0));
   auto rounded =
-      add(i32_ty, i32_val(0x7fff),
-          and_(i32_ty, lshr(i32_ty, as_uint32, i32_val(16)), i32_val(1)));
-  rounded = add(i32_ty, rounded, as_uint32);
-  auto res = select(exponent_not_all1s, rounded, as_uint32);
+      b.add(i32_ty, b.i32_val(0x7fff),
+            b.and_(i32_ty, b.lshr(i32_ty, as_uint32, b.i32_val(16)), b.i32_val(1)));
+  rounded = b.add(i32_ty, rounded, as_uint32);
+  auto res = b.select(exponent_not_all1s, rounded, as_uint32);
 
   auto preserve_nan =
-      and_(i1_ty, exponent_all1s,
-           icmp_ne(and_(i32_ty, as_uint32, i32_val(0xffff)), i32_val(0)));
-  auto nan = or_(i32_ty, as_uint32, i32_val(0x10000));
-  res = select(preserve_nan, nan, res);
-
-  auto shifted = lshr(i32_ty, res, i32_val(16));
-  auto truncated = trunc(i16_ty, shifted);
-  return bitcast(truncated, bf16_ty);
+      b.and_(i1_ty, exponent_all1s,
+             b.icmp_ne(b.and_(i32_ty, as_uint32, b.i32_val(0xffff)), b.i32_val(0)));
+  auto nan = b.or_(i32_ty, as_uint32, b.i32_val(0x10000));
+  res = b.select(preserve_nan, nan, res);
+
+  auto shifted = b.lshr(i32_ty, res, b.i32_val(16));
+  auto truncated = b.trunc(i16_ty, shifted);
+  return b.bitcast(truncated, bf16_ty);
 }
 
 static Value Fp8E5M2FNUZ_to_Fp16_oneValue(Location loc,
                                           ConversionPatternRewriter &rewriter,
                                           Value v) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto fp8x2VecTy = vec_ty(i8_ty, 2);
-  Value a = undef(fp8x2VecTy);
-  a = insert_element(fp8x2VecTy, a, int_val(8, 0), i32_val(0));
-  a = insert_element(fp8x2VecTy, a, v, i32_val(1));
-  a = bitcast(a, i16_ty);
+  Value a = b.undef(fp8x2VecTy);
+  a = b.insert_element(fp8x2VecTy, a, b.int_val(8, 0), b.i32_val(0));
+  a = b.insert_element(fp8x2VecTy, a, v, b.i32_val(1));
+  a = b.bitcast(a, i16_ty);
 
-  auto e = and_(i16_ty, a, int_val(16, 0x7C00));
-  auto m = and_(i16_ty, a, int_val(16, 0x0300));
-  auto sign = and_(i16_ty, a, int_val(16, 0x8000));
+  auto e = b.and_(i16_ty, a, b.int_val(16, 0x7C00));
+  auto m = b.and_(i16_ty, a, b.int_val(16, 0x0300));
+  auto sign = b.and_(i16_ty, a, b.int_val(16, 0x8000));
 
   // check whether all exponents are zeros
-  auto e_is_zero = icmp_eq(e, int_val(16, 0x0));
+  auto e_is_zero = b.icmp_eq(e, b.int_val(16, 0x0));
 
   // case 1, e is zero, need to move m right by 1 bit
-  auto m1 = lshr(i16_ty, m, int_val(16, 1));
-  auto o0 = or_(i16_ty, sign, m1);
+  auto m1 = b.lshr(i16_ty, m, b.int_val(16, 1));
+  auto o0 = b.or_(i16_ty, sign, m1);
 
   // case 2, e is nonzero, sub exponent by 1
-  auto e1 = sub(i16_ty, e, int_val(16, 0x0400));
+  auto e1 = b.sub(i16_ty, e, b.int_val(16, 0x0400));
 
-  auto e_is_one = icmp_eq(e, int_val(16, 0x0400));
-  auto m2 = add(i16_ty, m1, int_val(16, 0x0200));
+  auto e_is_one = b.icmp_eq(e, b.int_val(16, 0x0400));
+  auto m2 = b.add(i16_ty, m1, b.int_val(16, 0x0200));
 
-  auto o1 = or_(i16_ty, sign, or_(i16_ty, m, e1));
-  auto o2 = or_(i16_ty, sign, m2);
+  auto o1 = b.or_(i16_ty, sign, b.or_(i16_ty, m, e1));
+  auto o2 = b.or_(i16_ty, sign, m2);
 
-  auto o12 = select(e_is_one, o2, o1);
-  auto o = select(e_is_zero, o0, o12);
+  auto o12 = b.select(e_is_one, o2, o1);
+  auto o = b.select(e_is_zero, o0, o12);
 
-  return bitcast(o, f16_ty);
+  return b.bitcast(o, f16_ty);
 }
 
 static SmallVector<Value>
@@ -398,143 +407,149 @@ ConverterT Fp8E5M2FNUZ_to_Fp16(AMD::ISAFamily isaFamily) {
 static SmallVector<Value> Fp8E5M2_to_Bf16(Location loc,
                                           ConversionPatternRewriter &rewriter,
                                           const SmallVector<Value> &v) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto fp8x4VecTy = vec_ty(i8_ty, 4);
-  Value a0 = undef(fp8x4VecTy);
-  a0 = insert_element(fp8x4VecTy, a0, int_val(8, 0), i32_val(0));
-  a0 = insert_element(fp8x4VecTy, a0, v[0], i32_val(1));
-  a0 = insert_element(fp8x4VecTy, a0, int_val(8, 0), i32_val(2));
-  a0 = insert_element(fp8x4VecTy, a0, v[1], i32_val(3));
-  a0 = bitcast(a0, i32_ty);
-
-  Value a1 = undef(fp8x4VecTy);
-  a1 = insert_element(fp8x4VecTy, a1, int_val(8, 0), i32_val(0));
-  a1 = insert_element(fp8x4VecTy, a1, v[2], i32_val(1));
-  a1 = insert_element(fp8x4VecTy, a1, int_val(8, 0), i32_val(2));
-  a1 = insert_element(fp8x4VecTy, a1, v[3], i32_val(3));
-  a1 = bitcast(a1, i32_ty);
-
-  Value b0 = and_(i32_ty, a0, i32_val(0x7fff7fff));
-  Value b1 = and_(i32_ty, a1, i32_val(0x7fff7fff));
-  b0 = lshr(i32_ty, b0, i32_val(3));
-  b1 = lshr(i32_ty, b1, i32_val(3));
-
-  Value c0 = shl(i32_ty, b0, i32_val(16));
-  Value c1 = and_(i32_ty, b0, i32_val(0xFFFF0000));
-  Value c2 = shl(i32_ty, b1, i32_val(16));
-  Value c3 = and_(i32_ty, b1, i32_val(0xFFFF0000));
-
-  c0 = bitcast(c0, f32_ty);
-  c1 = bitcast(c1, f32_ty);
-  c2 = bitcast(c2, f32_ty);
-  c3 = bitcast(c3, f32_ty);
-
-  Value d0 = fmul(f32_ty, c0, f32_val(0x1p+112));
-  Value d1 = fmul(f32_ty, c1, f32_val(0x1p+112));
-  Value d2 = fmul(f32_ty, c2, f32_val(0x1p+112));
-  Value d3 = fmul(f32_ty, c3, f32_val(0x1p+112));
-
-  d0 = bitcast(d0, i32_ty);
-  d1 = bitcast(d1, i32_ty);
-  d2 = bitcast(d2, i32_ty);
-  d3 = bitcast(d3, i32_ty);
-
-  Value out0 = or_(i32_ty, lshr(i32_ty, d0, i32_val(16)), d1);
-  Value out1 = or_(i32_ty, lshr(i32_ty, d2, i32_val(16)), d3);
-
-  Value sign0 = and_(i32_ty, a0, i32_val(0x80008000));
-  Value sign1 = and_(i32_ty, a1, i32_val(0x80008000));
-
-  out0 = or_(i32_ty, out0, sign0);
-  out1 = or_(i32_ty, out1, sign1);
+  Value a0 = b.undef(fp8x4VecTy);
+  a0 = b.insert_element(fp8x4VecTy, a0, b.int_val(8, 0), b.i32_val(0));
+  a0 = b.insert_element(fp8x4VecTy, a0, v[0], b.i32_val(1));
+  a0 = b.insert_element(fp8x4VecTy, a0, b.int_val(8, 0), b.i32_val(2));
+  a0 = b.insert_element(fp8x4VecTy, a0, v[1], b.i32_val(3));
+  a0 = b.bitcast(a0, i32_ty);
+
+  Value a1 = b.undef(fp8x4VecTy);
+  a1 = b.insert_element(fp8x4VecTy, a1, b.int_val(8, 0), b.i32_val(0));
+  a1 = b.insert_element(fp8x4VecTy, a1, v[2], b.i32_val(1));
+  a1 = b.insert_element(fp8x4VecTy, a1, b.int_val(8, 0), b.i32_val(2));
+  a1 = b.insert_element(fp8x4VecTy, a1, v[3], b.i32_val(3));
+  a1 = b.bitcast(a1, i32_ty);
+
+  Value b0 = b.and_(i32_ty, a0, b.i32_val(0x7fff7fff));
+  Value b1 = b.and_(i32_ty, a1, b.i32_val(0x7fff7fff));
+  b0 = b.lshr(i32_ty, b0, b.i32_val(3));
+  b1 = b.lshr(i32_ty, b1, b.i32_val(3));
+
+  Value c0 = b.shl(i32_ty, b0, b.i32_val(16));
+  Value c1 = b.and_(i32_ty, b0, b.i32_val(0xFFFF0000));
+  Value c2 = b.shl(i32_ty, b1, b.i32_val(16));
+  Value c3 = b.and_(i32_ty, b1, b.i32_val(0xFFFF0000));
+
+  c0 = b.bitcast(c0, f32_ty);
+  c1 = b.bitcast(c1, f32_ty);
+  c2 = b.bitcast(c2, f32_ty);
+  c3 = b.bitcast(c3, f32_ty);
+
+  Value d0 = b.fmul(f32_ty, c0, b.f32_val(0x1p+112));
+  Value d1 = b.fmul(f32_ty, c1, b.f32_val(0x1p+112));
+  Value d2 = b.fmul(f32_ty, c2, b.f32_val(0x1p+112));
+  Value d3 = b.fmul(f32_ty, c3, b.f32_val(0x1p+112));
+
+  d0 = b.bitcast(d0, i32_ty);
+  d1 = b.bitcast(d1, i32_ty);
+  d2 = b.bitcast(d2, i32_ty);
+  d3 = b.bitcast(d3, i32_ty);
+
+  Value out0 = b.or_(i32_ty, b.lshr(i32_ty, d0, b.i32_val(16)), d1);
+  Value out1 = b.or_(i32_ty, b.lshr(i32_ty, d2, b.i32_val(16)), d3);
+
+  Value sign0 = b.and_(i32_ty, a0, b.i32_val(0x80008000));
+  Value sign1 = b.and_(i32_ty, a1, b.i32_val(0x80008000));
+
+  out0 = b.or_(i32_ty, out0, sign0);
+  out1 = b.or_(i32_ty, out1, sign1);
 
   auto bf16x2VecTy = vec_ty(bf16_ty, 2);
-  out0 = bitcast(out0, bf16x2VecTy);
-  out1 = bitcast(out1, bf16x2VecTy);
+  out0 = b.bitcast(out0, bf16x2VecTy);
+  out1 = b.bitcast(out1, bf16x2VecTy);
 
-  return {extract_element(bf16_ty, out0, i32_val(0)),
-          extract_element(bf16_ty, out0, i32_val(1)),
-          extract_element(bf16_ty, out1, i32_val(0)),
-          extract_element(bf16_ty, out1, i32_val(1))};
+  return {b.extract_element(bf16_ty, out0, b.i32_val(0)),
+          b.extract_element(bf16_ty, out0, b.i32_val(1)),
+          b.extract_element(bf16_ty, out1, b.i32_val(0)),
+          b.extract_element(bf16_ty, out1, b.i32_val(1))};
 }
 
 static SmallVector<Value> Bf16_to_Fp8E5M2(Location loc,
                                           ConversionPatternRewriter &rewriter,
                                           const SmallVector<Value> &v) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto bf16x2VecTy = vec_ty(bf16_ty, 2);
-  Value bf16x2Vec0 = undef(bf16x2VecTy);
-  Value bf16x2Vec1 = undef(bf16x2VecTy);
-  bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v[0], i32_val(0));
-  bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v[1], i32_val(1));
-  bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v[2], i32_val(0));
-  bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v[3], i32_val(1));
-  bf16x2Vec0 = bitcast(bf16x2Vec0, i32_ty);
-  bf16x2Vec1 = bitcast(bf16x2Vec1, i32_ty);
-
-  Value sign0 = and_(i32_ty, bf16x2Vec0, i32_val(0x80008000));
-  Value sign1 = and_(i32_ty, bf16x2Vec1, i32_val(0x80008000));
+  Value bf16x2Vec0 = b.undef(bf16x2VecTy);
+  Value bf16x2Vec1 = b.undef(bf16x2VecTy);
+  bf16x2Vec0 = b.insert_element(bf16x2VecTy, bf16x2Vec0, v[0], b.i32_val(0));
+  bf16x2Vec0 = b.insert_element(bf16x2VecTy, bf16x2Vec0, v[1], b.i32_val(1));
+  bf16x2Vec1 = b.insert_element(bf16x2VecTy, bf16x2Vec1, v[2], b.i32_val(0));
+  bf16x2Vec1 = b.insert_element(bf16x2VecTy, bf16x2Vec1, v[3], b.i32_val(1));
+  bf16x2Vec0 = b.bitcast(bf16x2Vec0, i32_ty);
+  bf16x2Vec1 = b.bitcast(bf16x2Vec1, i32_ty);
+
+  Value sign0 = b.and_(i32_ty, bf16x2Vec0, b.i32_val(0x80008000));
+  Value sign1 = b.and_(i32_ty, bf16x2Vec1, b.i32_val(0x80008000));
   auto fp8x4VecTy = vec_ty(i8_ty, 4);
-  Value sign = undef(fp8x4VecTy);
-  sign0 = bitcast(sign0, fp8x4VecTy);
-  sign1 = bitcast(sign1, fp8x4VecTy);
-  sign = insert_element(fp8x4VecTy, sign,
-                        extract_element(i8_ty, sign0, i32_val(1)), i32_val(0));
-  sign = insert_element(fp8x4VecTy, sign,
-                        extract_element(i8_ty, sign0, i32_val(3)), i32_val(1));
-  sign = insert_element(fp8x4VecTy, sign,
-                        extract_element(i8_ty, sign1, i32_val(1)), i32_val(2));
-  sign = insert_element(fp8x4VecTy, sign,
-                        extract_element(i8_ty, sign1, i32_val(3)), i32_val(3));
-  sign = bitcast(sign, i32_ty);
-
-  Value nosign0 = and_(i32_ty, bf16x2Vec0, i32_val(0x7fff7fff));
-  Value nosign1 = and_(i32_ty, bf16x2Vec1, i32_val(0x7fff7fff));
-
-  Value nosign_0_0 = and_(i32_ty, nosign0, i32_val(0xffff0000));
-  nosign_0_0 = umax(i32_ty, nosign_0_0, i32_val(0x38000000));
-  nosign_0_0 = umin(i32_ty, nosign_0_0, i32_val(0x57e00000));
-  Value nosign_0_1 = and_(i32_ty, nosign0, i32_val(0x0000ffff));
-  nosign_0_1 = umax(i32_ty, nosign_0_1, i32_val(0x3800));
-  nosign_0_1 = umin(i32_ty, nosign_0_1, i32_val(0x57e0));
-  nosign0 = or_(i32_ty, nosign_0_0, nosign_0_1);
-
-  Value nosign_1_0 = and_(i32_ty, nosign1, i32_val(0xffff0000));
-  nosign_1_0 = umax(i32_ty, nosign_1_0, i32_val(0x38000000));
-  nosign_1_0 = umin(i32_ty, nosign_1_0, i32_val(0x57e00000));
-  Value nosign_1_1 = and_(i32_ty, nosign1, i32_val(0x0000ffff));
-  nosign_1_1 = umax(i32_ty, nosign_1_1, i32_val(0x3800));
-  nosign_1_1 = umin(i32_ty, nosign_1_1, i32_val(0x57e0));
-  nosign1 = or_(i32_ty, nosign_1_0, nosign_1_1);
-
-  nosign0 = add(i32_ty, nosign0, i32_val(0x00100010));
-  nosign1 = add(i32_ty, nosign1, i32_val(0x00100010));
-  nosign0 = sub(i32_ty, nosign0, i32_val(0x38003800));
-  nosign1 = sub(i32_ty, nosign1, i32_val(0x38003800));
-  nosign0 = shl(i32_ty, nosign0, i32_val(3));
-  nosign1 = shl(i32_ty, nosign1, i32_val(3));
-
-  nosign0 = bitcast(nosign0, fp8x4VecTy);
-  nosign1 = bitcast(nosign1, fp8x4VecTy);
-  Value nosign = undef(fp8x4VecTy);
-  nosign =
-      insert_element(fp8x4VecTy, nosign,
-                     extract_element(i8_ty, nosign0, i32_val(1)), i32_val(0));
-  nosign =
-      insert_element(fp8x4VecTy, nosign,
-                     extract_element(i8_ty, nosign0, i32_val(3)), i32_val(1));
-  nosign =
-      insert_element(fp8x4VecTy, nosign,
-                     extract_element(i8_ty, nosign1, i32_val(1)), i32_val(2));
-  nosign =
-      insert_element(fp8x4VecTy, nosign,
-                     extract_element(i8_ty, nosign1, i32_val(3)), i32_val(3));
-  nosign = bitcast(nosign, i32_ty);
-
-  Value fp8x4Vec = or_(i32_ty, nosign, sign);
-  fp8x4Vec = bitcast(fp8x4Vec, fp8x4VecTy);
-  return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),
-          extract_element(i8_ty, fp8x4Vec, i32_val(1)),
-          extract_element(i8_ty, fp8x4Vec, i32_val(2)),
-          extract_element(i8_ty, fp8x4Vec, i32_val(3))};
+  Value sign = b.undef(fp8x4VecTy);
+  sign0 = b.bitcast(sign0, fp8x4VecTy);
+  sign1 = b.bitcast(sign1, fp8x4VecTy);
+  sign = b.insert_element(fp8x4VecTy, sign,
+                          b.extract_element(i8_ty, sign0, b.i32_val(1)),
+                          b.i32_val(0));
+  sign = b.insert_element(fp8x4VecTy, sign,
+                          b.extract_element(i8_ty, sign0, b.i32_val(3)),
+                          b.i32_val(1));
+  sign = b.insert_element(fp8x4VecTy, sign,
+                          b.extract_element(i8_ty, sign1, b.i32_val(1)),
+                          b.i32_val(2));
+  sign = b.insert_element(fp8x4VecTy, sign,
+                          b.extract_element(i8_ty, sign1, b.i32_val(3)),
+                          b.i32_val(3));
+  sign = b.bitcast(sign, i32_ty);
+
+  Value nosign0 = b.and_(i32_ty, bf16x2Vec0, b.i32_val(0x7fff7fff));
+  Value nosign1 = b.and_(i32_ty, bf16x2Vec1, b.i32_val(0x7fff7fff));
+
+  Value nosign_0_0 = b.and_(i32_ty, nosign0, b.i32_val(0xffff0000));
+  nosign_0_0 = b.umax(i32_ty, nosign_0_0, b.i32_val(0x38000000));
+  nosign_0_0 = b.umin(i32_ty, nosign_0_0, b.i32_val(0x57e00000));
+  Value nosign_0_1 = b.and_(i32_ty, nosign0, b.i32_val(0x0000ffff));
+  nosign_0_1 = b.umax(i32_ty, nosign_0_1, b.i32_val(0x3800));
+  nosign_0_1 = b.umin(i32_ty, nosign_0_1, b.i32_val(0x57e0));
+  nosign0 = b.or_(i32_ty, nosign_0_0, nosign_0_1);
+
+  Value nosign_1_0 = b.and_(i32_ty, nosign1, b.i32_val(0xffff0000));
+  nosign_1_0 = b.umax(i32_ty, nosign_1_0, b.i32_val(0x38000000));
+  nosign_1_0 = b.umin(i32_ty, nosign_1_0, b.i32_val(0x57e00000));
+  Value nosign_1_1 = b.and_(i32_ty, nosign1, b.i32_val(0x0000ffff));
+  nosign_1_1 = b.umax(i32_ty, nosign_1_1, b.i32_val(0x3800));
+  nosign_1_1 = b.umin(i32_ty, nosign_1_1, b.i32_val(0x57e0));
+  nosign1 = b.or_(i32_ty, nosign_1_0, nosign_1_1);
+
+  nosign0 = b.add(i32_ty, nosign0, b.i32_val(0x00100010));
+  nosign1 = b.add(i32_ty, nosign1, b.i32_val(0x00100010));
+  nosign0 = b.sub(i32_ty, nosign0, b.i32_val(0x38003800));
+  nosign1 = b.sub(i32_ty, nosign1, b.i32_val(0x38003800));
+  nosign0 = b.shl(i32_ty, nosign0, b.i32_val(3));
+  nosign1 = b.shl(i32_ty, nosign1, b.i32_val(3));
+
+  nosign0 = b.bitcast(nosign0, fp8x4VecTy);
+  nosign1 = b.bitcast(nosign1, fp8x4VecTy);
+  Value nosign = b.undef(fp8x4VecTy);
+  nosign = b.insert_element(fp8x4VecTy, nosign,
+                            b.extract_element(i8_ty, nosign0, b.i32_val(1)),
+                            b.i32_val(0));
+  nosign = b.insert_element(fp8x4VecTy, nosign,
+                            b.extract_element(i8_ty, nosign0, b.i32_val(3)),
+                            b.i32_val(1));
+  nosign = b.insert_element(fp8x4VecTy, nosign,
+                            b.extract_element(i8_ty, nosign1, b.i32_val(1)),
+                            b.i32_val(2));
+  nosign = b.insert_element(fp8x4VecTy, nosign,
+                            b.extract_element(i8_ty, nosign1, b.i32_val(3)),
+                            b.i32_val(3));
+  nosign = b.bitcast(nosign, i32_ty);
+
+  Value fp8x4Vec = b.or_(i32_ty, nosign, sign);
+  fp8x4Vec = b.bitcast(fp8x4Vec, fp8x4VecTy);
+  return {b.extract_element(i8_ty, fp8x4Vec, b.i32_val(0)),
+          b.extract_element(i8_ty, fp8x4Vec, b.i32_val(1)),
+          b.extract_element(i8_ty, fp8x4Vec, b.i32_val(2)),
+          b.extract_element(i8_ty, fp8x4Vec, b.i32_val(3))};
 }
 
 // ROCM type conversion between fp8 and bf16
@@ -583,33 +598,34 @@ Bf16_to_Fp8E5M2FNUZ(Location loc, ConversionPatternRewriter &rewriter,
 static Value Fp8E4M3FNUZ_to_Fp16_oneValue(Location loc,
                                           ConversionPatternRewriter &rewriter,
                                           Value v) {
+  auto tb = TritonLLVMOpBuilder(loc, rewriter);
   auto fp8x2VecTy = vec_ty(i8_ty, 2);
-  Value a = undef(fp8x2VecTy);
-  a = insert_element(fp8x2VecTy, a, int_val(8, 0), i32_val(0));
-  a = insert_element(fp8x2VecTy, a, v, i32_val(1));
-  a = bitcast(a, i16_ty);
+  Value a = tb.undef(fp8x2VecTy);
+  a = tb.insert_element(fp8x2VecTy, a, tb.int_val(8, 0), tb.i32_val(0));
+  a = tb.insert_element(fp8x2VecTy, a, v, tb.i32_val(1));
+  a = tb.bitcast(a, i16_ty);
 
-  auto e_mask = int_val(16, 0x7A00);
-  auto e = and_(i16_ty, a, e_mask);
+  auto e_mask = tb.int_val(16, 0x7A00);
+  auto e = tb.and_(i16_ty, a, e_mask);
 
-  auto m = and_(i16_ty, a, int_val(16, 0x0700));
-  auto sign = and_(i16_ty, a, int_val(16, 0x8000));
+  auto m = tb.and_(i16_ty, a, tb.int_val(16, 0x0700));
+  auto sign = tb.and_(i16_ty, a, tb.int_val(16, 0x8000));
 
   // check whether all exponents are zeros
-  auto e_is_zero = icmp_eq(e, int_val(16, 0x0));
-  auto b = and_(i16_ty, a, int_val(16, 0x7FFF));
-  auto b1 = lshr(i16_ty, b, int_val(16, 1));
+  auto e_is_zero = tb.icmp_eq(e, tb.int_val(16, 0x0));
+  auto b = tb.and_(i16_ty, a, tb.int_val(16, 0x7FFF));
+  auto b1 = tb.lshr(i16_ty, b, tb.int_val(16, 1));
 
   // case 1, e is nonzero, add exponent by 6
-  auto o0v = add(i16_ty, b1, int_val(16, 0x0C00));
-  auto o0 = or_(i16_ty, o0v, sign);
+  auto o0v = tb.add(i16_ty, b1, tb.int_val(16, 0x0C00));
+  auto o0 = tb.or_(i16_ty, o0v, sign);
 
   // case 2, e is nonzero, add exponent by 7
-  auto o1v = add(i16_ty, b1, int_val(16, 0x1C00));
-  auto o1 = or_(i16_ty, o1v, sign);
+  auto o1v = tb.add(i16_ty, b1, tb.int_val(16, 0x1C00));
+  auto o1 = tb.or_(i16_ty, o1v, sign);
 
-  auto io = select(e_is_zero, o0, o1);
-  return bitcast(io, f16_ty);
+  auto io = tb.select(e_is_zero, o0, o1);
+  return tb.bitcast(io, f16_ty);
 }
 
 static SmallVector<Value>
@@ -636,34 +652,35 @@ static ConverterT Fp8E4M3FNUZ_to_Fp16(AMD::ISAFamily isaFamily) {
 static Value Fp16_to_Fp8E4M3FNUZ_oneValue(Location loc,
                                           ConversionPatternRewriter &rewriter,
                                           Value v) {
-  auto vi16 = bitcast(v, i16_ty);
-  auto e10 = and_(vi16, int_val(16, 0x7C00));
-  auto e = lshr(i16_ty, e10, int_val(16, 10));
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  auto vi16 = b.bitcast(v, i16_ty);
+  auto e10 = b.and_(vi16, b.int_val(16, 0x7C00));
+  auto e = b.lshr(i16_ty, e10, b.int_val(16, 10));
 
-  auto s = and_(i16_ty, vi16, int_val(16, 0x8000));
+  auto s = b.and_(i16_ty, vi16, b.int_val(16, 0x8000));
 
-  auto m7 = and_(i16_ty, vi16, int_val(16, 0x0380));
-  auto m = shl(i16_ty, m7, int_val(16, 1));
+  auto m7 = b.and_(i16_ty, vi16, b.int_val(16, 0x0380));
+  auto m = b.shl(i16_ty, m7, b.int_val(16, 1));
 
   // three cases:
   //  1) e > 21 --> e = 1111,
   //  2) e <= 7 ---> e = 0,
   //  3) others, normal conversion
-  auto e1 = int_val(16, 0x7800);
-  auto e2 = int_val(16, 0x0);
-  auto e31 = sub(i16_ty, e10, int_val(16, 0x1C00));
-  auto e3 = shl(i16_ty, e31, int_val(16, 1));
+  auto e1 = b.int_val(16, 0x7800);
+  auto e2 = b.int_val(16, 0x0);
+  auto e31 = b.sub(i16_ty, e10, b.int_val(16, 0x1C00));
+  auto e3 = b.shl(i16_ty, e31, b.int_val(16, 1));
 
-  auto c13 = icmp_sgt(e, int_val(16, 21));
-  auto e13 = select(c13, e1, e3);
-  auto c23 = icmp_sle(e, int_val(16, 7));
-  auto re = select(c23, e2, e13);
+  auto c13 = b.icmp_sgt(e, b.int_val(16, 21));
+  auto e13 = b.select(c13, e1, e3);
+  auto c23 = b.icmp_sle(e, b.int_val(16, 7));
+  auto re = b.select(c23, e2, e13);
 
-  auto r = or_(i16_ty, s, or_(i16_ty, re, m));
+  auto r = b.or_(i16_ty, s, b.or_(i16_ty, re, m));
   auto fp8x2VecTy = vec_ty(i8_ty, 2);
-  auto res = bitcast(r, fp8x2VecTy);
+  auto res = b.bitcast(r, fp8x2VecTy);
 
-  return extract_element(i8_ty, res, i32_val(1));
+  return b.extract_element(i8_ty, res, b.i32_val(1));
 }
 
 static SmallVector<Value>
@@ -691,121 +708,123 @@ static ConverterT Fp16_to_Fp8E4M3FNUZ(AMD::ISAFamily isaFamily) {
 static SmallVector<Value> Fp8E4M3_to_Bf16(Location loc,
                                           ConversionPatternRewriter &rewriter,
                                           const SmallVector<Value> &v) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto fp8x4VecTy = vec_ty(i8_ty, 4);
-  Value a0 = undef(fp8x4VecTy);
-  a0 = insert_element(fp8x4VecTy, a0, int_val(8, 0), i32_val(0));
-  a0 = insert_element(fp8x4VecTy, a0, v[0], i32_val(1));
-  a0 = insert_element(fp8x4VecTy, a0, int_val(8, 0), i32_val(2));
-  a0 = insert_element(fp8x4VecTy, a0, v[1], i32_val(3));
-  a0 = bitcast(a0, i32_ty);
-
-  Value a1 = undef(fp8x4VecTy);
-  a1 = insert_element(fp8x4VecTy, a1, int_val(8, 0), i32_val(0));
-  a1 = insert_element(fp8x4VecTy, a1, v[2], i32_val(1));
-  a1 = insert_element(fp8x4VecTy, a1, int_val(8, 0), i32_val(2));
-  a1 = insert_element(fp8x4VecTy, a1, v[3], i32_val(3));
-  a1 = bitcast(a1, i32_ty);
-
-  Value b0 = and_(i32_ty, a0, i32_val(0x7fff7fff));
-  Value b1 = and_(i32_ty, a1, i32_val(0x7fff7fff));
-  b0 = lshr(i32_ty, b0, i32_val(4));
-  b1 = lshr(i32_ty, b1, i32_val(4));
-
-  b0 = add(i32_ty, b0, i32_val(0x3c003c00));
-  b1 = add(i32_ty, b1, i32_val(0x3c003c00));
-  Value sign0 = and_(i32_ty, a0, i32_val(0x80008000));
-  Value sign1 = and_(i32_ty, a1, i32_val(0x80008000));
+  Value a0 = b.undef(fp8x4VecTy);
+  a0 = b.insert_element(fp8x4VecTy, a0, b.int_val(8, 0), b.i32_val(0));
+  a0 = b.insert_element(fp8x4VecTy, a0, v[0], b.i32_val(1));
+  a0 = b.insert_element(fp8x4VecTy, a0, b.int_val(8, 0), b.i32_val(2));
+  a0 = b.insert_element(fp8x4VecTy, a0, v[1], b.i32_val(3));
+  a0 = b.bitcast(a0, i32_ty);
+
+  Value a1 = b.undef(fp8x4VecTy);
+  a1 = b.insert_element(fp8x4VecTy, a1, b.int_val(8, 0), b.i32_val(0));
+  a1 = b.insert_element(fp8x4VecTy, a1, v[2], b.i32_val(1));
+  a1 = b.insert_element(fp8x4VecTy, a1, b.int_val(8, 0), b.i32_val(2));
+  a1 = b.insert_element(fp8x4VecTy, a1, v[3], b.i32_val(3));
+  a1 = b.bitcast(a1, i32_ty);
+
+  Value b0 = b.and_(i32_ty, a0, b.i32_val(0x7fff7fff));
+  Value b1 = b.and_(i32_ty, a1, b.i32_val(0x7fff7fff));
+  b0 = b.lshr(i32_ty, b0, b.i32_val(4));
+  b1 = b.lshr(i32_ty, b1, b.i32_val(4));
+
+  b0 = b.add(i32_ty, b0, b.i32_val(0x3c003c00));
+  b1 = b.add(i32_ty, b1, b.i32_val(0x3c003c00));
+  Value sign0 = b.and_(i32_ty, a0, b.i32_val(0x80008000));
+  Value sign1 = b.and_(i32_ty, a1, b.i32_val(0x80008000));
 
   auto bf16x2VecTy = vec_ty(bf16_ty, 2);
-  Value bf16x2Vec0 = or_(i32_ty, sign0, b0);
-  Value bf16x2Vec1 = or_(i32_ty, sign1, b1);
-  bf16x2Vec0 = bitcast(bf16x2Vec0, bf16x2VecTy);
-  bf16x2Vec1 = bitcast(bf16x2Vec1, bf16x2VecTy);
-
-  return {extract_element(bf16_ty, bf16x2Vec0, i32_val(0)),
-          extract_element(bf16_ty, bf16x2Vec0, i32_val(1)),
-          extract_element(bf16_ty, bf16x2Vec1, i32_val(0)),
-          extract_element(bf16_ty, bf16x2Vec1, i32_val(1))};
+  Value bf16x2Vec0 = b.or_(i32_ty, sign0, b0);
+  Value bf16x2Vec1 = b.or_(i32_ty, sign1, b1);
+  bf16x2Vec0 = b.bitcast(bf16x2Vec0, bf16x2VecTy);
+  bf16x2Vec1 = b.bitcast(bf16x2Vec1, bf16x2VecTy);
+
+  return {b.extract_element(bf16_ty, bf16x2Vec0, b.i32_val(0)),
+          b.extract_element(bf16_ty, bf16x2Vec0, b.i32_val(1)),
+          b.extract_element(bf16_ty, bf16x2Vec1, b.i32_val(0)),
+          b.extract_element(bf16_ty, bf16x2Vec1, b.i32_val(1))};
 }
 
 static SmallVector<Value> Bf16_to_Fp8E4M3(Location loc,
                                           ConversionPatternRewriter &rewriter,
                                           const SmallVector<Value> &v) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto bf16x2VecTy = vec_ty(bf16_ty, 2);
-  Value bf16x2Vec0 = undef(bf16x2VecTy);
-  Value bf16x2Vec1 = undef(bf16x2VecTy);
-  bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v[0], i32_val(0));
-  bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v[1], i32_val(1));
-  bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v[2], i32_val(0));
-  bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v[3], i32_val(1));
-  bf16x2Vec0 = bitcast(bf16x2Vec0, i32_ty);
-  bf16x2Vec1 = bitcast(bf16x2Vec1, i32_ty);
-
-  Value sign0 = and_(i32_ty, bf16x2Vec0, i32_val(0x80008000));
-  Value sign1 = and_(i32_ty, bf16x2Vec1, i32_val(0x80008000));
+  Value bf16x2Vec0 = b.undef(bf16x2VecTy);
+  Value bf16x2Vec1 = b.undef(bf16x2VecTy);
+  bf16x2Vec0 = b.insert_element(bf16x2VecTy, bf16x2Vec0, v[0], b.i32_val(0));
+  bf16x2Vec0 = b.insert_element(bf16x2VecTy, bf16x2Vec0, v[1], b.i32_val(1));
+  bf16x2Vec1 = b.insert_element(bf16x2VecTy, bf16x2Vec1, v[2], b.i32_val(0));
+  bf16x2Vec1 = b.insert_element(bf16x2VecTy, bf16x2Vec1, v[3], b.i32_val(1));
+  bf16x2Vec0 = b.bitcast(bf16x2Vec0, i32_ty);
+  bf16x2Vec1 = b.bitcast(bf16x2Vec1, i32_ty);
+
+  Value sign0 = b.and_(i32_ty, bf16x2Vec0, b.i32_val(0x80008000));
+  Value sign1 = b.and_(i32_ty, bf16x2Vec1, b.i32_val(0x80008000));
   auto fp8x4VecTy = vec_ty(i8_ty, 4);
-  Value sign = undef(fp8x4VecTy);
-  sign0 = bitcast(sign0, fp8x4VecTy);
-  sign1 = bitcast(sign1, fp8x4VecTy);
-  sign = insert_element(fp8x4VecTy, sign,
-                        extract_element(i8_ty, sign0, i32_val(1)), i32_val(0));
-  sign = insert_element(fp8x4VecTy, sign,
-                        extract_element(i8_ty, sign0, i32_val(3)), i32_val(1));
-  sign = insert_element(fp8x4VecTy, sign,
-                        extract_element(i8_ty, sign1, i32_val(1)), i32_val(2));
-  sign = insert_element(fp8x4VecTy, sign,
-                        extract_element(i8_ty, sign1, i32_val(3)), i32_val(3));
-  sign = bitcast(sign, i32_ty);
-
-  Value nosign0 = and_(i32_ty, bf16x2Vec0, i32_val(0x7fff7fff));
-  Value nosign1 = and_(i32_ty, bf16x2Vec1, i32_val(0x7fff7fff));
-
-  Value nosign_0_0 = and_(i32_ty, nosign0, i32_val(0xffff0000));
-  nosign_0_0 = umax(i32_ty, nosign_0_0, i32_val(0x3c000000));
-  nosign_0_0 = umin(i32_ty, nosign_0_0, i32_val(0x43f00000));
-  Value nosign_0_1 = and_(i32_ty, nosign0, i32_val(0x0000ffff));
-  nosign_0_1 = umax(i32_ty, nosign_0_1, i32_val(0x3c00));
-  nosign_0_1 = umin(i32_ty, nosign_0_1, i32_val(0x43f0));
-  nosign0 = or_(i32_ty, nosign_0_0, nosign_0_1);
-
-  Value nosign_1_0 = and_(i32_ty, nosign1, i32_val(0xffff0000));
-  nosign_1_0 = umax(i32_ty, nosign_1_0, i32_val(0x3c000000));
-  nosign_1_0 = umin(i32_ty, nosign_1_0, i32_val(0x43f00000));
-  Value nosign_1_1 = and_(i32_ty, nosign1, i32_val(0x0000ffff));
-  nosign_1_1 = umax(i32_ty, nosign_1_1, i32_val(0x3c00));
-  nosign_1_1 = umin(i32_ty, nosign_1_1, i32_val(0x43f0));
-  nosign1 = or_(i32_ty, nosign_1_0, nosign_1_1);
-
-  nosign0 = add(i32_ty, nosign0, i32_val(0x80008));
-  nosign1 = add(i32_ty, nosign1, i32_val(0x80008));
-  nosign0 = sub(i32_ty, nosign0, i32_val(0x3c003c00));
-  nosign1 = sub(i32_ty, nosign1, i32_val(0x3c003c00));
-  nosign0 = lshr(i32_ty, nosign0, i32_val(4));
-  nosign1 = lshr(i32_ty, nosign1, i32_val(4));
-
-  nosign0 = bitcast(nosign0, fp8x4VecTy);
-  nosign1 = bitcast(nosign1, fp8x4VecTy);
-  Value nosign = undef(fp8x4VecTy);
+  Value sign = b.undef(fp8x4VecTy);
+  sign0 = b.bitcast(sign0, fp8x4VecTy);
+  sign1 = b.bitcast(sign1, fp8x4VecTy);
+  sign = b.insert_element(fp8x4VecTy, sign,
+                          b.extract_element(i8_ty, sign0, b.i32_val(1)), b.i32_val(0));
+  sign = b.insert_element(fp8x4VecTy, sign,
+                          b.extract_element(i8_ty, sign0, b.i32_val(3)), b.i32_val(1));
+  sign = b.insert_element(fp8x4VecTy, sign,
+                          b.extract_element(i8_ty, sign1, b.i32_val(1)), b.i32_val(2));
+  sign = b.insert_element(fp8x4VecTy, sign,
+                          b.extract_element(i8_ty, sign1, b.i32_val(3)), b.i32_val(3));
+  sign = b.bitcast(sign, i32_ty);
+
+  Value nosign0 = b.and_(i32_ty, bf16x2Vec0, b.i32_val(0x7fff7fff));
+  Value nosign1 = b.and_(i32_ty, bf16x2Vec1, b.i32_val(0x7fff7fff));
+
+  Value nosign_0_0 = b.and_(i32_ty, nosign0, b.i32_val(0xffff0000));
+  nosign_0_0 = b.umax(i32_ty, nosign_0_0, b.i32_val(0x3c000000));
+  nosign_0_0 = b.umin(i32_ty, nosign_0_0, b.i32_val(0x43f00000));
+  Value nosign_0_1 = b.and_(i32_ty, nosign0, b.i32_val(0x0000ffff));
+  nosign_0_1 = b.umax(i32_ty, nosign_0_1, b.i32_val(0x3c00));
+  nosign_0_1 = b.umin(i32_ty, nosign_0_1, b.i32_val(0x43f0));
+  nosign0 = b.or_(i32_ty, nosign_0_0, nosign_0_1);
+
+  Value nosign_1_0 = b.and_(i32_ty, nosign1, b.i32_val(0xffff0000));
+  nosign_1_0 = b.umax(i32_ty, nosign_1_0, b.i32_val(0x3c000000));
+  nosign_1_0 = b.umin(i32_ty, nosign_1_0, b.i32_val(0x43f00000));
+  Value nosign_1_1 = b.and_(i32_ty, nosign1, b.i32_val(0x0000ffff));
+  nosign_1_1 = b.umax(i32_ty, nosign_1_1, b.i32_val(0x3c00));
+  nosign_1_1 = b.umin(i32_ty, nosign_1_1, b.i32_val(0x43f0));
+  nosign1 = b.or_(i32_ty, nosign_1_0, nosign_1_1);
+
+  nosign0 = b.add(i32_ty, nosign0, b.i32_val(0x80008));
+  nosign1 = b.add(i32_ty, nosign1, b.i32_val(0x80008));
+  nosign0 = b.sub(i32_ty, nosign0, b.i32_val(0x3c003c00));
+  nosign1 = b.sub(i32_ty, nosign1, b.i32_val(0x3c003c00));
+  nosign0 = b.lshr(i32_ty, nosign0, b.i32_val(4));
+  nosign1 = b.lshr(i32_ty, nosign1, b.i32_val(4));
+
+  nosign0 = b.bitcast(nosign0, fp8x4VecTy);
+  nosign1 = b.bitcast(nosign1, fp8x4VecTy);
+  Value nosign = b.undef(fp8x4VecTy);
   nosign =
-      insert_element(fp8x4VecTy, nosign,
-                     extract_element(i8_ty, nosign0, i32_val(0)), i32_val(0));
+      b.insert_element(fp8x4VecTy, nosign,
+                       b.extract_element(i8_ty, nosign0, b.i32_val(0)), b.i32_val(0));
   nosign =
-      insert_element(fp8x4VecTy, nosign,
-                     extract_element(i8_ty, nosign0, i32_val(2)), i32_val(1));
+      b.insert_element(fp8x4VecTy, nosign,
+                       b.extract_element(i8_ty, nosign0, b.i32_val(2)), b.i32_val(1));
   nosign =
-      insert_element(fp8x4VecTy, nosign,
-                     extract_element(i8_ty, nosign1, i32_val(0)), i32_val(2));
+      b.insert_element(fp8x4VecTy, nosign,
+                       b.extract_element(i8_ty, nosign1, b.i32_val(0)), b.i32_val(2));
   nosign =
-      insert_element(fp8x4VecTy, nosign,
-                     extract_element(i8_ty, nosign1, i32_val(2)), i32_val(3));
-  nosign = bitcast(nosign, i32_ty);
-
-  Value fp8x4Vec = or_(i32_ty, nosign, sign);
-  fp8x4Vec = bitcast(fp8x4Vec, fp8x4VecTy);
-  return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),
-          extract_element(i8_ty, fp8x4Vec, i32_val(1)),
-          extract_element(i8_ty, fp8x4Vec, i32_val(2)),
-          extract_element(i8_ty, fp8x4Vec, i32_val(3))};
+      b.insert_element(fp8x4VecTy, nosign,
+                       b.extract_element(i8_ty, nosign1, b.i32_val(2)), b.i32_val(3));
+  nosign = b.bitcast(nosign, i32_ty);
+
+  Value fp8x4Vec = b.or_(i32_ty, nosign, sign);
+  fp8x4Vec = b.bitcast(fp8x4Vec, fp8x4VecTy);
+  return {b.extract_element(i8_ty, fp8x4Vec, b.i32_val(0)),
+          b.extract_element(i8_ty, fp8x4Vec, b.i32_val(1)),
+          b.extract_element(i8_ty, fp8x4Vec, b.i32_val(2)),
+          b.extract_element(i8_ty, fp8x4Vec, b.i32_val(3))};
 }
 
 template <typename SourceOp, typename DestOp>
@@ -909,6 +928,7 @@ struct FpToFpOpConversion
                                    ConversionPatternRewriter &rewriter,
                                    Type elemTy, MultipleOperandsRange operands,
                                    Location loc) const {
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto srcElementType = getElementType(op.getSrc());
     auto dstElementType = getElementType(op.getResult());
     auto roundingMode = op.getRounding();
@@ -973,7 +993,7 @@ struct FpToFpOpConversion
       for (Value &v : inVals)
         v = cvtFp32ToFp16(loc, rewriter, v,
                           roundingMode.value_or(RoundingMode::RTNE));
-    inVals.resize(numElements, undef(typeConverter->convertType(srcType)));
+    inVals.resize(numElements, b.undef(typeConverter->convertType(srcType)));
     SmallVector<Value> outVals;
     if (srcType != dstType) {
       auto getCvtFunc = getConversionFunc(srcType, dstType, roundingMode);
@@ -1102,10 +1122,11 @@ struct FSubOpConversion
 static SmallVector<Value> S8_to_Bf16(Location loc,
                                      ConversionPatternRewriter &rewriter,
                                      const SmallVector<Value> &v) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   SmallVector<Value> inValues = {v[0], v[1], v[2], v[3]};
   SmallVector<Value> outValues = {};
   for (Value inVal : inValues) {
-    Value i32Val = sext(i32_ty, inVal);
+    Value i32Val = b.sext(i32_ty, inVal);
 
     GCNBuilder builder;
     auto &cvt = *builder.create("v_cvt_f32_i32");
@@ -1114,10 +1135,10 @@ static SmallVector<Value> S8_to_Bf16(Location loc,
     cvt(res, operand);
     auto f32Val = builder.launch(rewriter, loc, f32_ty, false);
 
-    f32Val = bitcast(f32Val, i32_ty);
-    auto shifted = lshr(i32_ty, f32Val, i32_val(16));
-    auto truncated = trunc(i16_ty, shifted);
-    outValues.push_back(bitcast(truncated, bf16_ty));
+    f32Val = b.bitcast(f32Val, i32_ty);
+    auto shifted = b.lshr(i32_ty, f32Val, b.i32_val(16));
+    auto truncated = b.trunc(i16_ty, shifted);
+    outValues.push_back(b.bitcast(truncated, bf16_ty));
   }
   return outValues;
 }
@@ -1228,12 +1249,13 @@ struct ExpOpConversionApprox
                                    ConversionPatternRewriter &rewriter,
                                    Type elemTy, MultipleOperandsRange operands,
                                    Location loc) const {
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     // For non-FP32 input, call __ocml_exp_f64 for higher-precision calculation
     if (elemTy.getIntOrFloatBitWidth() != 32)
       return {};
 
     const double log2e = 1.4426950408889634;
-    Value prod = fmul(f32_ty, operands[0][0], f32_val(log2e));
+    Value prod = b.fmul(f32_ty, operands[0][0], b.f32_val(log2e));
 
     // Here we use llvm.exp2.f32 instead of math::Exp2Op. The latter
     // flushes denorms by default, but we want to preserve denorms by default
diff --git a/third_party/amd/lib/TritonAMDGPUToLLVM/LoadStoreOpToLLVM.cpp b/third_party/amd/lib/TritonAMDGPUToLLVM/LoadStoreOpToLLVM.cpp
index a45efd4a7..afc2c6e3a 100644
--- a/third_party/amd/lib/TritonAMDGPUToLLVM/LoadStoreOpToLLVM.cpp
+++ b/third_party/amd/lib/TritonAMDGPUToLLVM/LoadStoreOpToLLVM.cpp
@@ -28,9 +28,10 @@ namespace {
 // Used to mask out the redundant data accessed by threads.
 Value redundantDataMask(Type valueTy, ConversionPatternRewriter &rewriter,
                         Location loc, const AMD::TargetInfo &targetInfo) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto tensorTy = dyn_cast<RankedTensorType>(valueTy);
-  Value mask = int_val(1, 1);
-  auto tid = tid_val();
+  Value mask = b.int_val(1, 1);
+  auto tid = b.tid_val();
   auto clusterCTAId = targetInfo.getClusterCTAId(rewriter, loc);
   if (tensorTy) {
     auto layout = tensorTy.getEncoding();
@@ -41,9 +42,9 @@ Value redundantDataMask(Type valueTy, ConversionPatternRewriter &rewriter,
     auto warpsPerCTA = triton::gpu::getWarpsPerCTA(layout);
     auto order = triton::gpu::getOrder(layout);
     auto shapePerCTATile = triton::gpu::getShapePerCTATile(layout, shape);
-    Value warpSize = i32_val(triton::gpu::getWarpSize(layout));
-    Value laneId = urem(tid, warpSize);
-    Value warpId = udiv(tid, warpSize);
+    Value warpSize = b.i32_val(triton::gpu::getWarpSize(layout));
+    Value laneId = b.urem(tid, warpSize);
+    Value warpId = b.udiv(tid, warpSize);
     SmallVector<Value> multiDimWarpId =
         delinearize(rewriter, loc, warpId, warpsPerCTA, order);
     SmallVector<Value> multiDimThreadId =
@@ -55,14 +56,15 @@ Value redundantDataMask(Type valueTy, ConversionPatternRewriter &rewriter,
       // Otherwise, we need to mask threads that will replicate data on this
       // dimension. Calculate the thread index on this dimension for the CTA
       Value threadDim =
-          add(mul(multiDimWarpId[dim], i32_val(threadsPerWarp[dim])),
-              multiDimThreadId[dim]);
-      mask = and_(mask, icmp_slt(mul(threadDim, i32_val(sizePerThread[dim])),
-                                 i32_val(shape[dim])));
+          b.add(b.mul(multiDimWarpId[dim], b.i32_val(threadsPerWarp[dim])),
+                multiDimThreadId[dim]);
+      mask = b.and_(mask,
+                    b.icmp_slt(b.mul(threadDim, b.i32_val(sizePerThread[dim])),
+                               b.i32_val(shape[dim])));
     }
     // Do not write duplicated data when multicast is enabled
     if (triton::gpu::getNumCTAs(layout) > 1) {
-      auto _0 = i32_val(0);
+      auto _0 = b.i32_val(0);
       auto CTAsPerCGA = triton::gpu::getCTAsPerCGA(layout);
       auto CTASplitNum = triton::gpu::getCTASplitNum(layout);
       auto CTAOrder = triton::gpu::getCTAOrder(layout);
@@ -76,7 +78,7 @@ Value redundantDataMask(Type valueTy, ConversionPatternRewriter &rewriter,
           continue;
         // This wrapping rule must be consistent with emitCTAOffsetForLayout
         unsigned splitNum = std::min<unsigned>(shape[dim], CTASplitNum[dim]);
-        Value repId = udiv(multiDimClusterCTAId[dim], i32_val(splitNum));
+        Value repId = b.udiv(multiDimClusterCTAId[dim], b.i32_val(splitNum));
         // Consider the example where CTAsPerCGA = [4] and CTASplitNum = [2]:
         //     CTA0 and CTA2 holds data of block0,
         //     CTA1 and CTA3 holds data of block1.
@@ -86,14 +88,14 @@ Value redundantDataMask(Type valueTy, ConversionPatternRewriter &rewriter,
         // Actually in all existing cases of multicast, splitNum is always 1.
         // The mask is equivalent to:
         //     multiDimClusterCTAId[dim] == 0
-        mask = and_(mask, icmp_eq(repId, _0));
+        mask = b.and_(mask, b.icmp_eq(repId, _0));
       }
     }
   } else {
     // If the tensor is not ranked, then it is a scalar and only thread 0 of
     // CTA0 can write
-    mask = and_(mask, icmp_eq(clusterCTAId, i32_val(0)));
-    mask = and_(mask, icmp_eq(tid, i32_val(0)));
+    mask = b.and_(mask, b.icmp_eq(clusterCTAId, b.i32_val(0)));
+    mask = b.and_(mask, b.icmp_eq(tid, b.i32_val(0)));
   }
   return mask;
 }
@@ -121,14 +123,15 @@ struct LoadStoreConversionBase {
                                    const LLVMTypeConverter *typeConverter,
                                    Location loc, VectorType vecTy,
                                    ArrayRef<Value> elems, int64_t start) const {
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     int64_t vec = vecTy.getNumElements();
     // If we need to mask the loaded value with other elements
-    Value v = undef(vecTy);
+    Value v = b.undef(vecTy);
     for (size_t s = 0; s < vec; ++s) {
       Value otherElem = elems[start + s];
       Value indexVal =
           LLVM::createIndexConstant(rewriter, loc, typeConverter, s);
-      v = insert_element(vecTy, v, otherElem, indexVal);
+      v = b.insert_element(vecTy, v, otherElem, indexVal);
     }
     return v;
   }
@@ -234,6 +237,7 @@ struct LoadOpConversion : public ConvertOpToLLVMPattern<triton::LoadOp>,
   matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
     auto loc = op->getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
 
     // original values
     Value ptr = op.getPtr();
@@ -285,9 +289,9 @@ struct LoadOpConversion : public ConvertOpToLLVMPattern<triton::LoadOp>,
       const size_t movWidth = width < 16 ? 16 : width;
       assert(wordNElems * nWords * numVecs == numElems);
 
-      Value pred = mask ? maskElems[vecStart] : int_val(1, 1);
+      Value pred = mask ? maskElems[vecStart] : b.int_val(1, 1);
       auto vecTy = LLVM::getFixedVectorType(valueElemTy, vec);
-      Value ptr = addrspacecast(ptr_ty(getContext()), ptrElems[vecStart]);
+      Value ptr = b.addrspacecast(ptr_ty(getContext()), ptrElems[vecStart]);
 
       Value falseVal = createZeroVector(rewriter, loc, cast<VectorType>(vecTy));
       // If we need to mask the loaded value with other elements
@@ -301,7 +305,7 @@ struct LoadOpConversion : public ConvertOpToLLVMPattern<triton::LoadOp>,
       for (size_t ii = 0; ii < vec; ++ii) {
         Value vecIdx = createIndexAttrConstant(
             rewriter, loc, this->getTypeConverter()->getIndexType(), ii);
-        Value loaded = extract_element(valueElemTy, loadVal, vecIdx);
+        Value loaded = b.extract_element(valueElemTy, loadVal, vecIdx);
         loadedVals.push_back(loaded);
       }
     } // end vec
@@ -332,6 +336,7 @@ struct BufferLoadOpConversion
   matchAndRewrite(triton::amdgpu::BufferLoadOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
     auto loc = op->getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     LLVM::AMD::BufferEmitter bufferEmitter(rewriter, loc, targetInfo);
 
     // original values
@@ -372,7 +377,7 @@ struct BufferLoadOpConversion
     SmallVector<Value> loadedVals;
     Type vecTy = LLVM::getFixedVectorType(valueElemTy, vec);
     for (size_t vecStart = 0; vecStart < numElems; vecStart += vec) {
-      Value pred = mask ? maskElems[vecStart] : int_val(1, 1);
+      Value pred = mask ? maskElems[vecStart] : b.int_val(1, 1);
       Value falseVal = createZeroVector(rewriter, loc, cast<VectorType>(vecTy));
       if (otherElems.size() != 0)
         falseVal = packElementRangeIntoVector(
@@ -383,7 +388,7 @@ struct BufferLoadOpConversion
       for (size_t ii = 0; ii < vec; ++ii) {
         Value vecIdx = createIndexAttrConstant(
             rewriter, loc, this->getTypeConverter()->getIndexType(), ii);
-        Value loaded = extract_element(valueElemTy, loadVal, vecIdx);
+        Value loaded = b.extract_element(valueElemTy, loadVal, vecIdx);
         loadedVals.push_back(loaded);
       }
     } // end vec
@@ -419,6 +424,7 @@ struct StoreOpConversion : public ConvertOpToLLVMPattern<triton::StoreOp>,
     Value llValue = adaptor.getValue();
 
     auto loc = op->getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     MLIRContext *ctx = rewriter.getContext();
 
     auto valueTy = value.getType();
@@ -445,7 +451,7 @@ struct StoreOpConversion : public ConvertOpToLLVMPattern<triton::StoreOp>,
     const int numVecs = elemsPerThread / vec;
     Value rDataMask = redundantDataMask(valueTy, rewriter, loc, targetInfo);
     for (size_t vecStart = 0; vecStart < elemsPerThread; vecStart += vec) {
-      Value pred = mask ? and_(maskElems[vecStart], rDataMask) : rDataMask;
+      Value pred = mask ? b.and_(maskElems[vecStart], rDataMask) : rDataMask;
       auto vecTy = LLVM::getFixedVectorType(valueElemTy, vec);
 
       const size_t maxWordWidth = std::max<size_t>(32, valueElemNBits);
@@ -457,7 +463,7 @@ struct StoreOpConversion : public ConvertOpToLLVMPattern<triton::StoreOp>,
 
       SmallVector<std::pair<Value, std::string>> asmArgs;
       Value elem = valueElems[vecStart];
-      Value ptr = addrspacecast(ptr_ty(getContext()), ptrElems[vecStart]);
+      Value ptr = b.addrspacecast(ptr_ty(getContext()), ptrElems[vecStart]);
 
       // Create the store val
       Value storeVal = packElementRangeIntoVector(
@@ -488,6 +494,7 @@ struct BufferStoreOpConversion
   matchAndRewrite(triton::amdgpu::BufferStoreOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
     auto loc = op->getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     LLVM::AMD::BufferEmitter bufferEmitter(rewriter, loc, targetInfo);
 
     // original values
@@ -522,7 +529,7 @@ struct BufferStoreOpConversion
     Value rDataMask = redundantDataMask(valueTy, rewriter, loc, targetInfo);
     for (size_t vecStart = 0; vecStart < numElems; vecStart += vec) {
       Type vecTy = LLVM::getFixedVectorType(valueElemTy, vec);
-      Value pred = mask ? and_(maskElems[vecStart], rDataMask) : rDataMask;
+      Value pred = mask ? b.and_(maskElems[vecStart], rDataMask) : rDataMask;
       // Create the store val
       Value storeVal = packElementRangeIntoVector(
           rewriter, this->getTypeConverter(), loc, cast<VectorType>(vecTy),
@@ -567,6 +574,7 @@ struct AtomicCASOpConversion
                   ConversionPatternRewriter &rewriter) const override {
     // extract relevant info from Module
     auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     MLIRContext *ctx = rewriter.getContext();
     Value ptr = op.getPtr();
 
@@ -604,11 +612,11 @@ struct AtomicCASOpConversion
 
     // atomic ops
     for (size_t i = 0; i < elemsPerThread; i += vec) {
-      Value casVal = undef(vecTy);
+      Value casVal = b.undef(vecTy);
       for (int ii = 0; ii < vec; ++ii) {
         Value iiVal = createIndexAttrConstant(
             rewriter, loc, getTypeConverter()->getIndexType(), ii);
-        casVal = insert_element(vecTy, casVal, valElements[i + ii], iiVal);
+        casVal = b.insert_element(vecTy, casVal, valElements[i + ii], iiVal);
       }
 
       Value casPtr = ptrElements[i];
@@ -626,11 +634,12 @@ struct AtomicCASOpConversion
             StringRef("agent"));
 
         // Extract the new_loaded value from the pair.
-        Value ret = extract_val(valueElemTy, cmpxchg, i);
+        Value ret = b.extract_val(valueElemTy, cmpxchg, i);
 
         for (int ii = 0; ii < vec; ++ii) {
           resultVals[i + ii] =
-              vec == 1 ? ret : extract_element(valueElemTy, ret, i32_val(ii));
+              vec == 1 ? ret
+                       : b.extract_element(valueElemTy, ret, b.i32_val(ii));
         }
       } else { // for scalar
         // Build blocks to bypass the atomic instruction for ~rmwMask.
@@ -641,8 +650,8 @@ struct AtomicCASOpConversion
 
         // Fill entry block with global memory barrier and conditional branch.
         rewriter.setInsertionPointToEnd(curBlock);
-        auto tid = tid_val();
-        Value pred = icmp_eq(tid, i32_val(i));
+        auto tid = b.tid_val();
+        Value pred = b.icmp_eq(tid, b.i32_val(i));
         rewriter.create<LLVM::CondBrOp>(loc, pred, atomicBlock, endBlock);
 
         // Build main block with atomic_cmpxchg.
@@ -656,10 +665,10 @@ struct AtomicCASOpConversion
 
         if (atomicNeedsSharedMemory(op.getResult())) {
           // Extract the new_loaded value from the pair.
-          Value newLoaded = extract_val(valueElemTy, cmpxchg, 0);
+          Value newLoaded = b.extract_val(valueElemTy, cmpxchg, 0);
           Value atomPtr =
               getSharedMemoryBase(loc, rewriter, targetInfo, op.getOperation());
-          store(newLoaded, atomPtr);
+          b.store(newLoaded, atomPtr);
         }
 
         rewriter.create<LLVM::BrOp>(loc, ValueRange(), endBlock);
@@ -675,10 +684,10 @@ struct AtomicCASOpConversion
         GCNBuilder BuilderMemfenceLDS;
         BuilderMemfenceLDS.create<>("s_waitcnt lgkmcnt(0)")->operator()();
         BuilderMemfenceLDS.launch(rewriter, loc, void_ty(ctx));
-        barrier();
+        b.barrier();
         Value atomPtr =
             getSharedMemoryBase(loc, rewriter, targetInfo, op.getOperation());
-        Value ret = load(valueElemTy, atomPtr);
+        Value ret = b.load(valueElemTy, atomPtr);
         rewriter.replaceOp(op, {ret});
       }
     }
@@ -739,6 +748,7 @@ struct AtomicRMWOpConversion
   matchAndRewrite(triton::AtomicRMWOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
     auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     MLIRContext *ctx = rewriter.getContext();
 
     auto atomicRmwAttr = op.getAtomicRmwOp();
@@ -772,10 +782,10 @@ struct AtomicRMWOpConversion
       // mask
       numElems = tensorTy.getNumElements();
     }
-    Value mask = int_val(1, 1);
-    auto tid = tid_val();
-    mask = and_(mask,
-                icmp_slt(mul(tid, i32_val(elemsPerThread)), i32_val(numElems)));
+    Value mask = b.int_val(1, 1);
+    auto tid = b.tid_val();
+    mask = b.and_(mask,
+                  b.icmp_slt(b.mul(tid, b.i32_val(elemsPerThread)), b.i32_val(numElems)));
 
     auto memOrdering = op.getSem();
     auto atomicMemOrdering = getMemoryOrdering(memOrdering);
@@ -788,9 +798,9 @@ struct AtomicRMWOpConversion
       Value rmwPtr = ptrElements[i];
       // TODO: in case llMask is zero we can create only one branch for all
       // elemsPerThread.
-      Value rmwMask = llMask ? and_(mask, maskElements[i]) : mask;
+      Value rmwMask = llMask ? b.and_(mask, maskElements[i]) : mask;
 
-      Value undefVal = undef(retType);
+      Value undefVal = b.undef(retType);
       // Build blocks to bypass the atomic instruction for ~rmwMask.
       auto *curBlock = rewriter.getInsertionBlock();
       auto *endBlock = curBlock->splitBlock(rewriter.getInsertionPoint());
@@ -822,14 +832,14 @@ struct AtomicRMWOpConversion
                     loc, *maybeKind, ptrElements[i + 1], valElements[i + 1],
                     atomicMemOrdering, StringRef("agent"))
                 .getResult();
-        auto tmp = insert_element(vecTy, undef(vecTy), atom, i32_val(0));
-        atom = insert_element(vecTy, tmp, atom2, i32_val(1)).getResult();
+        auto tmp = b.insert_element(vecTy, b.undef(vecTy), atom, b.i32_val(0));
+        atom = b.insert_element(vecTy, tmp, atom2, b.i32_val(1)).getResult();
       }
       if (!tensorTy) {
         if (atomicNeedsSharedMemory(op.getResult())) {
           Value atomPtr =
               getSharedMemoryBase(loc, rewriter, targetInfo, op.getOperation());
-          store(atom, atomPtr);
+          b.store(atom, atomPtr);
         }
       }
       rewriter.create<LLVM::BrOp>(loc, atom, endBlock);
@@ -840,7 +850,7 @@ struct AtomicRMWOpConversion
         for (int ii = 0; ii < vec; ++ii) {
           resultVals[i + ii] =
               vec == 1 ? retVal
-                       : extract_element(valueElemTy, retVal, i32_val(ii));
+                       : b.extract_element(valueElemTy, retVal, b.i32_val(ii));
         }
       } else {
         if (!atomicNeedsSharedMemory(op.getResult())) {
@@ -849,8 +859,8 @@ struct AtomicRMWOpConversion
         }
         Value atomPtr =
             getSharedMemoryBase(loc, rewriter, targetInfo, op.getOperation());
-        barrier();
-        Value ret = load(valueElemTy, atomPtr);
+        b.barrier();
+        Value ret = b.load(valueElemTy, atomPtr);
         rewriter.replaceOp(op, {ret});
       }
     }
diff --git a/third_party/amd/lib/TritonAMDGPUToLLVM/TargetInfo.cpp b/third_party/amd/lib/TritonAMDGPUToLLVM/TargetInfo.cpp
index 3a40d73c2..96ff7a7b6 100644
--- a/third_party/amd/lib/TritonAMDGPUToLLVM/TargetInfo.cpp
+++ b/third_party/amd/lib/TritonAMDGPUToLLVM/TargetInfo.cpp
@@ -26,11 +26,12 @@ LLVM::LLVMFuncOp getOrInsertFunction(T &moduleOp, const Location loc,
 Value printfPromoteValue(RewriterBase &rewriter, Value value) {
   auto *context = rewriter.getContext();
   auto loc = UnknownLoc::get(context);
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto type = value.getType();
 
   if (isa<LLVM::LLVMPointerType>(type)) {
     // The llvm.ptrtoint op requires signless integer types.
-    return ptrtoint(i64_ty, value);
+    return b.ptrtoint(i64_ty, value);
   }
 
   assert(type.getIntOrFloatBitWidth() <= 64);
@@ -38,18 +39,18 @@ Value printfPromoteValue(RewriterBase &rewriter, Value value) {
   if (auto floatType = dyn_cast<FloatType>(type)) {
     Value newValue = value;
     if (!floatType.isF64())
-      newValue = fpext(f64_ty, newValue);
-    return bitcast(newValue, i64_ty);
+      newValue = b.fpext(f64_ty, newValue);
+    return b.bitcast(newValue, i64_ty);
   }
 
   assert(type.isIntOrIndex());
   if (type.getIntOrFloatBitWidth() < 64) {
     if (type.isUnsignedInteger())
-      return zext(ui64_ty, value);
+      return b.zext(ui64_ty, value);
     if (type.isSignedInteger())
-      return sext(i64_ty, value);
+      return b.sext(i64_ty, value);
     // Signless integers are printed using unsigned integer formats.
-    return zext(i64_ty, value);
+    return b.zext(i64_ty, value);
   }
 
   return value;
@@ -139,6 +140,7 @@ void TargetInfo::printfImpl(Value formatStrStart, int formatStrByteCount,
   auto moduleOp = rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();
   auto *ctx = rewriter.getContext();
   mlir::Location loc = UnknownLoc::get(ctx);
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
 
   // See
   // https://github.com/ROCm/ROCm-Device-Libs/blob/rocm-6.0.x/ockl/src/services.cl#L263-L361
@@ -164,16 +166,16 @@ void TargetInfo::printfImpl(Value formatStrStart, int formatStrByteCount,
   // Emit the intrinsic function call to begin the printf.
   Value zeroI64 = rewriter.create<LLVM::ConstantOp>(loc, i64_ty, 0);
   Value message =
-      call(printBeginFn, useStdErr ? ValueRange() : zeroI64).getResult();
+      b.call(printBeginFn, useStdErr ? ValueRange() : zeroI64).getResult();
 
   // Emit the intrinsic function call to handle the printf format string.
-  Value oneI32 = i32_val(1);
-  Value zeroI32 = i32_val(0);
+  Value oneI32 = b.i32_val(1);
+  Value zeroI32 = b.i32_val(0);
   Value formatStrLen =
       rewriter.create<LLVM::ConstantOp>(loc, i64_ty, formatStrByteCount);
   SmallVector<Value, 4> arguments = {message, formatStrStart, formatStrLen,
                                      args.empty() ? oneI32 : zeroI32};
-  message = call(printStrFn, arguments).getResult();
+  message = b.call(printStrFn, arguments).getResult();
 
   // Emit the intrinsic function call to handle arguments iteratively.
   // We can only handle at most 7 values each time.
@@ -184,7 +186,7 @@ void TargetInfo::printfImpl(Value formatStrStart, int formatStrByteCount,
 
     SmallVector<Value, 2 + kArgsPerGroup + 1> arguments;
     arguments.push_back(message);
-    arguments.push_back(i32_val(numArgs));
+    arguments.push_back(b.i32_val(numArgs));
     for (size_t i = group; i < bound; ++i) {
       arguments.push_back(printfPromoteValue(rewriter, args[i]));
     }
@@ -195,7 +197,7 @@ void TargetInfo::printfImpl(Value formatStrStart, int formatStrByteCount,
 
     Value isLast = (bound == args.size()) ? oneI32 : zeroI32;
     arguments.push_back(isLast);
-    message = call(printArgsFn, arguments).getResult();
+    message = b.call(printArgsFn, arguments).getResult();
   }
 }
 
@@ -226,6 +228,7 @@ void TargetInfo::printf(RewriterBase &rewriter, StringRef msg,
 void TargetInfo::assertFail(RewriterBase &rewriter, Location loc,
                             StringRef message, StringRef file, StringRef func,
                             int line) const {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   // Compose and print an assert message.
   llvm::SmallString<256> msgBuffer;
   llvm::Twine("device assertion failed: '" + message + "', in " + func +
@@ -238,7 +241,7 @@ void TargetInfo::assertFail(RewriterBase &rewriter, Location loc,
 
   // Set block barrrier before aborting kernel, give a chance for all
   // the threads in a block to check/print the assert failure.
-  barrier();
+  b.barrier();
   // Perform the trap to abort the kernel.
   rewriter.create<LLVM::Trap>(loc);
 }
diff --git a/third_party/amd/lib/TritonAMDGPUToLLVM/Utility.cpp b/third_party/amd/lib/TritonAMDGPUToLLVM/Utility.cpp
index 542b1ecbb..13be5ab5c 100644
--- a/third_party/amd/lib/TritonAMDGPUToLLVM/Utility.cpp
+++ b/third_party/amd/lib/TritonAMDGPUToLLVM/Utility.cpp
@@ -44,12 +44,13 @@ std::string mangleFunc(std::string name, Type type) {
 // the same `pred` value
 Value createVectorMaskFromPredicate(RewriterBase &rewriter, Location loc,
                                     Value pred, int64_t vecSize) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto vecMaskTy = LLVM::getFixedVectorType(rewriter.getI1Type(), vecSize);
-  Value maskVal = undef(vecMaskTy);
+  Value maskVal = b.undef(vecMaskTy);
   for (size_t s = 0; s < vecSize; ++s) {
     Value indexVal =
         rewriter.create<LLVM::ConstantOp>(loc, rewriter.getI64IntegerAttr(s));
-    maskVal = insert_element(vecMaskTy, maskVal, pred, indexVal);
+    maskVal = b.insert_element(vecMaskTy, maskVal, pred, indexVal);
   }
   return maskVal;
 }
@@ -73,6 +74,7 @@ Type castToVectorType(Type ty) {
 namespace mlir::LLVM::AMD {
 static Value shuffleCommon(Location loc, RewriterBase &rewriter, Value val,
                            Value i, int strideInt, ShflKind mode, Value clamp) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   unsigned bits = val.getType().getIntOrFloatBitWidth();
 
   // On AMD, the ds_swizzle_b32 and ds_permute_b32 instructions work on
@@ -80,30 +82,30 @@ static Value shuffleCommon(Location loc, RewriterBase &rewriter, Value val,
   auto valType = val.getType();
   if (!valType.isInteger(32) && bits <= 32) {
     if (!valType.isIntOrIndex())
-      val = bitcast(val, int_ty(bits));
+      val = b.bitcast(val, int_ty(bits));
     if (bits < 32)
-      val = sext(i32_ty, val);
+      val = b.sext(i32_ty, val);
 
     val = shuffleCommon(loc, rewriter, val, i, strideInt, mode, clamp);
 
     if (bits < 32)
-      val = trunc(int_ty(bits), val);
+      val = b.trunc(int_ty(bits), val);
     if (!valType.isIntOrIndex())
-      val = bitcast(val, valType);
+      val = b.bitcast(val, valType);
     return val;
   }
 
   if (bits == 64) {
     Type vecTy = vec_ty(f32_ty, 2);
-    Value vec = bitcast(val, vecTy);
-    Value val0 = extract_element(f32_ty, vec, i32_val(0));
-    Value val1 = extract_element(f32_ty, vec, i32_val(1));
+    Value vec = b.bitcast(val, vecTy);
+    Value val0 = b.extract_element(f32_ty, vec, b.i32_val(0));
+    Value val1 = b.extract_element(f32_ty, vec, b.i32_val(1));
     val0 = shuffleCommon(loc, rewriter, val0, i, strideInt, mode, clamp);
     val1 = shuffleCommon(loc, rewriter, val1, i, strideInt, mode, clamp);
-    vec = undef(vecTy);
-    vec = insert_element(vecTy, vec, val0, i32_val(0));
-    vec = insert_element(vecTy, vec, val1, i32_val(1));
-    return bitcast(vec, val.getType());
+    vec = b.undef(vecTy);
+    vec = b.insert_element(vecTy, vec, val0, b.i32_val(0));
+    vec = b.insert_element(vecTy, vec, val1, b.i32_val(1));
+    return b.bitcast(vec, val.getType());
   }
 
   auto mod = rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();
@@ -111,13 +113,13 @@ static Value shuffleCommon(Location loc, RewriterBase &rewriter, Value val,
       rewriter.create<::mlir::gpu::ThreadIdOp>(loc, ::mlir::gpu::Dimension::x);
   threadId = rewriter.create<arith::IndexCastOp>(loc, i32_ty, threadId);
   unsigned iWarpSize = triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);
-  Value warpSize = i32_val(iWarpSize);
-  Value laneId = urem(threadId, warpSize);
+  Value warpSize = b.i32_val(iWarpSize);
+  Value laneId = b.urem(threadId, warpSize);
   auto bpermute = [&](Value lane) {
     // Multiple lineId by 4. (More on permute instruction semantics:
     // https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/instinct-mi200-cdna2-instruction-set-architecture.pdf#page=180
-    Value byteOffset = i32_val(2);
-    Value permuteAddr = shl(lane, byteOffset);
+    Value byteOffset = b.i32_val(2);
+    Value permuteAddr = b.shl(lane, byteOffset);
     return rewriter.create<ROCDL::DsBpermuteOp>(loc, valType, permuteAddr, val);
   };
 
@@ -131,22 +133,22 @@ static Value shuffleCommon(Location loc, RewriterBase &rewriter, Value val,
                   ValueRange{rewriter.create<::mlir::gpu::ThreadIdOp>(
                       loc, rewriter.getIndexType(), ::mlir::gpu::Dimension::x)})
               .getResult(0);
-      Value stride = i32_val(32);
-      Value lineId = xor_(threadId, stride);
+      Value stride = b.i32_val(32);
+      Value lineId = b.xor_(threadId, stride);
       return bpermute(lineId);
     } else {
       // This map facilates the butterfly shuffle pattern for a stride less
       // than 16. The pattern stride is the key of the map.
       DenseMap<short, unsigned int> masks{
           {16, 0x401F}, {8, 0x201F}, {4, 0x101F}, {2, 0x081F}, {1, 0x041F}};
-      Value offset = i32_val(masks[strideInt]);
+      Value offset = b.i32_val(masks[strideInt]);
       return rewriter.create<ROCDL::DsSwizzleOp>(loc, valType, val, offset);
     }
     break;
   case ShflKind::up: {
-    Value mask = icmp_slt(laneId, i);
-    Value delta = sub(laneId, i);
-    Value index = select(mask, laneId, delta);
+    Value mask = b.icmp_slt(laneId, i);
+    Value delta = b.sub(laneId, i);
+    Value index = b.select(mask, laneId, delta);
     return bpermute(index);
   }
   case ShflKind::idx:
@@ -159,21 +161,25 @@ static Value shuffleCommon(Location loc, RewriterBase &rewriter, Value val,
 }
 
 Value shuffleXor(Location loc, RewriterBase &rewriter, Value val, int i) {
-  return shuffleCommon(loc, rewriter, val, i32_val(i), i, ShflKind::bfly,
-                       i32_val(0x1f));
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  return shuffleCommon(loc, rewriter, val, b.i32_val(i), i, ShflKind::bfly,
+                       b.i32_val(0x1f));
 }
 
 Value shuffleUp(Location loc, RewriterBase &rewriter, Value val, int i) {
-  return shuffleCommon(loc, rewriter, val, i32_val(i), i, ShflKind::up,
-                       i32_val(0x0));
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  return shuffleCommon(loc, rewriter, val, b.i32_val(i), i, ShflKind::up,
+                       b.i32_val(0x0));
 }
 
 Value shuffleIdx(Location loc, RewriterBase &rewriter, Value val, int i) {
-  return shuffleIdx(loc, rewriter, val, i32_val(i));
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  return shuffleIdx(loc, rewriter, val, b.i32_val(i));
 }
 
 Value shuffleIdx(Location loc, RewriterBase &rewriter, Value val, Value i) {
-  return shuffleCommon(loc, rewriter, val, i, 0, ShflKind::idx, i32_val(0x1f));
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  return shuffleCommon(loc, rewriter, val, i, 0, ShflKind::idx, b.i32_val(0x1f));
 }
 
 Value llGetPid(Location loc, RewriterBase &rewriter, ModuleOp moduleOp,
@@ -191,7 +197,7 @@ Value llGetPid(Location loc, RewriterBase &rewriter, ModuleOp moduleOp,
 Value llLoad(RewriterBase &rewriter, Location loc, Value ptr, Type elemTy,
              Value pred, Value falseVal, int64_t alignmentBytes,
              triton::CacheModifier cm) {
-
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   // Try to emit llvm.intr.masked.load if we can. In theory the backend should
   // be happier because we emit less branchy code to optimize. The backend will
   // lower it down however it wants at some point.
@@ -201,13 +207,13 @@ Value llLoad(RewriterBase &rewriter, Location loc, Value ptr, Type elemTy,
     // to bitcast to `vector<1xelemTy>` (and back)
     int64_t vecSize = getNumElements(elemTy);
     Type vecType = castToVectorType(elemTy);
-    falseVal = bitcast(falseVal, vecType);
+    falseVal = b.bitcast(falseVal, vecType);
     Value maskVal = createVectorMaskFromPredicate(rewriter, loc, pred, vecSize);
     bool nt = (cm == triton::CacheModifier::CG);
     Value vecData = rewriter.create<LLVM::MaskedLoadOp>(
         loc, vecType, ptr, maskVal, falseVal, alignmentBytes, nt);
     // If it is not a vector, remember to bitcast back to a scalar
-    vecData = bitcast(vecData, elemTy);
+    vecData = b.bitcast(vecData, elemTy);
     return vecData;
   }
 
@@ -238,6 +244,7 @@ Value llLoad(RewriterBase &rewriter, Location loc, Value ptr, Type elemTy,
 
 void llStore(RewriterBase &rewriter, Location loc, Value ptr, Value val,
              Value pred, int64_t alignmentBytes, triton::CacheModifier cm) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   // Try to emit llvm.intr.masked.store if we can. In theory the backend should
   // be happier because we emit less branchy code to optimize. The backend will
   // lower it down however it wants at some point.
@@ -247,7 +254,7 @@ void llStore(RewriterBase &rewriter, Location loc, Value ptr, Value val,
     Type elemTy = val.getType();
     int64_t vecSize = getNumElements(elemTy);
     Type vecType = castToVectorType(elemTy);
-    val = bitcast(val, vecType);
+    val = b.bitcast(val, vecType);
     Value maskVal = createVectorMaskFromPredicate(rewriter, loc, pred, vecSize);
     auto op = rewriter.create<LLVM::MaskedStoreOp>(loc, val, ptr, maskVal,
                                                    alignmentBytes);
diff --git a/third_party/nvidia/lib/NVGPUToLLVM/NVGPUToLLVMPass.cpp b/third_party/nvidia/lib/NVGPUToLLVM/NVGPUToLLVMPass.cpp
index 8cb654539..9dceea8a1 100644
--- a/third_party/nvidia/lib/NVGPUToLLVM/NVGPUToLLVMPass.cpp
+++ b/third_party/nvidia/lib/NVGPUToLLVM/NVGPUToLLVMPass.cpp
@@ -70,17 +70,18 @@ Type getTypeFromConstraint(char constraint, PatternRewriter &rewriter) {
 // val to i32 using ptrtoint(i32_ty, val)
 Value convertToType(Value val, std::string constraint, Location loc,
                     PatternRewriter &rewriter) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto isConstraintNumber = isNumber(constraint);
   if (!isConstraintNumber) {
     auto ty = getTypeFromConstraint(constraint[0], rewriter);
     if (isa<LLVM::LLVMPointerType>(val.getType())) {
-      return ptrtoint(ty, val);
+      return b.ptrtoint(ty, val);
     } else {
       assert(val.getType().getIntOrFloatBitWidth() <=
                  ty.getIntOrFloatBitWidth() &&
              "Cannot convert to a smaller type");
       if (val.getType().getIntOrFloatBitWidth() < ty.getIntOrFloatBitWidth())
-        return zext(ty, val);
+        return b.zext(ty, val);
     }
   }
   return val;
@@ -101,6 +102,7 @@ OperandsAndConstraints
 unpackOperands(const OperandsAndConstraints &operandsAndConstraints,
                PTXBuilder &ptxBuilder, Location loc,
                PatternRewriter &rewriter) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   OperandsAndConstraints unpackedOperands;
   for (const auto &[operand, constraint] : operandsAndConstraints) {
     auto llvmStruct = llvm::dyn_cast<LLVM::LLVMStructType>(operand.getType());
@@ -114,11 +116,11 @@ unpackOperands(const OperandsAndConstraints &operandsAndConstraints,
         if (isConstraintNumber) {
           auto constraintInt = std::stoi(constraint) + i;
           unpackedOperands.push_back(
-              {extract_val(llvmStruct.getBody()[i], operand, i),
+              {b.extract_val(llvmStruct.getBody()[i], operand, i),
                std::to_string(constraintInt)});
         } else {
           unpackedOperands.push_back(
-              {extract_val(llvmStruct.getBody()[i], operand, i), constraint});
+              {b.extract_val(llvmStruct.getBody()[i], operand, i), constraint});
         }
       }
     } else {
diff --git a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/BarrierOpToLLVM.cpp b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/BarrierOpToLLVM.cpp
index 746b910e1..3b0c563fb 100644
--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/BarrierOpToLLVM.cpp
+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/BarrierOpToLLVM.cpp
@@ -78,13 +78,14 @@ struct InitBarrierOpConversion
   matchAndRewrite(triton::nvidia_gpu::InitBarrierOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
     Location loc = op->getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto smemObj = LLVM::getSharedMemoryObjectFromStruct(
         loc, adaptor.getAlloc(),
         typeConverter->convertType(op.getAlloc().getType().getElementType()),
         rewriter);
 
     auto id = getThreadId(rewriter, loc);
-    auto pred = icmp_eq(id, i32_val(0));
+    auto pred = b.icmp_eq(id, b.i32_val(0));
     ::mlir::triton::PTXBuilder ptxBuilder;
     const std::string ptx = "@$0 mbarrier.init.shared::cta.b64 [$1], " +
                             std::to_string(op.getCount()) + ";";
@@ -107,13 +108,14 @@ struct InvalBarrierOpConversion
   matchAndRewrite(triton::nvidia_gpu::InvalBarrierOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
     Location loc = op->getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto smemObj = LLVM::getSharedMemoryObjectFromStruct(
         loc, adaptor.getAlloc(),
         typeConverter->convertType(op.getAlloc().getType().getElementType()),
         rewriter);
 
     auto id = getThreadId(rewriter, loc);
-    Value pred = icmp_eq(id, i32_val(0));
+    Value pred = b.icmp_eq(id, b.i32_val(0));
     ::mlir::triton::PTXBuilder ptxBuilder;
     const std::string ptx = "@$0 mbarrier.inval.shared::cta.b64 [$1];";
     auto &barSyncOp = *ptxBuilder.create<>(ptx);
@@ -135,14 +137,15 @@ struct BarrierExpectConversion
   matchAndRewrite(triton::nvidia_gpu::BarrierExpectOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
     Location loc = op->getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto smemObj = LLVM::getSharedMemoryObjectFromStruct(
         loc, adaptor.getAlloc(),
         typeConverter->convertType(op.getAlloc().getType().getElementType()),
         rewriter);
 
     auto id = getThreadId(rewriter, loc);
-    Value pred = icmp_eq(id, i32_val(0));
-    pred = and_(pred, adaptor.getPred());
+    Value pred = b.icmp_eq(id, b.i32_val(0));
+    pred = b.and_(pred, adaptor.getPred());
     ::mlir::triton::PTXBuilder ptxBuilder;
     const std::string ptx =
         "@$0 mbarrier.arrive.expect_tx.shared.b64 _, [$1], " +
diff --git a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertLayoutOpToLLVM.cpp b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertLayoutOpToLLVM.cpp
index 71fd3c0cd..ba4e11b43 100644
--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertLayoutOpToLLVM.cpp
+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertLayoutOpToLLVM.cpp
@@ -182,6 +182,7 @@ private:
                       ArrayRef<unsigned> origRepShape,
                       ArrayRef<unsigned> outOrd, SmallVector<Value> &vals,
                       Value smemBase) const {
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);
     auto layout = type.getEncoding();
     auto rank = type.getRank();
@@ -229,30 +230,30 @@ private:
         Value offset = linearize(rewriter, loc, multiDimOffsetWrapped,
                                  paddedRepShape, outOrd);
         auto elemPtrTy = ptr_ty(rewriter.getContext(), 3);
-        Value ptr = gep(elemPtrTy, llvmElemTy, smemBase, offset);
+        Value ptr = b.gep(elemPtrTy, llvmElemTy, smemBase, offset);
         auto vecTy = vec_ty(llvmElemTy, vec);
-        ptr = bitcast(ptr, ptr_ty(rewriter.getContext(), 3));
+        ptr = b.bitcast(ptr, ptr_ty(rewriter.getContext(), 3));
         if (stNotRd) {
-          Value valVec = undef(vecTy);
+          Value valVec = b.undef(vecTy);
           for (unsigned v = 0; v < vec; ++v) {
             auto currVal = vals[elemId + linearCTAId * accumSizePerThread + v];
             if (isInt1)
-              currVal = zext(llvmElemTy, currVal);
+              currVal = b.zext(llvmElemTy, currVal);
             else if (isPtr)
-              currVal = ptrtoint(llvmElemTy, currVal);
-            valVec = insert_element(vecTy, valVec, currVal, i32_val(v));
+              currVal = b.ptrtoint(llvmElemTy, currVal);
+            valVec = b.insert_element(vecTy, valVec, currVal, b.i32_val(v));
           }
-          store(valVec, ptr);
+          b.store(valVec, ptr);
         } else {
-          Value valVec = load(vecTy, ptr);
+          Value valVec = b.load(vecTy, ptr);
           for (unsigned v = 0; v < vec; ++v) {
-            Value currVal = extract_element(llvmElemTy, valVec, i32_val(v));
+            Value currVal = b.extract_element(llvmElemTy, valVec, b.i32_val(v));
             if (isInt1)
-              currVal = icmp_ne(currVal,
-                                rewriter.create<LLVM::ConstantOp>(
-                                    loc, i8_ty, rewriter.getI8IntegerAttr(0)));
+              currVal = b.icmp_ne(currVal,
+                                  rewriter.create<LLVM::ConstantOp>(
+                                      loc, i8_ty, rewriter.getI8IntegerAttr(0)));
             else if (isPtr)
-              currVal = inttoptr(llvmElemTyOrig, currVal);
+              currVal = b.inttoptr(llvmElemTyOrig, currVal);
             vals[elemId + linearCTAId * accumSizePerThread + v] = currVal;
           }
         }
@@ -271,6 +272,7 @@ private:
                               SmallVector<Value> &vals, Value smemBase,
                               ArrayRef<int64_t> shape,
                               bool isDestMma = false) const {
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     unsigned accumNumCTAsEachRep = 1;
     auto typeConverter = getTypeConverter();
     auto layout = type.getEncoding();
@@ -341,20 +343,20 @@ private:
       auto coord = coord2valT[elemId].first;
       Value offset = linearize(rewriter, loc, coord, paddedRepShape, outOrd);
       auto elemPtrTy = ptr_ty(rewriter.getContext(), 3);
-      Value ptr = gep(elemPtrTy, elemTy, smemBase, offset);
+      Value ptr = b.gep(elemPtrTy, elemTy, smemBase, offset);
       auto vecTy = vec_ty(elemTy, vec);
-      ptr = bitcast(ptr, ptr_ty(rewriter.getContext(), 3));
+      ptr = b.bitcast(ptr, ptr_ty(rewriter.getContext(), 3));
       if (stNotRd) {
-        Value valVec = undef(vecTy);
+        Value valVec = b.undef(vecTy);
         for (unsigned v = 0; v < vec; ++v) {
           auto currVal = coord2valT[elemId + v].second;
-          valVec = insert_element(vecTy, valVec, currVal, i32_val(v));
+          valVec = b.insert_element(vecTy, valVec, currVal, b.i32_val(v));
         }
-        store(valVec, ptr);
+        b.store(valVec, ptr);
       } else {
-        Value valVec = load(vecTy, ptr);
+        Value valVec = b.load(vecTy, ptr);
         for (unsigned v = 0; v < vec; ++v) {
-          Value currVal = extract_element(elemTy, valVec, i32_val(v));
+          Value currVal = b.extract_element(elemTy, valVec, b.i32_val(v));
           vals[elemId + v] = currVal;
         }
       }
@@ -368,6 +370,7 @@ private:
                               const TargetInfoBase &targetInfo) const {
     MLIRContext *ctx = rewriter.getContext();
     auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto typeConverter = getTypeConverter();
     auto srcTy = op.getSrc().getType();
     auto dstTy = op.getType();
@@ -383,7 +386,7 @@ private:
 
     Value smemBase =
         LLVM::getSharedMemoryBase(loc, rewriter, targetInfo, op.getOperation());
-    smemBase = bitcast(smemBase, elemPtrTy);
+    smemBase = b.bitcast(smemBase, elemPtrTy);
     auto smemShape = convertType<unsigned, int64_t>(srcShapePerCTA);
 
     // Store to local shared memory
@@ -397,8 +400,8 @@ private:
 
       for (unsigned i = 0; i < inIndices.size(); ++i) {
         Value offset = linearize(rewriter, loc, inIndices[i], smemShape);
-        Value ptr = gep(elemPtrTy, llvmElemTy, smemBase, offset);
-        store(inVals[i], ptr);
+        Value ptr = b.gep(elemPtrTy, llvmElemTy, smemBase, offset);
+        b.store(inVals[i], ptr);
       }
     }
 
@@ -410,7 +413,7 @@ private:
     {
       SmallVector<Value> srcShapePerCTACache;
       for (unsigned i = 0; i < rank; ++i)
-        srcShapePerCTACache.push_back(i32_val(srcShapePerCTA[i]));
+        srcShapePerCTACache.push_back(b.i32_val(srcShapePerCTA[i]));
 
       SmallVector<Value> outVals;
       auto outIndices = emitIndices(loc, rewriter, targetInfo, dstLayout, dstTy,
@@ -422,17 +425,18 @@ private:
 
         SmallVector<Value> multiDimCTAId, localCoord;
         for (unsigned d = 0; d < rank; ++d) {
-          multiDimCTAId.push_back(udiv(coord[d], srcShapePerCTACache[d]));
-          localCoord.push_back(urem(coord[d], srcShapePerCTACache[d]));
+          multiDimCTAId.push_back(b.udiv(coord[d], srcShapePerCTACache[d]));
+          localCoord.push_back(b.urem(coord[d], srcShapePerCTACache[d]));
         }
 
         Value remoteCTAId =
             linearize(rewriter, loc, multiDimCTAId, srcCTAsPerCGA, srcCTAOrder);
         Value localOffset = linearize(rewriter, loc, localCoord, smemShape);
 
-        Value ptr = gep(elemPtrTy, llvmElemTy, smemBase, localOffset);
-        outVals.push_back(targetInfo.loadDShared(
-            rewriter, loc, ptr, remoteCTAId, llvmElemTy, /*pred=*/true_val()));
+        Value ptr = b.gep(elemPtrTy, llvmElemTy, smemBase, localOffset);
+        outVals.push_back(targetInfo.loadDShared(rewriter, loc, ptr,
+                                                 remoteCTAId, llvmElemTy,
+                                                 /*pred=*/b.true_val()));
       }
 
       Value result =
@@ -455,6 +459,7 @@ private:
                                 ConversionPatternRewriter &rewriter,
                                 const TargetInfoBase &targetInfo) const {
     auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto typeConverter = getTypeConverter();
     RankedTensorType srcTy = op.getSrc().getType();
     RankedTensorType dstTy = op.getType();
@@ -464,7 +469,7 @@ private:
     Value smemBase =
         LLVM::getSharedMemoryBase(loc, rewriter, targetInfo, op.getOperation());
     auto elemPtrTy = ptr_ty(rewriter.getContext(), 3);
-    smemBase = bitcast(smemBase, elemPtrTy);
+    smemBase = b.bitcast(smemBase, elemPtrTy);
     auto shape = dstTy.getShape();
     unsigned rank = dstTy.getRank();
     SmallVector<unsigned> numReplicates(rank);
@@ -507,7 +512,7 @@ private:
       auto multiDimRepId =
           getMultiDimIndex<unsigned>(repId, numReplicates, outOrd);
       if (repId != 0) {
-        barrier();
+        b.barrier();
       }
 
       if (isLayoutMmaV1(srcLayout))
@@ -519,7 +524,7 @@ private:
                        multiDimRepId, inVec, paddedRepShape, origRepShape,
                        outOrd, vals, smemBase);
 
-      barrier();
+      b.barrier();
 
       if (isLayoutMmaV1(dstLayout))
         processReplicaForMMAV1(loc, rewriter, /*stNotRd*/ false, dstTy,
@@ -545,51 +550,52 @@ private:
                                 OpAdaptor adaptor,
                                 ConversionPatternRewriter &rewriter) const {
     auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto dstTy = op.getType();
     auto vals = unpackLLElements(loc, adaptor.getSrc(), rewriter);
     SmallVector<Value> retVals;
     for (int i = 0; i < vals.size(); i += 8) {
-      Value upper = undef(vec_ty(i8_ty, 4));
+      Value upper = b.undef(vec_ty(i8_ty, 4));
       for (int j = 0; j < 4; j++) {
-        upper =
-            insert_element(vec_ty(i8_ty, 4), upper, vals[i + j], i32_val(j));
+        upper = b.insert_element(vec_ty(i8_ty, 4), upper, vals[i + j],
+                                 b.i32_val(j));
       }
-      upper = bitcast(upper, i32_ty);
-      Value lower = undef(vec_ty(i8_ty, 4));
+      upper = b.bitcast(upper, i32_ty);
+      Value lower = b.undef(vec_ty(i8_ty, 4));
       for (int j = 0; j < 4; j++) {
-        lower = insert_element(vec_ty(i8_ty, 4), lower, vals[i + 4 + j],
-                               i32_val(j));
+        lower = b.insert_element(vec_ty(i8_ty, 4), lower, vals[i + 4 + j],
+                                 b.i32_val(j));
       }
-      lower = bitcast(lower, i32_ty);
-
-      Value threadIdMod4 = urem(getThreadId(rewriter, loc), i32_val(4));
-      Value cnd = or_(icmp_eq(threadIdMod4, i32_val(0)),
-                      icmp_eq(threadIdMod4, i32_val(3)));
-      Value selectorEx0 = select(cnd, i32_val(0x3210), i32_val(0x7654));
-      Value selectorEx1 = select(cnd, i32_val(0x7654), i32_val(0x3210));
-      Value selectorEx4 = select(cnd, i32_val(0x5410), i32_val(0x1054));
-      Value selectorEx5 = select(cnd, i32_val(0x7632), i32_val(0x3276));
-
-      Value isOne = icmp_eq(threadIdMod4, i32_val(1));
-      Value isTwo = icmp_eq(threadIdMod4, i32_val(2));
-      Value isThree = icmp_eq(threadIdMod4, i32_val(3));
-      Value upperIdx = i32_val(0);
-      upperIdx = select(isOne, i32_val(3), upperIdx);
-      upperIdx = select(isTwo, i32_val(1), upperIdx);
-      upperIdx = select(isThree, i32_val(2), upperIdx);
-
-      Value lowerIdx = i32_val(1);
-      lowerIdx = select(isOne, i32_val(2), lowerIdx);
-      lowerIdx = select(isTwo, i32_val(0), lowerIdx);
-      lowerIdx = select(isThree, i32_val(3), lowerIdx);
+      lower = b.bitcast(lower, i32_ty);
+
+      Value threadIdMod4 = b.urem(getThreadId(rewriter, loc), b.i32_val(4));
+      Value cnd = b.or_(b.icmp_eq(threadIdMod4, b.i32_val(0)),
+                        b.icmp_eq(threadIdMod4, b.i32_val(3)));
+      Value selectorEx0 = b.select(cnd, b.i32_val(0x3210), b.i32_val(0x7654));
+      Value selectorEx1 = b.select(cnd, b.i32_val(0x7654), b.i32_val(0x3210));
+      Value selectorEx4 = b.select(cnd, b.i32_val(0x5410), b.i32_val(0x1054));
+      Value selectorEx5 = b.select(cnd, b.i32_val(0x7632), b.i32_val(0x3276));
+
+      Value isOne = b.icmp_eq(threadIdMod4, b.i32_val(1));
+      Value isTwo = b.icmp_eq(threadIdMod4, b.i32_val(2));
+      Value isThree = b.icmp_eq(threadIdMod4, b.i32_val(3));
+      Value upperIdx = b.i32_val(0);
+      upperIdx = b.select(isOne, b.i32_val(3), upperIdx);
+      upperIdx = b.select(isTwo, b.i32_val(1), upperIdx);
+      upperIdx = b.select(isThree, b.i32_val(2), upperIdx);
+
+      Value lowerIdx = b.i32_val(1);
+      lowerIdx = b.select(isOne, b.i32_val(2), lowerIdx);
+      lowerIdx = b.select(isTwo, b.i32_val(0), lowerIdx);
+      lowerIdx = b.select(isThree, b.i32_val(3), lowerIdx);
 
       Value upper0 =
           LLVM::NVIDIA::permute(loc, rewriter, upper, lower, selectorEx0);
       Value lower0 =
           LLVM::NVIDIA::permute(loc, rewriter, upper, lower, selectorEx1);
-      Value mask = i32_val(0xFFFFFFFF);
+      Value mask = b.i32_val(0xFFFFFFFF);
       // Set clamp tp shuffle only within 4 lanes.
-      Value clamp = i32_val(0x1C1F);
+      Value clamp = b.i32_val(0x1C1F);
       upper0 =
           rewriter.create<NVVM::ShflOp>(loc, i32_ty, mask, upper0, upperIdx,
                                         clamp, NVVM::ShflKind::idx, UnitAttr());
@@ -598,15 +604,15 @@ private:
                                         clamp, NVVM::ShflKind::idx, UnitAttr());
       Value upper1 =
           LLVM::NVIDIA::permute(loc, rewriter, upper0, lower0, selectorEx4);
-      Value vecVal = bitcast(upper1, vec_ty(i8_ty, 4));
+      Value vecVal = b.bitcast(upper1, vec_ty(i8_ty, 4));
       for (int i = 0; i < 4; i++) {
-        retVals.push_back(extract_element(i8_ty, vecVal, i32_val(i)));
+        retVals.push_back(b.extract_element(i8_ty, vecVal, b.i32_val(i)));
       }
       Value lower1 =
           LLVM::NVIDIA::permute(loc, rewriter, upper0, lower0, selectorEx5);
-      vecVal = bitcast(lower1, vec_ty(i8_ty, 4));
+      vecVal = b.bitcast(lower1, vec_ty(i8_ty, 4));
       for (int i = 0; i < 4; i++) {
-        retVals.push_back(extract_element(i8_ty, vecVal, i32_val(i)));
+        retVals.push_back(b.extract_element(i8_ty, vecVal, b.i32_val(i)));
       }
     }
     Value result =
@@ -619,6 +625,7 @@ private:
   lowerMmaToDotOperand(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,
                        ConversionPatternRewriter &rewriter) const {
     auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto srcTy = op.getSrc().getType();
     auto dstTy = op.getType();
     if (matchMmaV3AndDotOperandLayout(srcTy, dstTy)) {
@@ -648,13 +655,13 @@ private:
       if (auto intTy = dyn_cast<IntegerType>(elemTy) && elemSize <= 16) {
         auto fold = 32 / elemSize;
         for (unsigned i = 0; i < elems; i += fold) {
-          Value val = i32_val(0);
+          Value val = b.i32_val(0);
           for (unsigned j = 0; j < fold; j++) {
             auto ext =
-                shl(i32_ty, zext(i32_ty, vals[i + j]), i32_val(elemSize * j));
-            val = or_(i32_ty, val, ext);
+                b.shl(i32_ty, b.zext(i32_ty, vals[i + j]), b.i32_val(elemSize * j));
+            val = b.or_(i32_ty, val, ext);
           }
-          vecVals.push_back(bitcast(val, i32_ty));
+          vecVals.push_back(b.bitcast(val, i32_ty));
         }
       } else {
         unsigned vecSize = std::max<unsigned>(32 / elemSize, 1);
@@ -662,8 +669,8 @@ private:
         for (unsigned i = 0; i < elems; i += vecSize) {
           Value packed = rewriter.create<LLVM::UndefOp>(loc, vecTy);
           for (unsigned j = 0; j < vecSize; j++)
-            packed = insert_element(vecTy, packed, vals[i + j], i32_val(j));
-          vecVals.push_back(bitcast(packed, i32_ty));
+            packed = b.insert_element(vecTy, packed, vals[i + j], b.i32_val(j));
+          vecVals.push_back(b.bitcast(packed, i32_ty));
         }
       }
       Value view =
@@ -711,6 +718,7 @@ struct LocalAllocOpConversion
 
     auto *ctx = rewriter.getContext();
     Location loc = op->getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
 
     RankedTensorType srcTy = op.getSrc().getType();
     SmallVector<unsigned> shape =
@@ -730,15 +738,15 @@ struct LocalAllocOpConversion
     auto kBlock = str_attr("block");
 
     Value threadId = getThreadId(rewriter, loc);
-    Value threadsPerWarp = i32_val(layout->getInDimSize(kLane));
-    Value laneId = urem(threadId, threadsPerWarp);
-    Value warpId = udiv(threadId, threadsPerWarp);
+    Value threadsPerWarp = b.i32_val(layout->getInDimSize(kLane));
+    Value laneId = b.urem(threadId, threadsPerWarp);
+    Value warpId = b.udiv(threadId, threadsPerWarp);
 
     auto regBase = applyLinearLayout(loc, rewriter, *layout,
-                                     {{kRegister, i32_val(0)},
+                                     {{kRegister, b.i32_val(0)},
                                       {kLane, laneId},
                                       {kWarp, warpId},
-                                      {kBlock, i32_val(0)}})[0]
+                                      {kBlock, b.i32_val(0)}})[0]
                        .second;
     auto srcVals = unpackLLElements(loc, adaptor.getSrc(), rewriter);
     auto srcVec = layout->getNumConsecutiveInOut();
@@ -748,8 +756,8 @@ struct LocalAllocOpConversion
           layout
               ->apply({{kRegister, i}, {kLane, 0}, {kWarp, 0}, {kBlock, 0}})[0]
               .second;
-      Value offset = xor_(regBase, i32_val(regIdx));
-      auto vecAddr = gep(smemPtrTy, llvmElemTy, smemBase, offset);
+      Value offset = b.xor_(regBase, b.i32_val(regIdx));
+      auto vecAddr = b.gep(smemPtrTy, llvmElemTy, smemBase, offset);
       vecAddr.setInbounds(true);
       SmallVector<Value> inValsVec;
       for (int j = 0; j < srcVec; j++)
diff --git a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp
index 6847c0550..e0a2219a1 100644
--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp
+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp
@@ -24,6 +24,7 @@ computeOffsets(Value threadId, bool isARow, bool isBRow, ArrayRef<int> fpw,
                ArrayRef<int> spw, ArrayRef<int> rep,
                ConversionPatternRewriter &rewriter, Location loc,
                Type resultTy) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto *ctx = rewriter.getContext();
   auto wpt = cast<NvidiaMmaEncodingAttr>(
                  cast<DotOperandEncodingAttr>(
@@ -31,55 +32,55 @@ computeOffsets(Value threadId, bool isARow, bool isBRow, ArrayRef<int> fpw,
                      .getParent())
                  .getWarpsPerCTA();
 
-  Value _1 = i32_val(1);
-  Value _3 = i32_val(3);
-  Value _4 = i32_val(4);
-  Value _16 = i32_val(16);
-  Value _32 = i32_val(32);
+  Value _1 = b.i32_val(1);
+  Value _3 = b.i32_val(3);
+  Value _4 = b.i32_val(4);
+  Value _16 = b.i32_val(16);
+  Value _32 = b.i32_val(32);
 
-  Value lane = urem(threadId, _32);
-  Value warp = udiv(threadId, _32);
+  Value lane = b.urem(threadId, _32);
+  Value warp = b.udiv(threadId, _32);
 
   // warp offset
-  Value warp0 = urem(warp, i32_val(wpt[0]));
-  Value warp12 = udiv(warp, i32_val(wpt[0]));
-  Value warp1 = urem(warp12, i32_val(wpt[1]));
-  Value warpMOff = mul(warp0, i32_val(spw[0]));
-  Value warpNOff = mul(warp1, i32_val(spw[1]));
+  Value warp0 = b.urem(warp, b.i32_val(wpt[0]));
+  Value warp12 = b.udiv(warp, b.i32_val(wpt[0]));
+  Value warp1 = b.urem(warp12, b.i32_val(wpt[1]));
+  Value warpMOff = b.mul(warp0, b.i32_val(spw[0]));
+  Value warpNOff = b.mul(warp1, b.i32_val(spw[1]));
   // Quad offset
-  Value quadMOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[0]));
-  Value quadNOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[1]));
+  Value quadMOff = b.mul(b.udiv(b.and_(lane, _16), _4), b.i32_val(fpw[0]));
+  Value quadNOff = b.mul(b.udiv(b.and_(lane, _16), _4), b.i32_val(fpw[1]));
   // Pair offset
-  Value pairMOff = udiv(urem(lane, _16), _4);
-  pairMOff = urem(pairMOff, i32_val(fpw[0]));
-  pairMOff = mul(pairMOff, _4);
-  Value pairNOff = udiv(urem(lane, _16), _4);
-  pairNOff = udiv(pairNOff, i32_val(fpw[0]));
-  pairNOff = urem(pairNOff, i32_val(fpw[1]));
-  pairNOff = mul(pairNOff, _4);
+  Value pairMOff = b.udiv(b.urem(lane, _16), _4);
+  pairMOff = b.urem(pairMOff, b.i32_val(fpw[0]));
+  pairMOff = b.mul(pairMOff, _4);
+  Value pairNOff = b.udiv(b.urem(lane, _16), _4);
+  pairNOff = b.udiv(pairNOff, b.i32_val(fpw[0]));
+  pairNOff = b.urem(pairNOff, b.i32_val(fpw[1]));
+  pairNOff = b.mul(pairNOff, _4);
   // scale
-  pairMOff = mul(pairMOff, i32_val(rep[0] / 2));
-  quadMOff = mul(quadMOff, i32_val(rep[0] / 2));
-  pairNOff = mul(pairNOff, i32_val(rep[1] / 2));
-  quadNOff = mul(quadNOff, i32_val(rep[1] / 2));
+  pairMOff = b.mul(pairMOff, b.i32_val(rep[0] / 2));
+  quadMOff = b.mul(quadMOff, b.i32_val(rep[0] / 2));
+  pairNOff = b.mul(pairNOff, b.i32_val(rep[1] / 2));
+  quadNOff = b.mul(quadNOff, b.i32_val(rep[1] / 2));
   // Quad pair offset
-  Value laneMOff = add(pairMOff, quadMOff);
-  Value laneNOff = add(pairNOff, quadNOff);
+  Value laneMOff = b.add(pairMOff, quadMOff);
+  Value laneNOff = b.add(pairNOff, quadNOff);
   // A offset
-  Value offsetAM = add(warpMOff, laneMOff);
-  Value offsetAK = and_(lane, _3);
+  Value offsetAM = b.add(warpMOff, laneMOff);
+  Value offsetAK = b.and_(lane, _3);
   // B offset
-  Value offsetBN = add(warpNOff, laneNOff);
-  Value offsetBK = and_(lane, _3);
+  Value offsetBN = b.add(warpNOff, laneNOff);
+  Value offsetBK = b.and_(lane, _3);
   // i indices
-  Value offsetCM = add(and_(lane, _1), offsetAM);
+  Value offsetCM = b.add(b.and_(lane, _1), offsetAM);
   if (isARow) {
-    offsetAM = add(offsetAM, urem(threadId, _4));
-    offsetAK = i32_val(0);
+    offsetAM = b.add(offsetAM, b.urem(threadId, _4));
+    offsetAK = b.i32_val(0);
   }
   if (!isBRow) {
-    offsetBN = add(offsetBN, urem(threadId, _4));
-    offsetBK = i32_val(0);
+    offsetBN = b.add(offsetBN, b.urem(threadId, _4));
+    offsetBK = b.i32_val(0);
   }
 
   return std::make_tuple(offsetAM, offsetAK, offsetBN, offsetBK);
@@ -89,6 +90,7 @@ static Value loadA(Value tensor, const SharedMemoryObject &smemObj,
                    Value thread, Location loc,
                    const LLVMTypeConverter *typeConverter,
                    ConversionPatternRewriter &rewriter, Type resultTy) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   static constexpr std::array<int, 3> fpw{{2, 2, 1}};
   auto mmaEncoding = cast<NvidiaMmaEncodingAttr>(
       cast<DotOperandEncodingAttr>(
@@ -117,8 +119,8 @@ static Value loadA(Value tensor, const SharedMemoryObject &smemObj,
   int vecA = sharedLayout.getVec();
 
   auto strides = smemObj.strides;
-  Value strideAM = isARow ? strides[0] : i32_val(1);
-  Value strideAK = isARow ? i32_val(1) : strides[1];
+  Value strideAM = isARow ? strides[0] : b.i32_val(1);
+  Value strideAK = isARow ? b.i32_val(1) : strides[1];
   Value strideA0 = isARow ? strideAK : strideAM;
   Value strideA1 = isARow ? strideAM : strideAK;
 
@@ -135,15 +137,15 @@ static Value loadA(Value tensor, const SharedMemoryObject &smemObj,
   // pre-compute pointer lanes
   Value offA0 = isARow ? offsetAK : offsetAM;
   Value offA1 = isARow ? offsetAM : offsetAK;
-  Value phaseA = urem(udiv(offA1, i32_val(perPhaseA)), i32_val(maxPhaseA));
-  offA0 = add(offA0, cSwizzleOffset);
+  Value phaseA = b.urem(b.udiv(offA1, b.i32_val(perPhaseA)), b.i32_val(maxPhaseA));
+  offA0 = b.add(offA0, cSwizzleOffset);
   SmallVector<Value> offA(numPtrA);
   for (int i = 0; i < numPtrA; i++) {
-    Value offA0I = add(offA0, i32_val(i * (isARow ? 4 : strideRepM)));
-    offA0I = udiv(offA0I, i32_val(vecA));
-    offA0I = xor_(offA0I, phaseA);
-    offA0I = mul(offA0I, i32_val(vecA));
-    offA[i] = add(mul(offA0I, strideA0), mul(offA1, strideA1));
+    Value offA0I = b.add(offA0, b.i32_val(i * (isARow ? 4 : strideRepM)));
+    offA0I = b.udiv(offA0I, b.i32_val(vecA));
+    offA0I = b.xor_(offA0I, phaseA);
+    offA0I = b.mul(offA0I, b.i32_val(vecA));
+    offA[i] = b.add(b.mul(offA0I, strideA0), b.mul(offA1, strideA1));
   }
 
   Type elemX2Ty = vec_ty(f16_ty, 2);
@@ -158,31 +160,31 @@ static Value loadA(Value tensor, const SharedMemoryObject &smemObj,
 
   std::map<std::pair<int, int>, std::pair<Value, Value>> has;
   for (int i = 0; i < numPtrA; i++)
-    ptrA[i] = gep(ptr_ty(ctx, 3), f16_ty, smemBase, offA[i]);
+    ptrA[i] = b.gep(ptr_ty(ctx, 3), f16_ty, smemBase, offA[i]);
 
   auto ld = [&](decltype(has) &vals, int m, int k, Value val0, Value val1) {
     vals[{m, k}] = {val0, val1};
   };
   auto loadA = [&](int m, int k) {
     int offidx = (isARow ? k / 4 : m) % numPtrA;
-    Value thePtrA = gep(ptr_ty(ctx, 3), elemTy, smemBase, offA[offidx]);
+    Value thePtrA = b.gep(ptr_ty(ctx, 3), elemTy, smemBase, offA[offidx]);
 
     int stepAM = isARow ? m : m / numPtrA * numPtrA;
     int stepAK = isARow ? k / (numPtrA * vecA) * (numPtrA * vecA) : k;
-    Value offset = add(mul(i32_val(stepAM * strideRepM), strideAM),
-                       mul(i32_val(stepAK), strideAK));
-    Value pa = gep(ptr_ty(ctx, 3), elemTy, thePtrA, offset);
+    Value offset = b.add(b.mul(b.i32_val(stepAM * strideRepM), strideAM),
+                         b.mul(b.i32_val(stepAK), strideAK));
+    Value pa = b.gep(ptr_ty(ctx, 3), elemTy, thePtrA, offset);
     Type vecTy = vec_ty(i32_ty, std::max<int>(vecA / 2, 1));
     Type aPtrTy = ptr_ty(ctx, 3);
-    Value ha = load(vecTy, bitcast(pa, aPtrTy));
+    Value ha = b.load(vecTy, b.bitcast(pa, aPtrTy));
     // record lds that needs to be moved
-    Value ha00 = bitcast(extract_element(ha, i32_val(0)), elemX2Ty);
-    Value ha01 = bitcast(extract_element(ha, i32_val(1)), elemX2Ty);
+    Value ha00 = b.bitcast(b.extract_element(ha, b.i32_val(0)), elemX2Ty);
+    Value ha01 = b.bitcast(b.extract_element(ha, b.i32_val(1)), elemX2Ty);
     ld(has, m, k, ha00, ha01);
 
     if (vecA > 4) {
-      Value ha10 = bitcast(extract_element(ha, i32_val(2)), elemX2Ty);
-      Value ha11 = bitcast(extract_element(ha, i32_val(3)), elemX2Ty);
+      Value ha10 = b.bitcast(b.extract_element(ha, b.i32_val(2)), elemX2Ty);
+      Value ha11 = b.bitcast(b.extract_element(ha, b.i32_val(3)), elemX2Ty);
       if (isARow)
         ld(has, m, k + 4, ha10, ha11);
       else
@@ -202,8 +204,8 @@ static Value loadA(Value tensor, const SharedMemoryObject &smemObj,
   SmallVector<Value> elems;
   elems.reserve(has.size() * 2);
   for (auto item : has) { // has is a map, the key should be ordered.
-    elems.push_back(bitcast(item.second.first, i32_ty));
-    elems.push_back(bitcast(item.second.second, i32_ty));
+    elems.push_back(b.bitcast(item.second.first, i32_ty));
+    elems.push_back(b.bitcast(item.second.second, i32_ty));
   }
 
   Value res = packLLElements(loc, typeConverter, elems, rewriter, resultTy);
@@ -214,6 +216,7 @@ static Value loadB(Value tensor, const SharedMemoryObject &smemObj,
                    Value thread, Location loc,
                    const LLVMTypeConverter *typeConverter,
                    ConversionPatternRewriter &rewriter, Type resultTy) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   static constexpr std::array<int, 3> fpw{{2, 2, 1}};
   auto mmaEncoding = cast<NvidiaMmaEncodingAttr>(
       cast<DotOperandEncodingAttr>(
@@ -237,8 +240,8 @@ static Value loadB(Value tensor, const SharedMemoryObject &smemObj,
       cast<RankedTensorType>(resultTy).getEncoding());
 
   int vecB = sharedLayout.getVec();
-  Value strideBN = isBRow ? i32_val(1) : strides[1];
-  Value strideBK = isBRow ? strides[0] : i32_val(1);
+  Value strideBN = isBRow ? b.i32_val(1) : strides[1];
+  Value strideBK = isBRow ? strides[0] : b.i32_val(1);
   Value strideB0 = isBRow ? strideBN : strideBK;
   Value strideB1 = isBRow ? strideBK : strideBN;
   int strideRepN = wpt[1] * fpw[1] * 8;
@@ -259,17 +262,17 @@ static Value loadB(Value tensor, const SharedMemoryObject &smemObj,
 
   Value offB0 = isBRow ? offsetBN : offsetBK;
   Value offB1 = isBRow ? offsetBK : offsetBN;
-  Value phaseB = urem(udiv(offB1, i32_val(perPhaseB)), i32_val(maxPhaseB));
+  Value phaseB = b.urem(b.udiv(offB1, b.i32_val(perPhaseB)), b.i32_val(maxPhaseB));
   Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);
 
-  offB0 = add(offB0, cSwizzleOffset);
+  offB0 = b.add(offB0, cSwizzleOffset);
   SmallVector<Value> offB(numPtrB);
   for (int i = 0; i < numPtrB; ++i) {
-    Value offB0I = add(offB0, i32_val(i * (isBRow ? strideRepN : 4)));
-    offB0I = udiv(offB0I, i32_val(vecB));
-    offB0I = xor_(offB0I, phaseB);
-    offB0I = mul(offB0I, i32_val(vecB));
-    offB[i] = add(mul(offB0I, strideB0), mul(offB1, strideB1));
+    Value offB0I = b.add(offB0, b.i32_val(i * (isBRow ? strideRepN : 4)));
+    offB0I = b.udiv(offB0I, b.i32_val(vecB));
+    offB0I = b.xor_(offB0I, phaseB);
+    offB0I = b.mul(offB0I, b.i32_val(vecB));
+    offB[i] = b.add(b.mul(offB0I, strideB0), b.mul(offB1, strideB1));
   }
 
   Type elemTy = f16_ty;
@@ -282,7 +285,7 @@ static Value loadB(Value tensor, const SharedMemoryObject &smemObj,
   SmallVector<Value> ptrB(numPtrB);
   ValueTable hbs;
   for (int i = 0; i < numPtrB; ++i)
-    ptrB[i] = gep(ptr_ty(ctx, 3), f16_ty, smem, offB[i]);
+    ptrB[i] = b.gep(ptr_ty(ctx, 3), f16_ty, smem, offB[i]);
 
   auto ld = [&](decltype(hbs) &vals, int m, int k, Value val0, Value val1) {
     vals[{m, k}] = {val0, val1};
@@ -294,19 +297,19 @@ static Value loadB(Value tensor, const SharedMemoryObject &smemObj,
 
     int stepBN = isBRow ? n / numPtrB * numPtrB : n;
     int stepBK = isBRow ? K : K / (numPtrB * vecB) * (numPtrB * vecB);
-    Value offset = add(mul(i32_val(stepBN * strideRepN), strideBN),
-                       mul(i32_val(stepBK), strideBK));
-    Value pb = gep(ptr_ty(ctx, 3), elemTy, thePtrB, offset);
+    Value offset = b.add(b.mul(b.i32_val(stepBN * strideRepN), strideBN),
+                         b.mul(b.i32_val(stepBK), strideBK));
+    Value pb = b.gep(ptr_ty(ctx, 3), elemTy, thePtrB, offset);
 
     Type vecTy = vec_ty(i32_ty, std::max(vecB / 2, 1));
-    Value hb = load(vecTy, bitcast(pb, ptr_ty(ctx, 3)));
+    Value hb = b.load(vecTy, b.bitcast(pb, ptr_ty(ctx, 3)));
     // record lds that needs to be moved
-    Value hb00 = bitcast(extract_element(hb, i32_val(0)), elemX2Ty);
-    Value hb01 = bitcast(extract_element(hb, i32_val(1)), elemX2Ty);
+    Value hb00 = b.bitcast(b.extract_element(hb, b.i32_val(0)), elemX2Ty);
+    Value hb01 = b.bitcast(b.extract_element(hb, b.i32_val(1)), elemX2Ty);
     ld(hbs, n, K, hb00, hb01);
     if (vecB > 4) {
-      Value hb10 = bitcast(extract_element(hb, i32_val(2)), elemX2Ty);
-      Value hb11 = bitcast(extract_element(hb, i32_val(3)), elemX2Ty);
+      Value hb10 = b.bitcast(b.extract_element(hb, b.i32_val(2)), elemX2Ty);
+      Value hb11 = b.bitcast(b.extract_element(hb, b.i32_val(3)), elemX2Ty);
       if (isBRow)
         ld(hbs, n + 1, K, hb10, hb11);
       else
@@ -327,8 +330,8 @@ static Value loadB(Value tensor, const SharedMemoryObject &smemObj,
 
   SmallVector<Value> elems;
   for (auto &item : hbs) { // has is a map, the key should be ordered.
-    elems.push_back(bitcast(item.second.first, i32_ty));
-    elems.push_back(bitcast(item.second.second, i32_ty));
+    elems.push_back(b.bitcast(item.second.first, i32_ty));
+    elems.push_back(b.bitcast(item.second.second, i32_ty));
   }
 
   Value res = packLLElements(loc, typeConverter, elems, rewriter, resultTy);
diff --git a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp
index 21c2bee58..00a825ba2 100644
--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp
+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp
@@ -100,17 +100,18 @@ private:
 
 SmallVector<Value>
 MMA16816SmemLoader::computeLdmatrixMatOffs(Value lane, Value cSwizzleOffset) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   Value warpB = multiDimWarpId[0];
   Value warpId = kOrder == 2 ? multiDimWarpId[1] : multiDimWarpId[2];
   // 4x4 matrices
-  Value rowInMat = urem(lane, i32_val(8)); // row in the 8x8 matrix
-  Value matIndex =
-      udiv(lane, i32_val(8)); // linear index of the matrix in the 2x2 matrices
+  Value rowInMat = b.urem(lane, b.i32_val(8)); // row in the 8x8 matrix
+  Value matIndex = b.udiv(
+      lane, b.i32_val(8)); // linear index of the matrix in the 2x2 matrices
 
   // Decompose matIndex => s_0, s_1, that is the coordinate in 2x2 matrices in a
   // warp
-  Value matIndexY = urem(matIndex, i32_val(2));
-  Value matIndexX = udiv(matIndex, i32_val(2));
+  Value matIndexY = b.urem(matIndex, b.i32_val(2));
+  Value matIndexX = b.udiv(matIndex, b.i32_val(2));
 
   // We use different orders for a and b for better performance.
   Value kMatArr =
@@ -141,13 +142,13 @@ MMA16816SmemLoader::computeLdmatrixMatOffs(Value lane, Value cSwizzleOffset) {
   // access will be out of bound. In the future we should change this case to
   // ldmatrix.x2
   if (kOrder == 1 && nPerWarp == 8) {
-    matOff[nonKOrder] = mul(warpId, i32_val(warpMatOffset));
+    matOff[nonKOrder] = b.mul(warpId, b.i32_val(warpMatOffset));
   } else {
-    matOff[nonKOrder] = add(
-        mul(warpId, i32_val(warpMatOffset)), // warp offset (kOrder=2)
-        mul(nkMatArr,
-            i32_val(
-                inWarpMatOffset))); // matrix offset inside a warp (kOrder=2)
+    matOff[nonKOrder] = b.add(
+        b.mul(warpId, b.i32_val(warpMatOffset)), // warp offset (kOrder=2)
+        b.mul(nkMatArr,
+              b.i32_val(
+                  inWarpMatOffset))); // matrix offset inside a warp (kOrder=2)
   }
   matOff[kOrder] = kMatArr;
 
@@ -156,39 +157,41 @@ MMA16816SmemLoader::computeLdmatrixMatOffs(Value lane, Value cSwizzleOffset) {
   Value stridedMatIndex = matOff[order[1]];
   // Add the offset of the slice
   Value contiguousSliceMatOffset =
-      udiv(cSwizzleOffset, i32_val(contiguousMatShape));
+      b.udiv(cSwizzleOffset, b.i32_val(contiguousMatShape));
 
   SmallVector<Value> offs(numPtrs);
-  Value phase = urem(udiv(rowInMat, i32_val(perPhase)), i32_val(maxPhase));
+  Value phase =
+      b.urem(b.udiv(rowInMat, b.i32_val(perPhase)), b.i32_val(maxPhase));
   // To prevent out-of-bound access of B when warpsPerTile * 16 > tile_size.
   // In such a case, we need to wrap around the offset of B.
   // |0 1 2 3 0 1 2 3| -> | 0(0) 1(1) 2(2) 3(3) |
   // |0 1 2 3 0 1 2 3|    | 0(0) 1(1) 2(2) 3(3) |
   //          ~~~~~~~ out-of-bound access
 
-  Value rowOffset =
-      urem(add(rowInMat, mul(stridedMatIndex, i32_val(stridedMatShape))),
-           i32_val(tileShape[order[1]]));
+  Value rowOffset = b.urem(
+      b.add(rowInMat, b.mul(stridedMatIndex, b.i32_val(stridedMatShape))),
+      b.i32_val(tileShape[order[1]]));
   auto contiguousTileNumMats = tileShape[order[0]] / matShape[order[0]];
 
   for (int i = 0; i < numPtrs; ++i) {
     Value contiguousIndex =
-        add(contiguousMatIndex, i32_val(i * contiguousLoadMatOffset));
+        b.add(contiguousMatIndex, b.i32_val(i * contiguousLoadMatOffset));
     if (warpsPerCTA[order[0]] > contiguousTileNumMats ||
         contiguousTileNumMats % warpsPerCTA[order[0]] != 0)
-      contiguousIndex = urem(contiguousIndex, i32_val(contiguousTileNumMats));
-    contiguousIndex = add(contiguousIndex, contiguousSliceMatOffset);
-    Value contiguousIndexSwizzled = xor_(contiguousIndex, phase);
+      contiguousIndex =
+          b.urem(contiguousIndex, b.i32_val(contiguousTileNumMats));
+    contiguousIndex = b.add(contiguousIndex, contiguousSliceMatOffset);
+    Value contiguousIndexSwizzled = b.xor_(contiguousIndex, phase);
     if (tileShape[0] != 1) {
       Value batchOffset =
-          mul(warpB, i32_val(tileShape[order[0]] * tileShape[order[1]]));
-      offs[i] =
-          add(batchOffset,
-              add(mul(contiguousIndexSwizzled, i32_val(contiguousMatShape)),
-                  mul(rowOffset, stridedSmemOffset)));
+          b.mul(warpB, b.i32_val(tileShape[order[0]] * tileShape[order[1]]));
+      offs[i] = b.add(batchOffset, b.add(b.mul(contiguousIndexSwizzled,
+                                               b.i32_val(contiguousMatShape)),
+                                         b.mul(rowOffset, stridedSmemOffset)));
     } else {
-      offs[i] = add(mul(contiguousIndexSwizzled, i32_val(contiguousMatShape)),
-                    mul(rowOffset, stridedSmemOffset));
+      offs[i] =
+          b.add(b.mul(contiguousIndexSwizzled, b.i32_val(contiguousMatShape)),
+                b.mul(rowOffset, stridedSmemOffset));
     }
   }
 
@@ -221,6 +224,7 @@ MMA16816SmemLoader::computeLdmatrixMatOffs(Value lane, Value cSwizzleOffset) {
 
 SmallVector<Value> MMA16816SmemLoader::computeLdsMatOffs(Value lane,
                                                          Value cSwizzleOffset) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   Value warpB = multiDimWarpId[0];
   Value warpOff = kOrder == 2 ? multiDimWarpId[1] : multiDimWarpId[2];
   int cTileShape = tileShape[order[0]];
@@ -239,40 +243,44 @@ SmallVector<Value> MMA16816SmemLoader::computeLdsMatOffs(Value lane,
   int numQuadI = 2;
 
   // outer index base
-  Value iBase = udiv(lane, i32_val(laneWidth));
+  Value iBase = b.udiv(lane, b.i32_val(laneWidth));
 
   for (int rep = 0; rep < numPtrs / (2 * kWidth); ++rep)
     for (int quadId = 0; quadId < 2; ++quadId)
       for (int elemId = 0; elemId < kWidth; ++elemId) {
         // inner index base
-        Value jBase = mul(urem(lane, i32_val(laneWidth)), i32_val(kWidth));
-        jBase = add(jBase, i32_val(elemId));
+        Value jBase =
+            b.mul(b.urem(lane, b.i32_val(laneWidth)), b.i32_val(kWidth));
+        jBase = b.add(jBase, b.i32_val(elemId));
         // inner index offset
-        Value jOff = i32_val(0);
+        Value jOff = b.i32_val(0);
         if (!needTrans) {
-          jOff = add(jOff, i32_val(quadId));
-          jOff = add(jOff, i32_val(rep * contiguousLoadMatOffset));
+          jOff = b.add(jOff, b.i32_val(quadId));
+          jOff = b.add(jOff, b.i32_val(rep * contiguousLoadMatOffset));
         }
         // outer index offset
-        Value iOff = mul(warpOff, i32_val(warpMatOffset));
+        Value iOff = b.mul(warpOff, b.i32_val(warpMatOffset));
         if (needTrans) {
           int pStride = kOrder == 2 ? 1 : 2;
-          iOff = add(iOff, i32_val(quadId * inWarpMatOffset));
-          iOff = add(iOff, i32_val(rep * contiguousLoadMatOffset * pStride));
+          iOff = b.add(iOff, b.i32_val(quadId * inWarpMatOffset));
+          iOff =
+              b.add(iOff, b.i32_val(rep * contiguousLoadMatOffset * pStride));
         }
         // swizzle
         if (!needTrans) {
-          Value phase = urem(udiv(iBase, i32_val(perPhase)), i32_val(maxPhase));
-          jOff = add(jOff, udiv(cSwizzleOffset, i32_val(quadWidth)));
-          jOff = xor_(jOff, phase);
+          Value phase =
+              b.urem(b.udiv(iBase, b.i32_val(perPhase)), b.i32_val(maxPhase));
+          jOff = b.add(jOff, b.udiv(cSwizzleOffset, b.i32_val(quadWidth)));
+          jOff = b.xor_(jOff, phase);
         } else {
-          Value phase = urem(udiv(jBase, i32_val(perPhase)), i32_val(maxPhase));
-          iOff = add(iOff, udiv(cSwizzleOffset, i32_val(quadHeight)));
-          iOff = xor_(iOff, phase);
+          Value phase =
+              b.urem(b.udiv(jBase, b.i32_val(perPhase)), b.i32_val(maxPhase));
+          iOff = b.add(iOff, b.udiv(cSwizzleOffset, b.i32_val(quadHeight)));
+          iOff = b.xor_(iOff, phase);
         }
         // To prevent out-of-bound access when tile is too small.
-        Value i = add(iBase, mul(iOff, i32_val(quadHeight)));
-        Value j = add(jBase, mul(jOff, i32_val(quadWidth)));
+        Value i = b.add(iBase, b.mul(iOff, b.i32_val(quadHeight)));
+        Value j = b.add(jBase, b.mul(jOff, b.i32_val(quadWidth)));
         // Compute id of this ptr
         int idx = rep * 2 * kWidth;
         if (needTrans) {
@@ -285,14 +293,14 @@ SmallVector<Value> MMA16816SmemLoader::computeLdsMatOffs(Value lane,
         }
 
         if (needTrans) {
-          offs[idx] = add(i, mul(j, stridedSmemOffset));
+          offs[idx] = b.add(i, b.mul(j, stridedSmemOffset));
         } else {
-          offs[idx] = add(mul(i, stridedSmemOffset), j);
+          offs[idx] = b.add(b.mul(i, stridedSmemOffset), j);
         }
         if (tileShape[0] != 1) {
-          Value batchOffset =
-              mul(warpB, i32_val(tileShape[order[0]] * tileShape[order[1]]));
-          offs[idx] = add(batchOffset, offs[idx]);
+          Value batchOffset = b.mul(
+              warpB, b.i32_val(tileShape[order[0]] * tileShape[order[1]]));
+          offs[idx] = b.add(batchOffset, offs[idx]);
         }
       }
 
@@ -302,6 +310,7 @@ SmallVector<Value> MMA16816SmemLoader::computeLdsMatOffs(Value lane,
 std::tuple<Value, Value, Value, Value>
 MMA16816SmemLoader::loadX4(int batch, int mat0, int mat1, ArrayRef<Value> ptrs,
                            Type matTy, Type shemTy) const {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   assert(mat0 % 2 == 0 && mat1 % 2 == 0 && "smem matrix load must be aligned");
   int matIdx[3] = {0, mat0, mat1};
 
@@ -337,14 +346,14 @@ MMA16816SmemLoader::loadX4(int batch, int mat0, int mat1, ArrayRef<Value> ptrs,
   }
 
   if (canUseLdmatrix) {
-    Value stridedOffset =
-        mul(i32_val(matIdx[order[1]] * stridedLoadMatOffset * stridedMatShape),
-            stridedSmemOffset);
+    Value stridedOffset = b.mul(
+        b.i32_val(matIdx[order[1]] * stridedLoadMatOffset * stridedMatShape),
+        stridedSmemOffset);
     if (batch != 0)
-      stridedOffset = add(
-          stridedOffset, mul(i32_val(batch * warpsPerCTA[0]), smemBatchOffset));
+      stridedOffset = b.add(
+          stridedOffset, b.mul(b.i32_val(batch * warpsPerCTA[0]), smemBatchOffset));
 
-    Value readPtr = gep(ptr_ty(ctx, 3), shemTy, ptr, stridedOffset);
+    Value readPtr = b.gep(ptr_ty(ctx, 3), shemTy, ptr, stridedOffset);
 
     PTXBuilder builder;
     // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a
@@ -360,8 +369,8 @@ MMA16816SmemLoader::loadX4(int batch, int mat0, int mat1, ArrayRef<Value> ptrs,
     // The result type is 4xi32, each i32 is composed of 2xf16
     // elements (adjacent two columns in a row) or a single f32 element.
     Value resV4 = builder.launch(rewriter, loc, resTy);
-    return {extract_val(elemTy, resV4, 0), extract_val(elemTy, resV4, 1),
-            extract_val(elemTy, resV4, 2), extract_val(elemTy, resV4, 3)};
+    return {b.extract_val(elemTy, resV4, 0), b.extract_val(elemTy, resV4, 1),
+            b.extract_val(elemTy, resV4, 2), b.extract_val(elemTy, resV4, 3)};
   } else {
     // base pointers
     std::array<std::array<Value, 4>, 2> ptrs;
@@ -377,11 +386,11 @@ MMA16816SmemLoader::loadX4(int batch, int mat0, int mat1, ArrayRef<Value> ptrs,
                                   : stridedLoadMatOffset * stridedMatShape;
     else
       _i1 += (kOrder == 2 ? 1 : stridedLoadMatOffset) * stridedMatShape;
-    Value i0 = mul(i32_val(_i0), stridedSmemOffset);
-    Value i1 = mul(i32_val(_i1), stridedSmemOffset);
+    Value i0 = b.mul(b.i32_val(_i0), stridedSmemOffset);
+    Value i1 = b.mul(b.i32_val(_i1), stridedSmemOffset);
     if (batch != 0) {
-      i0 = add(i0, mul(i32_val(batch * warpsPerCTA[0]), smemBatchOffset));
-      i1 = add(i1, mul(i32_val(batch * warpsPerCTA[0]), smemBatchOffset));
+      i0 = b.add(i0, b.mul(b.i32_val(batch * warpsPerCTA[0]), smemBatchOffset));
+      i1 = b.add(i1, b.mul(b.i32_val(batch * warpsPerCTA[0]), smemBatchOffset));
     }
     std::array<Value, 2> ii = {i0, i1};
     // load 4 32-bit values from shared memory
@@ -390,7 +399,7 @@ MMA16816SmemLoader::loadX4(int batch, int mat0, int mat1, ArrayRef<Value> ptrs,
 
     for (int i = 0; i < 4; ++i)
       for (int j = 0; j < vecWidth; ++j) {
-        vptrs[i][j] = gep(ptr_ty(ctx, 3), shemTy, ptrs[i / 2][j], ii[i % 2]);
+        vptrs[i][j] = b.gep(ptr_ty(ctx, 3), shemTy, ptrs[i / 2][j], ii[i % 2]);
       }
     // row + trans and col + no-trans are equivalent
     bool isActualTrans =
@@ -402,27 +411,27 @@ MMA16816SmemLoader::loadX4(int batch, int mat0, int mat1, ArrayRef<Value> ptrs,
     int canonWidth = (8 * elemBytes * inc) / canonBits;
     Type canonInt = int_ty(canonBits);
     std::array<Value, 4> retElems;
-    retElems.fill(undef(vec_ty(canonInt, 32 / canonBits)));
+    retElems.fill(b.undef(vec_ty(canonInt, 32 / canonBits)));
     for (int r = 0; r < 2; ++r) {
       for (int em = 0; em < 2 * vecWidth; em += inc) {
         int e = em % vecWidth;
         int m = em / vecWidth;
         int idx = m * 2 + r;
-        Value ptr = bitcast(vptrs[idx][e], ptr_ty(ctx, 3));
-        Value val = load(packedTy, ptr);
-        Value canonval = bitcast(val, vec_ty(canonInt, canonWidth));
+        Value ptr = b.bitcast(vptrs[idx][e], ptr_ty(ctx, 3));
+        Value val = b.load(packedTy, ptr);
+        Value canonval = b.bitcast(val, vec_ty(canonInt, canonWidth));
         for (int w = 0; w < canonWidth; ++w) {
           int ridx = idx + w * kWidth / vecWidth;
-          retElems[ridx] =
-              insert_element(retElems[ridx],
-                             extract_element(canonval, i32_val(w)), i32_val(e));
+          retElems[ridx] = b.insert_element(
+              retElems[ridx], b.extract_element(canonval, b.i32_val(w)),
+              b.i32_val(e));
         }
       }
     }
     if (isActualTrans)
       std::swap(retElems[1], retElems[2]);
-    return {bitcast(retElems[0], i32_ty), bitcast(retElems[1], i32_ty),
-            bitcast(retElems[2], i32_ty), bitcast(retElems[3], i32_ty)};
+    return {b.bitcast(retElems[0], i32_ty), b.bitcast(retElems[1], i32_ty),
+            b.bitcast(retElems[2], i32_ty), b.bitcast(retElems[3], i32_ty)};
   }
 }
 
@@ -535,6 +544,7 @@ getLoadMatrixFn(MemDescType descTy, const SharedMemoryObject &smemObj,
                 Value lane, ValueTable &vals, bool isA,
                 const LLVMTypeConverter *typeConverter,
                 ConversionPatternRewriter &rewriter, Location loc) {
+  auto tb = TritonLLVMOpBuilder(loc, rewriter);
   auto shapePerCTA = getShapePerCTA(descTy);
   Type eltTy = descTy.getElementType();
   // We assumes that the input operand of Dot should be from shared layout.
@@ -550,7 +560,7 @@ getLoadMatrixFn(MemDescType descTy, const SharedMemoryObject &smemObj,
       std::max<int>(shapePerCTA[2] / mmaLayout.getWarpsPerCTA()[2], 8);
 
   // (a, b) is the coordinate.
-  auto load = [=, &rewriter, &vals](int batch, int a, int b) {
+  auto load = [=, &rewriter, &vals, &tb](int batch, int a, int b) {
     MMA16816SmemLoader loader(
         nPerWarp, warpsPerTile, sharedLayout.getOrder(),
         mmaLayout.getWarpsPerCTA(), kOrder, kWidth, smemObj.strides,
@@ -566,7 +576,7 @@ getLoadMatrixFn(MemDescType descTy, const SharedMemoryObject &smemObj,
     Type smemTy = getSharedMemTy(eltTy);
     for (int i = 0; i < numPtrs; ++i)
       ptrs[i] =
-          gep(ptr_ty(rewriter.getContext(), 3), smemTy, smemBase, offs[i]);
+          tb.gep(ptr_ty(rewriter.getContext(), 3), smemTy, smemBase, offs[i]);
     // actually load from shared memory
     auto matTy = LLVM::LLVMStructType::getLiteral(eltTy.getContext(),
                                                   SmallVector<Type>(4, i32_ty));
@@ -595,6 +605,7 @@ Value loadArg(ConversionPatternRewriter &rewriter, Location loc,
               MemDescType descTy, DotOperandEncodingAttr encoding,
               const SharedMemoryObject &smemObj,
               const LLVMTypeConverter *typeConverter, Value thread, bool isA) {
+  auto tb = TritonLLVMOpBuilder(loc, rewriter);
   auto shapePerCTA = getShapePerCTA(descTy);
   int bitwidth = descTy.getElementTypeBitWidth();
   auto mmaLayout = mlir::cast<NvidiaMmaEncodingAttr>(encoding.getParent());
@@ -609,16 +620,16 @@ Value loadArg(ConversionPatternRewriter &rewriter, Location loc,
 
   auto warpsPerCTA = mmaLayout.getWarpsPerCTA();
   auto order = triton::gpu::getOrder(mmaLayout);
-  Value warp = udiv(thread, i32_val(32));
-  Value lane = urem(thread, i32_val(32));
+  Value warp = tb.udiv(thread, tb.i32_val(32));
+  Value lane = tb.urem(thread, tb.i32_val(32));
 
   SmallVector<Value> multiDimWarpId =
       delinearize(rewriter, loc, warp, warpsPerCTA, order);
-  Value warpB = urem(multiDimWarpId[0], i32_val(shapePerCTA[0]));
+  Value warpB = tb.urem(multiDimWarpId[0], tb.i32_val(shapePerCTA[0]));
   int warpsPerTile;
   auto rank = shapePerCTA.size();
-  Value warpM = urem(multiDimWarpId[1], i32_val(shapePerCTA[1] / 16));
-  Value warpN = urem(multiDimWarpId[2], i32_val(shapePerCTA[2] / 8));
+  Value warpM = tb.urem(multiDimWarpId[1], tb.i32_val(shapePerCTA[1] / 16));
+  Value warpN = tb.urem(multiDimWarpId[2], tb.i32_val(shapePerCTA[2] / 8));
   if (isA)
     warpsPerTile = std::min<int>(warpsPerCTA[1], shapePerCTA[1] / 16);
   else
@@ -751,13 +762,14 @@ SharedMemoryObject
 getExpandedSharedMemoryObject(ConversionPatternRewriter &rewriter, Location loc,
                               SharedMemoryObject smemObj,
                               ArrayRef<int64_t> shape) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto strides = smemObj.getStrides();
   auto offsets = smemObj.getOffsets();
   auto rank = strides.size();
   if (rank == 3)
     return smemObj;
-  auto expandedStrides = insertValue(strides, 0, i32_val(shape[0] * shape[1]));
-  auto expandedOffsets = insertValue(offsets, 0, i32_val(0));
+  auto expandedStrides = insertValue(strides, 0, b.i32_val(shape[0] * shape[1]));
+  auto expandedOffsets = insertValue(offsets, 0, b.i32_val(0));
   auto expandedSmemObj =
       SharedMemoryObject(smemObj.getBase(), smemObj.getBaseElemType(),
                          expandedStrides, expandedOffsets);
diff --git a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/DotOpToLLVM/MMAv1.cpp b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/DotOpToLLVM/MMAv1.cpp
index 9d40f1072..4e95e8949 100644
--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/DotOpToLLVM/MMAv1.cpp
+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/DotOpToLLVM/MMAv1.cpp
@@ -41,6 +41,7 @@ LogicalResult convertMMA884(triton::DotOp op, triton::DotOp::Adaptor adaptor,
                             ConversionPatternRewriter &rewriter) {
   auto *ctx = op.getContext();
   auto loc = op.getLoc();
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
 
   Value A = op.getA();
   Value B = op.getB();
@@ -133,7 +134,7 @@ LogicalResult convertMMA884(triton::DotOp op, triton::DotOp::Adaptor adaptor,
     Value res = builder.launch(rewriter, loc, getMmaRetType(ATensorTy));
 
     for (auto i = 0; i < 8; i++) {
-      Value elem = extract_val(f32_ty, res, i);
+      Value elem = b.extract_val(f32_ty, res, i);
       acc[idx[i]] = elem;
     }
   };
diff --git a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/DotOpToLLVM/MMAv2.cpp b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/DotOpToLLVM/MMAv2.cpp
index c2940a043..84bcb66f2 100644
--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/DotOpToLLVM/MMAv2.cpp
+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/DotOpToLLVM/MMAv2.cpp
@@ -16,6 +16,7 @@ using ValueTableV2 = std::map<std::array<int, 3>, Value>;
 Value loadC(Value tensor, Value llTensor,
             const LLVMTypeConverter *typeConverter, Location loc,
             ConversionPatternRewriter &rewriter) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   MLIRContext *ctx = tensor.getContext();
   auto tensorTy = cast<RankedTensorType>(tensor.getType());
   size_t fcSize = triton::gpu::getTotalElemsPerThread(tensor.getType());
@@ -41,8 +42,9 @@ Value loadC(Value tensor, Value llTensor,
     for (int i = 0; i < fcSize; i += numCPackedElem) {
       Value pack = rewriter.create<LLVM::UndefOp>(loc, cPackTy);
       for (int j = 0; j < numCPackedElem; ++j) {
-        pack = insert_element(
-            cPackTy, pack, extract_val(cElemTy, llTensor, i + j), i32_val(j));
+        pack = b.insert_element(cPackTy, pack,
+                                b.extract_val(cElemTy, llTensor, i + j),
+                                b.i32_val(j));
       }
       cPack.push_back(pack);
     }
@@ -382,6 +384,7 @@ LogicalResult convertDot(const LLVMTypeConverter *typeConverter,
                          Value a, Value b, Value c, Value d, Value loadedA,
                          Value loadedB, Value loadedC, DotOp op,
                          DotOpAdaptor adaptor, bool isTuring) {
+  auto tb = TritonLLVMOpBuilder(loc, rewriter);
   MLIRContext *ctx = c.getContext();
   auto aTensorTy = cast<RankedTensorType>(a.getType());
   auto bTensorTy = cast<RankedTensorType>(b.getType());
@@ -456,7 +459,7 @@ LogicalResult convertDot(const LLVMTypeConverter *typeConverter,
     Type elemTy = cast<LLVM::LLVMStructType>(mmaOut.getType()).getBody()[0];
     for (int i = 0; i < numMmaRets; ++i) {
       fc[(m * colsPerThread + 4 * n) / numCPackedElem + i + batchOffset * b] =
-          extract_val(elemTy, mmaOut, i);
+          tb.extract_val(elemTy, mmaOut, i);
     }
   };
 
@@ -476,8 +479,8 @@ LogicalResult convertDot(const LLVMTypeConverter *typeConverter,
     for (int j = 0; j < numCPackedElem; ++j) {
       results[i * numCPackedElem + j] =
           numCPackedElem > 1
-              ? bitcast(extract_element(fc[i], i32_val(j)), resElemTy)
-              : bitcast(fc[i], resElemTy);
+              ? tb.bitcast(tb.extract_element(fc[i], tb.i32_val(j)), resElemTy)
+              : tb.bitcast(fc[i], resElemTy);
     }
   }
   Value res = packLLElements(loc, typeConverter, results, rewriter, structTy);
diff --git a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/DotOpToLLVM/WGMMA.cpp b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/DotOpToLLVM/WGMMA.cpp
index 1bb55373e..9eb113da9 100644
--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/DotOpToLLVM/WGMMA.cpp
+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/DotOpToLLVM/WGMMA.cpp
@@ -90,6 +90,7 @@ int64_t getSwizzlingFromLayout(const SharedEncodingAttr &layout,
 
 static Value createDescriptor(ConversionPatternRewriter &rewriter, Location loc,
                               int64_t swizzling, uint32_t stride) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   // Create descriptor based on the format described in the spec:
   // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-shared-memory-layout-matrix-descriptor
   union WGMMADescriptor {
@@ -128,7 +129,7 @@ static Value createDescriptor(ConversionPatternRewriter &rewriter, Location loc,
   }
   desc.strideDimensionBaseOffset = swizzling >> 1;
   desc.leadDimensionBaseOffset = (swizzling * stride) >> 4;
-  return int_val(64, desc.descriptor);
+  return b.int_val(64, desc.descriptor);
 }
 
 class DotOpMmaV3SmemLoader {
@@ -140,6 +141,7 @@ public:
                        ConversionPatternRewriter &rewriter, Location loc)
       : base(base), shape(shape), warpId(warpId), dimWpt(dimWpt), trans(trans),
         instrShape(instrShape) {
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto ty = cast<MemDescType>(tensor.getType());
     auto sharedLayout = cast<SharedEncodingAttr>(ty.getEncoding());
     ord = sharedLayout.getOrder();
@@ -147,7 +149,7 @@ public:
     const int maxPhase = sharedLayout.getMaxPhase();
     elemBytes = ty.getElementTypeBitWidth() / 8;
     elemsPerSwizzlingRow = 128 / perPhase / elemBytes;
-    elemsPerSwizzlingRowVal = i32_val(elemsPerSwizzlingRow);
+    elemsPerSwizzlingRowVal = b.i32_val(elemsPerSwizzlingRow);
 
     uint32_t widthInByte = shape[ord[0]] * elemBytes;
     int64_t swizzling = getSwizzlingFromLayout(sharedLayout, widthInByte);
@@ -157,25 +159,26 @@ public:
 
   Value smemLoad(int a, int b, ConversionPatternRewriter &rewriter,
                  Location loc) {
-    Value k = i32_val(b * instrShape[1]);
-    Value m = add(i32_val(a * dimWpt * instrShape[0]),
-                  mul(warpId, i32_val(instrShape[0])));
+    auto tb = TritonLLVMOpBuilder(loc, rewriter);
+    Value k = tb.i32_val(b * instrShape[1]);
+    Value m = tb.add(tb.i32_val(a * dimWpt * instrShape[0]),
+                     tb.mul(warpId, tb.i32_val(instrShape[0])));
     if (trans) {
       std::swap(k, m);
     }
-    Value leading_offset = mul(udiv(k, elemsPerSwizzlingRowVal),
-                               i32_val(shape[ord[1]] * elemsPerSwizzlingRow));
-    Value stride_offset = mul(m, elemsPerSwizzlingRowVal);
-    Value offset = add(add(leading_offset, stride_offset),
-                       urem(k, elemsPerSwizzlingRowVal));
-    Value off1 = mul(i32_val(elemBytes), offset);
-    Value off_ = zext(i64_ty, udiv(off1, i32_val(16)));
-
-    Value loadDesc = add(descriptor, off_);
+    Value leading_offset = tb.mul(tb.udiv(k, elemsPerSwizzlingRowVal),
+                                  tb.i32_val(shape[ord[1]] * elemsPerSwizzlingRow));
+    Value stride_offset = tb.mul(m, elemsPerSwizzlingRowVal);
+    Value offset = tb.add(tb.add(leading_offset, stride_offset),
+                          tb.urem(k, elemsPerSwizzlingRowVal));
+    Value off1 = tb.mul(tb.i32_val(elemBytes), offset);
+    Value off_ = tb.zext(i64_ty, tb.udiv(off1, tb.i32_val(16)));
+
+    Value loadDesc = tb.add(descriptor, off_);
     // Add the base at the end to make it easier to do loop invariant code
     // motion.
-    loadDesc = add(loadDesc, lshr(shl(ptrtoint(i64_ty, base), int_val(64, 46)),
-                                  int_val(64, 50)));
+    loadDesc = tb.add(loadDesc, tb.lshr(tb.shl(tb.ptrtoint(i64_ty, base), tb.int_val(64, 46)),
+                                        tb.int_val(64, 50)));
     return loadDesc;
   }
 
@@ -197,6 +200,7 @@ DotOpMmaV3SmemLoader loadA(const LLVMTypeConverter *typeConverter,
                            ConversionPatternRewriter &rewriter, Location loc,
                            const NvidiaMmaEncodingAttr &mmaEncoding,
                            Value tensor, Value smemObjBase, Value thread) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto aTy = cast<TensorOrMemDesc>(tensor.getType());
   auto aSharedLayout = dyn_cast<SharedEncodingAttr>(aTy.getEncoding());
   assert(aSharedLayout && "only support load dot operand from shared.");
@@ -211,13 +215,13 @@ DotOpMmaV3SmemLoader loadA(const LLVMTypeConverter *typeConverter,
 
   // The descriptor should be calculated based on the first warp of the
   // warpgroup.
-  Value warp = and_(udiv(thread, i32_val(32)), i32_val(0xFFFFFFFC));
+  Value warp = b.and_(b.udiv(thread, b.i32_val(32)), b.i32_val(0xFFFFFFFC));
   // Workaround for a bug in ptxas 12.3 that cause a failure in
   // test_core.py::test_dot. The shuffle will force the compiler to treat the
   // value as uniform and prevent wrong optimizations.
   warp = mlir::LLVM::NVIDIA::shuffleIdx(loc, rewriter, warp, 0);
-  Value warpM = urem(warp, i32_val(wpt[0]));
-  Value warpId = urem(warpM, i32_val(shapePerCTA[0] / instrShape[0]));
+  Value warpM = b.urem(warp, b.i32_val(wpt[0]));
+  Value warpId = b.urem(warpM, b.i32_val(shapePerCTA[0] / instrShape[0]));
 
   return {tensor,
           smemObjBase,
@@ -234,6 +238,7 @@ DotOpMmaV3SmemLoader loadB(const LLVMTypeConverter *typeConverter,
                            ConversionPatternRewriter &rewriter, Location loc,
                            NvidiaMmaEncodingAttr &mmaEncoding, Value tensor,
                            Value base, Value thread) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto bTy = cast<MemDescType>(tensor.getType());
   auto bSharedLayout = cast<SharedEncodingAttr>(bTy.getEncoding());
   assert(bSharedLayout && "only support load B from shared.");
@@ -246,10 +251,10 @@ DotOpMmaV3SmemLoader loadB(const LLVMTypeConverter *typeConverter,
   int numRepK = ceil<unsigned>(shapePerCTA[0], instrShape[2]);
   int numRepN = ceil<unsigned>(shapePerCTA[1], instrShape[1] * wpt[1]);
 
-  Value warp = and_(udiv(thread, i32_val(32)), i32_val(0xFFFFFFFC));
-  Value warpMN = udiv(warp, i32_val(wpt[0]));
-  Value warpN = urem(warpMN, i32_val(wpt[1]));
-  Value warpId = urem(warpN, i32_val(shapePerCTA[1] / instrShape[1]));
+  Value warp = b.and_(b.udiv(thread, b.i32_val(32)), b.i32_val(0xFFFFFFFC));
+  Value warpMN = b.udiv(warp, b.i32_val(wpt[0]));
+  Value warpN = b.urem(warpMN, b.i32_val(wpt[1]));
+  Value warpId = b.urem(warpN, b.i32_val(shapePerCTA[1] / instrShape[1]));
 
   return {tensor,
           base,
@@ -269,6 +274,7 @@ llvm::SmallVector<Value> loadReg(ConversionPatternRewriter &rewriter,
                                  const SmallVector<Value> &elements,
                                  int startIndex, int numElements,
                                  Operation *insertBefore) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   OpBuilder::InsertionGuard g(rewriter);
   rewriter.setInsertionPoint(insertBefore);
 
@@ -290,9 +296,9 @@ llvm::SmallVector<Value> loadReg(ConversionPatternRewriter &rewriter,
     Value pack = rewriter.create<LLVM::UndefOp>(loc, packTy);
     for (int j = 0; j < numElemsPer32Bits; ++j) {
       Value element = elements[startIndex + i * numElemsPer32Bits + j];
-      pack = insert_element(packTy, pack, element, i32_val(j));
+      pack = b.insert_element(packTy, pack, element, b.i32_val(j));
     }
-    pack = bitcast(pack, rewriter.getIntegerType(32));
+    pack = b.bitcast(pack, rewriter.getIntegerType(32));
     mmaOut[i] = pack;
   }
   return mmaOut;
@@ -303,15 +309,18 @@ SmallVector<Value> unpackAccumulator(ConversionPatternRewriter &rewriter,
                                      Location loc,
                                      const SmallVector<Value> &packed,
                                      RankedTensorType tensorTy) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   if (!tensorTy.getElementType().isF16())
     return packed;
   // For fp16 the accumulator is pack into 32-bit integers so we need to unpack
   // it.
   SmallVector<Value> results;
   for (Value elem : packed) {
-    elem = bitcast(elem, vec_ty(rewriter.getF16Type(), 2));
-    results.push_back(extract_element(rewriter.getF16Type(), elem, i32_val(0)));
-    results.push_back(extract_element(rewriter.getF16Type(), elem, i32_val(1)));
+    elem = b.bitcast(elem, vec_ty(rewriter.getF16Type(), 2));
+    results.push_back(
+        b.extract_element(rewriter.getF16Type(), elem, b.i32_val(0)));
+    results.push_back(
+        b.extract_element(rewriter.getF16Type(), elem, b.i32_val(1)));
   }
   return results;
 }
@@ -332,19 +341,20 @@ static Value faddAccumulate(ConversionPatternRewriter &rewriter, Location loc,
 static SmallVector<Value> emitWait(ConversionPatternRewriter &rewriter,
                                    Location loc, SmallVector<Value> acc,
                                    int pendings) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   SmallVector<Type> types(acc.size(), acc[0].getType());
   auto structTy =
       LLVM::LLVMStructType::getLiteral(rewriter.getContext(), types);
   Value llvmStruct = rewriter.create<LLVM::UndefOp>(loc, structTy);
   int i = 0;
   for (Value v : acc) {
-    llvmStruct = insert_val(structTy, llvmStruct, v, i++);
+    llvmStruct = b.insert_val(structTy, llvmStruct, v, i++);
   }
   Value res = rewriter.create<triton::nvgpu::WGMMAWaitGroupOp>(loc, llvmStruct,
                                                                pendings);
   SmallVector<Value> results;
   for (int i = 0; i < acc.size(); ++i) {
-    results.push_back(extract_val(types[0], res, i));
+    results.push_back(b.extract_val(types[0], res, i));
   }
   return results;
 }
@@ -356,6 +366,7 @@ LogicalResult convertDot(const LLVMTypeConverter *typeConverter,
                          Value loadedC, bool allowTF32,
                          bool needsPartialAccumulator,
                          uint32_t maxNumImpreciseAcc, bool sync, Value thread) {
+  auto tb = TritonLLVMOpBuilder(loc, rewriter);
   auto aTensorTy = cast<TensorOrMemDesc>(a.getType());
   auto bTensorTy = cast<TensorOrMemDesc>(b.getType());
   auto dTensorTy = cast<RankedTensorType>(d.getType());
@@ -428,13 +439,13 @@ LogicalResult convertDot(const LLVMTypeConverter *typeConverter,
       auto accTy =
           LLVM::LLVMStructType::getLiteral(rewriter.getContext(), elemTypes);
       Value d;
-      Value useC = i1_val(0);
+      Value useC = tb.i1_val(0);
       if (!zeroAcc) {
         d = packLLElements(loc, typeConverter, mmaOut, rewriter, accTy);
-        useC = i1_val(1);
+        useC = tb.i1_val(1);
       }
       if (useCOperand)
-        useC = and_(useC, useCOperand);
+        useC = tb.and_(useC, useCOperand);
       uint32_t numLowPrecisionAcc = 0;
       Value partialAcc;
       for (int k = 0; k < numRepK; ++k) {
@@ -462,7 +473,7 @@ LogicalResult convertDot(const LLVMTypeConverter *typeConverter,
         mmaAcc = rewriter.create<triton::nvgpu::WGMMAOp>(
             loc, accTy, a, b, useC, mmaAcc, M, N, K, eltTypeC, eltTypeA,
             eltTypeB, layoutA, layoutB);
-        useC = i1_val(1);
+        useC = tb.i1_val(1);
         if (needsPartialAccumulator)
           partialAcc = mmaAcc;
         else
diff --git a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ElementwiseOpToLLVM.cpp b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ElementwiseOpToLLVM.cpp
index ef69b96fc..521765aee 100644
--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ElementwiseOpToLLVM.cpp
+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ElementwiseOpToLLVM.cpp
@@ -259,11 +259,11 @@ static ConverterT makeConverterFromPtx(const std::string &ptxAsm, Type inType,
                                        Type outType,
                                        const int inVecWidthBits = 32,
                                        const int outVecWidthBits = 32) {
-
   ConverterT converter =
       [ptxAsm, inType, outType, inVecWidthBits,
        outVecWidthBits](Location loc, ConversionPatternRewriter &rewriter,
                         const SmallVector<Value> &v) -> SmallVector<Value> {
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     int numElements = v.size();
     assert(numElements == 4 || numElements == 2 && "invalid vector size");
 
@@ -273,12 +273,12 @@ static ConverterT makeConverterFromPtx(const std::string &ptxAsm, Type inType,
     // first, we pack `v` into 32-bit ints
     int inVecWidth = inVecWidthBits / inBitwidth;
     auto inVecTy = vec_ty(inType, inVecWidth);
-    SmallVector<Value> inPacked(numElements / inVecWidth, undef(inVecTy));
+    SmallVector<Value> inPacked(numElements / inVecWidth, b.undef(inVecTy));
     for (size_t i = 0; i < numElements; i++)
-      inPacked[i / inVecWidth] = insert_element(
-          inVecTy, inPacked[i / inVecWidth], v[i], i32_val(i % inVecWidth));
+      inPacked[i / inVecWidth] = b.insert_element(
+          inVecTy, inPacked[i / inVecWidth], v[i], b.i32_val(i % inVecWidth));
     for (size_t i = 0; i < inPacked.size(); i++)
-      inPacked[i] = bitcast(inPacked[i], int_ty(inVecWidthBits));
+      inPacked[i] = b.bitcast(inPacked[i], int_ty(inVecWidthBits));
 
     // then, we run the provided inline PTX
     int outVecWidth = outVecWidthBits / outBitwidth;
@@ -305,13 +305,13 @@ static ConverterT makeConverterFromPtx(const std::string &ptxAsm, Type inType,
       auto outStructTy = struct_ty(SmallVector<Type>(outNums, outVecTy));
       auto outStruct = builder.launch(rewriter, loc, outStructTy, false);
       for (int i = 0; i < outNums; i++)
-        outPacked.push_back(extract_val(outVecTy, outStruct, i));
+        outPacked.push_back(b.extract_val(outVecTy, outStruct, i));
     }
     // unpack the output
     SmallVector<Value> ret;
     for (size_t i = 0; i < numElements; i++)
-      ret.push_back(extract_element(outType, outPacked[i / outVecWidth],
-                                    i32_val(i % outVecWidth)));
+      ret.push_back(b.extract_element(outType, outPacked[i / outVecWidth],
+                                      b.i32_val(i % outVecWidth)));
     return ret;
   };
   return converter;
@@ -464,6 +464,7 @@ struct FpToFpOpConversion
                                    ConversionPatternRewriter &rewriter,
                                    Type elemTy, MultipleOperandsRange operands,
                                    Location loc) const {
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto srcElementType = getElementType(op.getSrc());
     auto dstElementType = getElementType(op.getResult());
     auto roundingMode = op.getRounding();
@@ -520,7 +521,7 @@ struct FpToFpOpConversion
     if (useFP16IntermediateSrc)
       for (Value &v : inVals)
         v = convertFp32ToFp16(loc, rewriter, v, RoundingMode::RTZ);
-    inVals.resize(numElements, undef(typeConverter->convertType(srcType)));
+    inVals.resize(numElements, b.undef(typeConverter->convertType(srcType)));
     SmallVector<Value> outVals = cvtFunc(loc, rewriter, inVals);
     assert(outVals.size() == inVals.size());
     outVals.resize(std::min(numElements, operands.size()));
@@ -766,12 +767,13 @@ struct ExpOpConversionApprox
                                    ConversionPatternRewriter &rewriter,
                                    Type elemTy, MultipleOperandsRange operands,
                                    Location loc) const {
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     // For non-FP32 input, call __nv_expf for higher-precision calculation
     if (elemTy.getIntOrFloatBitWidth() != 32)
       return {};
 
     const double log2e = 1.4426950408889634;
-    Value prod = fmul(f32_ty, operands[0][0], f32_val(log2e));
+    Value prod = b.fmul(f32_ty, operands[0][0], b.f32_val(log2e));
 
     PTXBuilder ptxBuilder;
     auto &exp2 = ptxBuilder.create<PTXInstr>("ex2")->o("approx").o("f32");
diff --git a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/LoadStoreOpToLLVM.cpp b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/LoadStoreOpToLLVM.cpp
index cb430d8fa..b7f7e4a8c 100644
--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/LoadStoreOpToLLVM.cpp
+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/LoadStoreOpToLLVM.cpp
@@ -27,9 +27,10 @@ namespace {
 // Used to mask out the redundant data accessed by threads.
 Value redundantDataMask(Type valueTy, ConversionPatternRewriter &rewriter,
                         Location loc, const NVIDIA::TargetInfo &targetInfo) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto tensorTy = dyn_cast<RankedTensorType>(valueTy);
-  Value mask = int_val(1, 1);
-  auto tid = tid_val();
+  Value mask = b.int_val(1, 1);
+  auto tid = b.tid_val();
   auto clusterCTAId = targetInfo.getClusterCTAId(rewriter, loc);
   if (tensorTy) {
     auto layout = tensorTy.getEncoding();
@@ -41,9 +42,9 @@ Value redundantDataMask(Type valueTy, ConversionPatternRewriter &rewriter,
     auto order = triton::gpu::getOrder(layout);
     auto warpOrder = triton::gpu::getWarpOrder(layout);
     auto shapePerCTATile = triton::gpu::getShapePerCTATile(layout, shape);
-    Value warpSize = i32_val(32);
-    Value laneId = urem(tid, warpSize);
-    Value warpId = udiv(tid, warpSize);
+    Value warpSize = b.i32_val(32);
+    Value laneId = b.urem(tid, warpSize);
+    Value warpId = b.udiv(tid, warpSize);
     SmallVector<Value> multiDimWarpId =
         delinearize(rewriter, loc, warpId, warpsPerCTA, warpOrder);
     SmallVector<Value> multiDimThreadId =
@@ -55,14 +56,14 @@ Value redundantDataMask(Type valueTy, ConversionPatternRewriter &rewriter,
       // Otherwise, we need to mask threads that will replicate data on this
       // dimension. Calculate the thread index on this dimension for the CTA
       Value threadDim =
-          add(mul(multiDimWarpId[dim], i32_val(threadsPerWarp[dim])),
-              multiDimThreadId[dim]);
-      mask = and_(mask, icmp_slt(mul(threadDim, i32_val(sizePerThread[dim])),
-                                 i32_val(shape[dim])));
+          b.add(b.mul(multiDimWarpId[dim], b.i32_val(threadsPerWarp[dim])),
+                multiDimThreadId[dim]);
+      mask = b.and_(mask, b.icmp_slt(b.mul(threadDim, b.i32_val(sizePerThread[dim])),
+                                     b.i32_val(shape[dim])));
     }
     // Do not write duplicated data when multicast is enabled
     if (triton::gpu::getNumCTAs(layout) > 1) {
-      auto _0 = i32_val(0);
+      auto _0 = b.i32_val(0);
       auto CTAsPerCGA = triton::gpu::getCTAsPerCGA(layout);
       auto CTASplitNum = triton::gpu::getCTASplitNum(layout);
       auto CTAOrder = triton::gpu::getCTAOrder(layout);
@@ -76,7 +77,7 @@ Value redundantDataMask(Type valueTy, ConversionPatternRewriter &rewriter,
           continue;
         // This wrapping rule must be consistent with emitCTAOffsetForLayout
         unsigned splitNum = std::min<unsigned>(shape[dim], CTASplitNum[dim]);
-        Value repId = udiv(multiDimClusterCTAId[dim], i32_val(splitNum));
+        Value repId = b.udiv(multiDimClusterCTAId[dim], b.i32_val(splitNum));
         // Consider the example where CTAsPerCGA = [4] and CTASplitNum = [2]:
         //     CTA0 and CTA2 holds data of block0,
         //     CTA1 and CTA3 holds data of block1.
@@ -86,14 +87,14 @@ Value redundantDataMask(Type valueTy, ConversionPatternRewriter &rewriter,
         // Actually in all existing cases of multicast, splitNum is always 1.
         // The mask is equivalent to:
         //     multiDimClusterCTAId[dim] == 0
-        mask = and_(mask, icmp_eq(repId, _0));
+        mask = b.and_(mask, b.icmp_eq(repId, _0));
       }
     }
   } else {
     // If the tensor is not ranked, then it is a scalar and only thread 0 of
     // CTA0 can write
-    mask = and_(mask, icmp_eq(clusterCTAId, i32_val(0)));
-    mask = and_(mask, icmp_eq(tid, i32_val(0)));
+    mask = b.and_(mask, b.icmp_eq(clusterCTAId, b.i32_val(0)));
+    mask = b.and_(mask, b.icmp_eq(tid, b.i32_val(0)));
   }
   return mask;
 }
@@ -162,6 +163,7 @@ struct LoadOpConversion : public ConvertOpToLLVMPattern<triton::LoadOp>,
   matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
     auto loc = op->getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto typeConverter = getTypeConverter();
 
     // original values
@@ -253,7 +255,7 @@ struct LoadOpConversion : public ConvertOpToLLVMPattern<triton::LoadOp>,
 
       PTXBuilder ptxBuilder;
 
-      Value pred = mask ? maskElems[vecStart] : int_val(1, 1);
+      Value pred = mask ? maskElems[vecStart] : b.int_val(1, 1);
 
       const std::string readConstraint =
           (width == 64) ? "l" : ((width == 32) ? "r" : "c");
@@ -305,14 +307,14 @@ struct LoadOpConversion : public ConvertOpToLLVMPattern<triton::LoadOp>,
           size_t size = width / valueElemNBits;
 
           auto vecTy = LLVM::getFixedVectorType(valueElemTy, size);
-          Value v = undef(vecTy);
+          Value v = b.undef(vecTy);
           for (size_t s = 0; s < size; ++s) {
             Value falseVal = otherElems[vecStart + ii * size + s];
             Value sVal = createIndexAttrConstant(
                 rewriter, loc, typeConverter->getIndexType(), s);
-            v = insert_element(vecTy, v, falseVal, sVal);
+            v = b.insert_element(vecTy, v, falseVal, sVal);
           }
-          v = bitcast(v, IntegerType::get(getContext(), width));
+          v = b.bitcast(v, IntegerType::get(getContext(), width));
 
           PTXInstr::Operand *opr{};
 
@@ -344,19 +346,19 @@ struct LoadOpConversion : public ConvertOpToLLVMPattern<triton::LoadOp>,
       for (unsigned int ii = 0; ii < nWords; ++ii) {
         Value curr;
         if (isa<LLVM::LLVMStructType>(retTy)) {
-          curr = extract_val(IntegerType::get(getContext(), width), ret, ii);
+          curr = b.extract_val(IntegerType::get(getContext(), width), ret, ii);
         } else {
           curr = ret;
         }
-        curr = bitcast(curr, LLVM::getFixedVectorType(valueElemTy,
-                                                      width / valueElemNBits));
+        curr = b.bitcast(curr, LLVM::getFixedVectorType(
+                                   valueElemTy, width / valueElemNBits));
         rets.push_back(curr);
       }
       int tmp = width / valueElemNBits;
       for (size_t ii = 0; ii < vec; ++ii) {
         Value vecIdx = createIndexAttrConstant(
             rewriter, loc, typeConverter->getIndexType(), ii % tmp);
-        Value loaded = extract_element(valueElemTy, rets[ii / tmp], vecIdx);
+        Value loaded = b.extract_element(valueElemTy, rets[ii / tmp], vecIdx);
         loadedVals.push_back(loaded);
       }
     } // end vec
@@ -389,6 +391,7 @@ struct StoreOpConversion : public ConvertOpToLLVMPattern<triton::StoreOp>,
     Value llValue = adaptor.getValue();
 
     auto loc = op->getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     MLIRContext *ctx = rewriter.getContext();
 
     auto valueTy = value.getType();
@@ -448,19 +451,19 @@ struct StoreOpConversion : public ConvertOpToLLVMPattern<triton::StoreOp>,
       SmallVector<std::pair<Value, std::string>> asmArgs;
       for (size_t wordIdx = 0; wordIdx < nWords; ++wordIdx) {
         // llWord is a width-len composition
-        Value llWord = undef(wordTy);
+        Value llWord = b.undef(wordTy);
         // Insert each value element to the composition
         for (size_t elemIdx = 0; elemIdx < wordNElems; ++elemIdx) {
           const size_t elemOffset = vecStart + wordIdx * wordNElems + elemIdx;
           assert(elemOffset < valueElems.size());
           Value elem = valueElems[elemOffset];
           if (elem.getType().isInteger(1))
-            elem = sext(i8_ty, elem);
-          elem = bitcast(elem, valueElemTy);
+            elem = b.sext(i8_ty, elem);
+          elem = b.bitcast(elem, valueElemTy);
 
-          llWord = insert_element(wordTy, llWord, elem, i32_val(elemIdx));
+          llWord = b.insert_element(wordTy, llWord, elem, b.i32_val(elemIdx));
         }
-        llWord = bitcast(llWord, valArgTy);
+        llWord = b.bitcast(llWord, valArgTy);
         std::string constraint =
             (width == 64) ? "l" : ((width == 32) ? "r" : "c");
         asmArgs.emplace_back(llWord, constraint);
@@ -470,7 +473,7 @@ struct StoreOpConversion : public ConvertOpToLLVMPattern<triton::StoreOp>,
       PTXBuilder ptxBuilder;
       auto *asmArgList = ptxBuilder.newListOperand(asmArgs);
 
-      Value maskVal = llMask ? and_(mask, maskElems[vecStart]) : mask;
+      Value maskVal = llMask ? b.and_(mask, maskElems[vecStart]) : mask;
 
       auto *asmAddr =
           ptxBuilder.newAddrOperand(ptrElems[vecStart], "l", in_off);
@@ -505,8 +508,9 @@ struct StoreOpConversion : public ConvertOpToLLVMPattern<triton::StoreOp>,
 
 void createBarrier(ConversionPatternRewriter &rewriter, Location loc,
                    int numCTAs) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   if (numCTAs == 1) {
-    barrier();
+    b.barrier();
   } else {
     rewriter.create<triton::nvidia_gpu::ClusterArriveOp>(loc, false);
     rewriter.create<triton::nvidia_gpu::ClusterWaitOp>(loc);
@@ -527,6 +531,7 @@ struct AtomicCASOpConversion
   matchAndRewrite(triton::AtomicCASOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
     auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     MLIRContext *ctx = rewriter.getContext();
 
     auto moduleOp = op->getParentOfType<ModuleOp>();
@@ -567,11 +572,11 @@ struct AtomicCASOpConversion
     SmallVector<Value> resultVals(elemsPerThread);
 
     for (size_t i = 0; i < elemsPerThread; i += vec) {
-      Value casVal = undef(vecTy);
+      Value casVal = b.undef(vecTy);
       for (int ii = 0; ii < vec; ++ii) {
         Value iiVal = createIndexAttrConstant(
             rewriter, loc, getTypeConverter()->getIndexType(), ii);
-        casVal = insert_element(vecTy, casVal, valElements[i + ii], iiVal);
+        casVal = b.insert_element(vecTy, casVal, valElements[i + ii], iiVal);
       }
 
       Value casPtr = ptrElements[i];
@@ -599,7 +604,8 @@ struct AtomicCASOpConversion
         auto ret = ptxBuilderAtomicCAS.launch(rewriter, loc, retType);
         for (int ii = 0; ii < vec; ++ii) {
           resultVals[i + ii] =
-              vec == 1 ? ret : extract_element(valueElemTy, ret, i32_val(ii));
+              vec == 1 ? ret
+                       : b.extract_element(valueElemTy, ret, b.i32_val(ii));
         }
       } else {
         auto old = ptxBuilderAtomicCAS.launch(rewriter, loc, valueElemTy);
@@ -609,7 +615,7 @@ struct AtomicCASOpConversion
         }
         Value atomPtr = LLVM::getSharedMemoryBase(loc, rewriter, targetInfo,
                                                   op.getOperation());
-        atomPtr = bitcast(atomPtr, ptr_ty(ctx, 3));
+        atomPtr = b.bitcast(atomPtr, ptr_ty(ctx, 3));
         // Only threads with mask = True store the result
         PTXBuilder ptxBuilderStore;
         auto *dstOprStore = ptxBuilderStore.newAddrOperand(atomPtr, "r");
@@ -620,7 +626,7 @@ struct AtomicCASOpConversion
         auto ASMReturnTy = void_ty(ctx);
         ptxBuilderStore.launch(rewriter, loc, ASMReturnTy);
         createBarrier(rewriter, loc, numCTAs);
-        Value ret = load(valueElemTy, atomPtr);
+        Value ret = b.load(valueElemTy, atomPtr);
         rewriter.replaceOp(op, {ret});
       }
     }
@@ -661,6 +667,7 @@ struct AtomicRMWOpConversion
   matchAndRewrite(triton::AtomicRMWOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
     auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     MLIRContext *ctx = rewriter.getContext();
 
     auto moduleOp = op->getParentOfType<ModuleOp>();
@@ -727,7 +734,7 @@ struct AtomicRMWOpConversion
     SmallVector<Value> resultVals(elemsPerThread);
     for (size_t i = 0; i < elemsPerThread; i += vec * packed) {
       Value rmwPtr = ptrElements[i];
-      Value rmwMask = llMask ? and_(mask, maskElements[i]) : mask;
+      Value rmwMask = llMask ? b.and_(mask, maskElements[i]) : mask;
       std::string sTy;
       PTXBuilder ptxBuilderAtomicRMW;
       // 16-bit -> "h", 32-bit -> "r", 64-bit -> "l"
@@ -755,10 +762,10 @@ struct AtomicRMWOpConversion
               ptxBuilderAtomicRMW.newOperand(valElements[i + ii], tyId));
         }
       } else if (packed > 1) {
-        Value rmwVal = undef(packedTy);
+        Value rmwVal = b.undef(packedTy);
         for (int ii = 0; ii < packed; ++ii) {
-          rmwVal = insert_element(packedTy, rmwVal, valElements[i + ii],
-                                  i32_val(ii));
+          rmwVal = b.insert_element(packedTy, rmwVal, valElements[i + ii],
+                                    b.i32_val(ii));
         }
         valOpr = ptxBuilderAtomicRMW.newOperand(rmwVal, tyId);
       } else {
@@ -828,11 +835,12 @@ struct AtomicRMWOpConversion
 
         if (vec > 1) {
           for (unsigned ii = 0; ii < vec; ++ii) {
-            resultVals[i + ii] = extract_val(valueElemTy, ret, ii);
+            resultVals[i + ii] = b.extract_val(valueElemTy, ret, ii);
           }
         } else if (packed > 1) {
           for (unsigned ii = 0; ii < packed; ++ii) {
-            resultVals[i + ii] = extract_element(valueElemTy, ret, i32_val(ii));
+            resultVals[i + ii] =
+                b.extract_element(valueElemTy, ret, b.i32_val(ii));
           }
         } else {
           resultVals[i] = ret;
@@ -848,7 +856,7 @@ struct AtomicRMWOpConversion
         }
         Value atomPtr = LLVM::getSharedMemoryBase(loc, rewriter, targetInfo,
                                                   op.getOperation());
-        atomPtr = bitcast(atomPtr, ptr_ty(ctx, 3));
+        atomPtr = b.bitcast(atomPtr, ptr_ty(ctx, 3));
         // Only threads with rmwMask = True store the result
         PTXBuilder ptxBuilderStore;
         auto &storeShared =
@@ -858,7 +866,7 @@ struct AtomicRMWOpConversion
         storeShared(ptrOpr, valOpr).predicate(rmwMask);
         ptxBuilderStore.launch(rewriter, loc, void_ty(ctx));
         createBarrier(rewriter, loc, numCTAs);
-        Value ret = load(valueElemTy, atomPtr);
+        Value ret = b.load(valueElemTy, atomPtr);
         rewriter.replaceOp(op, {ret});
       }
     }
@@ -886,6 +894,7 @@ struct AsyncCopyGlobalToLocalOpConversion
   matchAndRewrite(triton::gpu::AsyncCopyGlobalToLocalOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
     auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     Value res = op.getResult();
     Value mask = op.getMask();
     Value other = op.getOther();
@@ -989,7 +998,7 @@ struct AsyncCopyGlobalToLocalOpConversion
           // remaining slots with 0 if cp-size > src-size.
           // XXX(Keren): Always assume other = 0 for now.
           auto selectOp =
-              select(maskElems[elemIdx], i32_val(wordBytes), i32_val(0));
+              b.select(maskElems[elemIdx], b.i32_val(wordBytes), b.i32_val(0));
           srcSize = ptxBuilder.newOperand(selectOp, "r");
         }
 
@@ -1034,6 +1043,7 @@ struct AsyncTMACopyGlobalToLocalOpConversion
     assert(op.getEvict() == triton::EvictionPolicy::NORMAL &&
            "eviction policy not supported yet.");
     auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     Type llvmElemTy =
         typeConverter->convertType(op.getResult().getType().getElementType());
     auto barrierMemObj = LLVM::getSharedMemoryObjectFromStruct(
@@ -1048,12 +1058,12 @@ struct AsyncTMACopyGlobalToLocalOpConversion
     auto mod = op->getParentOfType<ModuleOp>();
     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);
     int warpSize = triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);
-    Value warpID = udiv(id, i32_val(warpSize));
+    Value warpID = b.udiv(id, b.i32_val(warpSize));
     warpID = LLVM::NVIDIA::shuffleIdx(loc, rewriter, warpID, 0);
     Value pred = adaptor.getPred();
     // Select just one thread for the TMA copy. This also helps the compiler to
     // figure out that the op is uniform.
-    pred = and_(pred, LLVM::NVIDIA::createElectPredicate(loc, rewriter));
+    pred = b.and_(pred, LLVM::NVIDIA::createElectPredicate(loc, rewriter));
 
     int elementSizeInBytes =
         op.getResult().getType().getElementType().getIntOrFloatBitWidth() / 8;
@@ -1075,16 +1085,16 @@ struct AsyncTMACopyGlobalToLocalOpConversion
     for (int copyIdx = 0; copyIdx < numCopies; copyIdx += numWarps) {
       int numWarpsToCopy = std::min(numCopies - copyIdx, numWarps);
       if (numWarpsToCopy == 1)
-        warpID = i32_val(0);
+        warpID = b.i32_val(0);
       Value boxPred =
-          and_(pred, icmp_ult(id, i32_val(numWarpsToCopy * warpSize)));
+          b.and_(pred, b.icmp_ult(id, b.i32_val(numWarpsToCopy * warpSize)));
       ::mlir::triton::PTXBuilder ptxBuilderTMA;
       Type elemPtrTy = ptr_ty(rewriter.getContext(), 3);
-      Value copyIdxVal = add(warpID, i32_val(copyIdx));
+      Value copyIdxVal = b.add(warpID, b.i32_val(copyIdx));
       Value shMemOffset =
-          mul(copyIdxVal, i32_val(totalNumElements / numCopies));
+          b.mul(copyIdxVal, b.i32_val(totalNumElements / numCopies));
       Value shMemPtr =
-          gep(elemPtrTy, llvmElemTy, dstMemObj.getBase(), shMemOffset);
+          b.gep(elemPtrTy, llvmElemTy, dstMemObj.getBase(), shMemOffset);
       SmallVector<PTXBuilder::Operand *> operands = {
           ptxBuilderTMA.newOperand(boxPred, "b"),
           ptxBuilderTMA.newOperand(shMemPtr, "r"),
@@ -1096,8 +1106,8 @@ struct AsyncTMACopyGlobalToLocalOpConversion
       for (int i = 0; i < rank; i++) {
         Value coord = adaptor.getCoord()[rank - i - 1];
         if (i == 0) {
-          Value offset = mul(copyIdxVal, i32_val(128 / elementSizeInBytes));
-          coord = add(coord, offset);
+          Value offset = b.mul(copyIdxVal, b.i32_val(128 / elementSizeInBytes));
+          coord = b.add(coord, offset);
         }
         operands.push_back(ptxBuilderTMA.newOperand(coord, "r"));
         tmaInst += "$" + std::to_string(operandIdx++);
@@ -1126,6 +1136,7 @@ struct AsyncTMACopyLocalToGlobalOpConversion
                   OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
     auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     Type llvmElemTy =
         typeConverter->convertType(op.getSrc().getType().getElementType());
     auto dstMemObj = LLVM::getSharedMemoryObjectFromStruct(
@@ -1143,7 +1154,7 @@ struct AsyncTMACopyLocalToGlobalOpConversion
     auto mod = op->getParentOfType<ModuleOp>();
     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);
     int warpSize = triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);
-    Value warpID = udiv(id, i32_val(warpSize));
+    Value warpID = b.udiv(id, b.i32_val(warpSize));
     warpID = LLVM::NVIDIA::shuffleIdx(loc, rewriter, warpID, 0);
     int innerBlockSize = op.getSrc().getType().getShape().back();
     int contigDimSizeInByte = innerBlockSize * elementSizeInBytes;
@@ -1160,16 +1171,16 @@ struct AsyncTMACopyLocalToGlobalOpConversion
     for (int copyIdx = 0; copyIdx < numCopies; copyIdx += numWarps) {
       int numWarpsToCopy = std::min(numCopies - copyIdx, numWarps);
       if (numWarpsToCopy == 1)
-        warpID = i32_val(0);
+        warpID = b.i32_val(0);
       Value boxPred =
-          and_(pred, icmp_ult(id, i32_val(numWarpsToCopy * warpSize)));
+          b.and_(pred, b.icmp_ult(id, b.i32_val(numWarpsToCopy * warpSize)));
       ::mlir::triton::PTXBuilder ptxBuilderTMA;
       Type elemPtrTy = ptr_ty(rewriter.getContext(), 3);
-      Value copyIdxVal = add(warpID, i32_val(copyIdx));
+      Value copyIdxVal = b.add(warpID, b.i32_val(copyIdx));
       Value shMemOffset =
-          mul(copyIdxVal, i32_val(totalNumElements / numCopies));
+          b.mul(copyIdxVal, b.i32_val(totalNumElements / numCopies));
       Value shMemPtr =
-          gep(elemPtrTy, llvmElemTy, dstMemObj.getBase(), shMemOffset);
+          b.gep(elemPtrTy, llvmElemTy, dstMemObj.getBase(), shMemOffset);
       SmallVector<PTXBuilder::Operand *> operands = {
           ptxBuilderTMA.newOperand(boxPred, "b"),
           ptxBuilderTMA.newOperand(adaptor.getDescPtr(), "l")};
@@ -1179,8 +1190,8 @@ struct AsyncTMACopyLocalToGlobalOpConversion
       for (int i = 0; i < rank; i++) {
         Value coord = adaptor.getCoord()[rank - i - 1];
         if (i == 0) {
-          Value offset = mul(copyIdxVal, i32_val(128 / elementSizeInBytes));
-          coord = add(coord, offset);
+          Value offset = b.mul(copyIdxVal, b.i32_val(128 / elementSizeInBytes));
+          coord = b.add(coord, offset);
         }
         operands.push_back(ptxBuilderTMA.newOperand(coord, "r"));
         tmaInst += "$" + std::to_string(operandIdx++);
diff --git a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/TMAToLLVM.cpp b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/TMAToLLVM.cpp
index c64ba1915..fc6178b2a 100644
--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/TMAToLLVM.cpp
+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/TMAToLLVM.cpp
@@ -20,6 +20,7 @@ void tensormap_cp_fenceproxy(Location loc, MLIRContext *ctx,
                              ConversionPatternRewriter &rewriter, Value outPtr,
                              Value inPtr) {
   PTXBuilder ptxBuilder;
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
 
   // prepare asm operands
   auto *outAddrOpr = ptxBuilder.newAddrOperand(outPtr, "l");
@@ -34,7 +35,7 @@ void tensormap_cp_fenceproxy(Location loc, MLIRContext *ctx,
   // Execute collectively on first warp in block
   constexpr int kWarpSize = 32;
   Value threadId = getThreadId(rewriter, loc);
-  Value pred = icmp_slt(threadId, i32_val(kWarpSize));
+  Value pred = b.icmp_slt(threadId, b.i32_val(kWarpSize));
   cp(outAddrOpr, inAddrOpr, sizeOpr).predicate(pred);
 
   ptxBuilder.launch(rewriter, loc, void_ty(ctx));
@@ -45,6 +46,7 @@ void tensormap_replace_generic(Location loc, MLIRContext *ctx,
                                std::string fieldName, Value descPtr,
                                int32_t newVal) {
   PTXBuilder ptxBuilder;
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
 
   // prepare asm operands
   auto *descAddrOpr = ptxBuilder.newAddrOperand(descPtr, "l");
@@ -58,7 +60,7 @@ void tensormap_replace_generic(Location loc, MLIRContext *ctx,
                       .o("b32");
 
   Value threadId = getThreadId(rewriter, loc);
-  Value pred = icmp_eq(threadId, i32_val(0));
+  Value pred = b.icmp_eq(threadId, b.i32_val(0));
   replace(descAddrOpr, newValOpr).predicate(pred);
 
   ptxBuilder.launch(rewriter, loc, void_ty(ctx));
@@ -70,6 +72,7 @@ void tensormap_replace_generic(Location loc, MLIRContext *ctx,
                                Value newVal,
                                std::optional<int32_t> ord = std::nullopt) {
   PTXBuilder ptxBuilder;
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
 
   auto newValTy = newVal.getType();
   int width = 0;
@@ -97,7 +100,7 @@ void tensormap_replace_generic(Location loc, MLIRContext *ctx,
                       .o("b64", width == 64);
 
   Value threadId = getThreadId(rewriter, loc);
-  Value pred = icmp_eq(threadId, i32_val(0));
+  Value pred = b.icmp_eq(threadId, b.i32_val(0));
 
   if (ord) {
     replace(descAddrOpr, ordOpr, newValOpr).predicate(pred);
@@ -188,6 +191,7 @@ struct ExperimentalTensormapFenceproxyAcquireOpConversion
 
     auto loc = op.getLoc();
     PTXBuilder ptxBuilder;
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
 
     // prepare asm operands
     auto *descAddrOpr = ptxBuilder.newAddrOperand(adaptor.getDescPtr(), "l");
@@ -196,7 +200,7 @@ struct ExperimentalTensormapFenceproxyAcquireOpConversion
     // Define the instruction opcode
     constexpr int kWarpSize = 32;
     Value threadId = getThreadId(rewriter, loc);
-    Value pred = icmp_slt(threadId, i32_val(kWarpSize));
+    Value pred = b.icmp_slt(threadId, b.i32_val(kWarpSize));
     auto &fence =
         *ptxBuilder.create<>("fence.proxy.tensormap::generic.acquire.gpu");
     fence(descAddrOpr, sizeOpr).predicate(pred);
@@ -216,13 +220,15 @@ struct ExperimentalTensormapFenceproxyAcquireOpConversion
 void zero_fill_tma(Location loc, MLIRContext *ctx,
                    ConversionPatternRewriter &rewriter,
                    const NVIDIA::TargetInfo &targetInfo, Value descPtr) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   // Write out zeros
   constexpr int kWarpSize = 32;
   Value threadId = getThreadId(rewriter, loc);
-  Value pred = icmp_slt(threadId, i32_val(kWarpSize));
+  Value pred = b.icmp_slt(threadId, b.i32_val(kWarpSize));
 
-  auto fillVal = i32_val(0);
-  auto writeAddr = gep(descPtr.getType(), fillVal.getType(), descPtr, threadId);
+  auto fillVal = b.i32_val(0);
+  auto writeAddr =
+      b.gep(descPtr.getType(), fillVal.getType(), descPtr, threadId);
   targetInfo.storeShared(rewriter, loc, writeAddr, fillVal, pred);
 
   // Sync warp
@@ -246,6 +252,7 @@ struct ExperimentalTensormapCreateOpConversion
   matchAndRewrite(triton::ExperimentalTensormapCreateOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
     Location loc = op->getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto ctx = getContext();
 
     auto smemBase = LLVM::getSharedMemoryBase(loc, rewriter, targetInfo, op);
diff --git a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/TargetInfo.cpp b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/TargetInfo.cpp
index 75f935410..a7aef8666 100644
--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/TargetInfo.cpp
+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/TargetInfo.cpp
@@ -43,19 +43,20 @@ std::pair<Type, Value> printfPromoteValue(RewriterBase &rewriter, Value value) {
   Value newOp = value;
   Type newType = type;
   auto loc = UnknownLoc::get(context);
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
 
   bool isUnsigned = type.isUnsignedInteger();
   if (type.isIntOrIndex() && type.getIntOrFloatBitWidth() < 32) {
     if (isUnsigned) {
       newType = ui32_ty;
-      newOp = zext(newType, value);
+      newOp = b.zext(newType, value);
     } else {
       newType = i32_ty;
-      newOp = sext(newType, value);
+      newOp = b.sext(newType, value);
     }
   } else if (type.isBF16() || type.isF16() || type.isF32()) {
     newType = f64_ty;
-    newOp = fpext(newType, value);
+    newOp = b.fpext(newType, value);
   }
 
   return {newType, newOp};
@@ -137,7 +138,8 @@ Value TargetInfo::getClusterCTAId(RewriterBase &rewriter, Location loc) const {
 
 Value TargetInfo::ballot(RewriterBase &rewriter, Location loc, Type type,
                          Value cmp) const {
-  Value threadMask = int_val(type.getIntOrFloatBitWidth(), -1);
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  Value threadMask = b.int_val(type.getIntOrFloatBitWidth(), -1);
   return rewriter.create<NVVM::VoteBallotOp>(loc, type, threadMask, cmp);
 }
 
@@ -175,6 +177,7 @@ static bool isConstantTruePred(Value pred) {
 void TargetInfo::storeDShared(RewriterBase &rewriter, Location loc, Value ptr,
                               std::optional<Value> ctaId, Value val,
                               Value pred) const {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   MLIRContext *ctx = rewriter.getContext();
   auto ptrTy = cast<LLVM::LLVMPointerType>(ptr.getType());
   assert(ptrTy.getAddressSpace() == 3 && "Invalid addr space for load_dsmem");
@@ -196,7 +199,7 @@ void TargetInfo::storeDShared(RewriterBase &rewriter, Location loc, Value ptr,
            "don't know how to load/store vectors of sub-byte elems");
     SmallVector<Value> vals = unpackLLVector(loc, val, rewriter);
     for (Value &v : vals) {
-      v = zext(int_ty(8), bitcast(v, int_ty(elemBitwidth)));
+      v = b.zext(int_ty(8), b.bitcast(v, int_ty(elemBitwidth)));
     }
     storeDShared(rewriter, loc, ptr, ctaId, packLLVector(loc, vals, rewriter),
                  pred);
@@ -206,7 +209,7 @@ void TargetInfo::storeDShared(RewriterBase &rewriter, Location loc, Value ptr,
   if (!elemTy.isInteger()) {
     SmallVector<Value> vals = unpackLLVector(loc, val, rewriter);
     for (Value &v : vals) {
-      v = bitcast(v, int_ty(elemBitwidth));
+      v = b.bitcast(v, int_ty(elemBitwidth));
     }
     storeDShared(rewriter, loc, ptr, ctaId, packLLVector(loc, vals, rewriter),
                  pred);
@@ -227,7 +230,7 @@ void TargetInfo::storeDShared(RewriterBase &rewriter, Location loc, Value ptr,
       Value v = packLLVector(
           loc, ArrayRef(oldVals).slice(i * elemsPerPack, elemsPerPack),
           rewriter);
-      newVals.push_back(bitcast(v, i32_ty));
+      newVals.push_back(b.bitcast(v, i32_ty));
     }
     storeDShared(rewriter, loc, ptr, ctaId,
                  packLLVector(loc, newVals, rewriter), pred);
@@ -242,8 +245,8 @@ void TargetInfo::storeDShared(RewriterBase &rewriter, Location loc, Value ptr,
     auto newVecTy = vec_ty(elemTy, maxVec);
     SmallVector<Value> vals = unpackLLVector(loc, val, rewriter);
     for (int i = 0; i < vec / maxVec; i++) {
-      auto newPtr = gep(ptr.getType(), elemTy, ptr, i32_val(i * maxVec),
-                        /*inbounds=*/true);
+      auto newPtr = b.gep(ptr.getType(), elemTy, ptr, b.i32_val(i * maxVec),
+                          /*inbounds=*/true);
       storeDShared(
           rewriter, loc, newPtr, ctaId,
           packLLVector(loc, ArrayRef(vals).slice(i * maxVec, maxVec), rewriter),
@@ -276,7 +279,7 @@ void TargetInfo::storeDShared(RewriterBase &rewriter, Location loc, Value ptr,
   if (vec > 1) {
     SmallVector<std::pair<Value, std::string>> vecVals;
     for (int i = 0; i < vec; i++) {
-      vecVals.push_back({extract_element(val, i32_val(i)), constraint});
+      vecVals.push_back({b.extract_element(val, b.i32_val(i)), constraint});
     }
     valOpr = builder.newListOperand(vecVals);
   } else {
@@ -289,6 +292,7 @@ void TargetInfo::storeDShared(RewriterBase &rewriter, Location loc, Value ptr,
 Value TargetInfo::loadDShared(RewriterBase &rewriter, Location loc, Value ptr,
                               std::optional<Value> ctaId, Type loadTy,
                               Value pred) const {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   MLIRContext *ctx = rewriter.getContext();
   auto ptrTy = cast<LLVM::LLVMPointerType>(ptr.getType());
   assert(ptrTy.getAddressSpace() == 3 && "Invalid addr space for load_dsmem");
@@ -313,7 +317,7 @@ Value TargetInfo::loadDShared(RewriterBase &rewriter, Location loc, Value ptr,
     SmallVector<Value> vals = unpackLLVector(
         loc, loadDShared(rewriter, loc, ptr, ctaId, int_ty(8), pred), rewriter);
     assert(vals.size() == 1);
-    return bitcast(trunc(int_ty(elemBitwidth), vals[0]), elemTy);
+    return b.bitcast(b.trunc(int_ty(elemBitwidth), vals[0]), elemTy);
   }
 
   // We only know how to load integers.
@@ -322,7 +326,7 @@ Value TargetInfo::loadDShared(RewriterBase &rewriter, Location loc, Value ptr,
     SmallVector<Value> vals = unpackLLVector(
         loc, loadDShared(rewriter, loc, ptr, ctaId, newLoadTy, pred), rewriter);
     for (Value &v : vals) {
-      v = bitcast(v, elemTy);
+      v = b.bitcast(v, elemTy);
     }
     return packLLVector(loc, vals, rewriter);
   }
@@ -339,7 +343,7 @@ Value TargetInfo::loadDShared(RewriterBase &rewriter, Location loc, Value ptr,
     // Unpack the b32's into the original vector type.
     SmallVector<Value> vals;
     for (Value v : unpackLLVector(loc, res, rewriter)) {
-      Value vv = bitcast(v, vec_ty(elemTy, 32 / elemBitwidth));
+      Value vv = b.bitcast(v, vec_ty(elemTy, 32 / elemBitwidth));
       for (Value vvv : unpackLLVector(loc, vv, rewriter)) {
         vals.push_back(vvv);
       }
@@ -354,8 +358,8 @@ Value TargetInfo::loadDShared(RewriterBase &rewriter, Location loc, Value ptr,
 
     SmallVector<Value> vals;
     for (int i = 0; i < vec / maxVec; i++) {
-      auto newPtr = gep(ptr.getType(), elemTy, ptr, i32_val(i * maxVec),
-                        /*inbounds=*/true);
+      auto newPtr = b.gep(ptr.getType(), elemTy, ptr, b.i32_val(i * maxVec),
+                          /*inbounds=*/true);
       auto newVal = loadDShared(rewriter, loc, newPtr, ctaId,
                                 vec_ty(elemTy, maxVec), pred);
       for (Value v : unpackLLVector(loc, newVal, rewriter)) {
@@ -387,13 +391,13 @@ Value TargetInfo::loadDShared(RewriterBase &rewriter, Location loc, Value ptr,
   if (isConstantTruePred(pred)) {
     Type resultTy = vec == 1 ? Type(int_ty(elemBitwidth))
                              : Type(vec_ty(int_ty(elemBitwidth), vec));
-    load = load(resultTy, ptr);
+    load = b.load(resultTy, ptr);
     if (vec > 1) {
       Type structTy = struct_ty(SmallVector<Type>(vec, int_ty(elemBitwidth)));
-      Value structValue = undef(structTy);
+      Value structValue = b.undef(structTy);
       for (int i = 0; i < vec; i++) {
-        structValue = insert_val(structTy, structValue,
-                                 extract_element(load, i32_val(i)), i);
+        structValue = b.insert_val(structTy, structValue,
+                                   b.extract_element(load, b.i32_val(i)), i);
       }
       load = structValue;
     }
@@ -441,13 +445,14 @@ bool TargetInfo::warpReduce(RewriterBase &rewriter, Location loc,
                             SmallVector<Value> &acc, triton::ReduceOp op,
                             unsigned numLaneToReduce,
                             unsigned interleave) const {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   if (auto kind = matchReduxKind(op, computeCapability)) {
     // Based on benchmarking on A100 redux op gives a speed up only when doing
     // a single reduction (not partitioned) and when the mask is static.
     // Therefore we currently only enable it to reduce across all the lanes.
     if (numLaneToReduce == 32) {
       assert(acc.size() == 1);
-      Value mask = i32_val(0xFFFFFFFF);
+      Value mask = b.i32_val(0xFFFFFFFF);
       // Even though we currently don't use redux for partitioned reduction
       // the code below supports it in case we want to tweak the heuristic.
       if (numLaneToReduce < 32) {
@@ -455,22 +460,22 @@ bool TargetInfo::warpReduce(RewriterBase &rewriter, Location loc,
         // each group of numLaneToReduce threads has the correct mask.
         unsigned bitmask = (1 << numLaneToReduce) - 1;
         Value threadId = getThreadId(rewriter, loc);
-        Value laneId = urem(threadId, i32_val(32));
-        mask = shl(i32_val(bitmask),
-                   and_(laneId, i32_val(~(numLaneToReduce - 1))));
+        Value laneId = b.urem(threadId, b.i32_val(32));
+        mask = b.shl(b.i32_val(bitmask),
+                     b.and_(laneId, b.i32_val(~(numLaneToReduce - 1))));
       }
       for (unsigned i = 0; i < acc.size(); ++i) {
         unsigned bitwidth = cast<IntegerType>(acc[i].getType()).getWidth();
         if (bitwidth < 32) {
           if (*kind == NVVM::ReduxKind::MIN || *kind == NVVM::ReduxKind::MAX)
-            acc[i] = sext(i32_ty, acc[i]);
+            acc[i] = b.sext(i32_ty, acc[i]);
           else
-            acc[i] = zext(i32_ty, acc[i]);
+            acc[i] = b.zext(i32_ty, acc[i]);
         }
         acc[i] = rewriter.create<NVVM::ReduxOp>(loc, acc[i].getType(), acc[0],
                                                 *kind, mask);
         if (bitwidth < 32)
-          acc[i] = trunc(int_ty(bitwidth), acc[i]);
+          acc[i] = b.trunc(int_ty(bitwidth), acc[i]);
       }
       return true;
     }
@@ -480,6 +485,7 @@ bool TargetInfo::warpReduce(RewriterBase &rewriter, Location loc,
 
 void TargetInfo::storeMatrixShared(RewriterBase &rewriter, Location loc,
                                    Value ptr, Value val) const {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto vals = unpackLLVector(loc, val, rewriter);
   // Ensure input consists of 4 vectors, each holding 2 elements of 16 bits
   assert(vals[0].getType().getIntOrFloatBitWidth() == 16 &&
@@ -489,11 +495,11 @@ void TargetInfo::storeMatrixShared(RewriterBase &rewriter, Location loc,
   Type packedTy = vec_ty(vals[0].getType(), 2);
   SmallVector<Value> inputs;
   for (int i = 0; i < 4; i++) {
-    Value input = undef(packedTy);
+    Value input = b.undef(packedTy);
     for (int j = 0; j < 2; j++) {
-      input = insert_element(packedTy, input, vals[i * 2 + j], i32_val(j));
+      input = b.insert_element(packedTy, input, vals[i * 2 + j], b.i32_val(j));
     }
-    inputs.push_back(bitcast(input, i32_ty));
+    inputs.push_back(b.bitcast(input, i32_ty));
   }
   rewriter.create<triton::nvgpu::StoreMatrixOp>(loc, ptr, inputs);
 }
@@ -511,11 +517,12 @@ void TargetInfo::printf(RewriterBase &rewriter, Value formatStrStart,
   auto moduleOp = rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();
   auto funcOp = getVprintfDeclaration(rewriter);
   auto loc = UnknownLoc::get(ctx);
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
 
-  Value one = i32_val(1);
-  Value zero = i32_val(0);
+  Value one = b.i32_val(1);
+  Value zero = b.i32_val(0);
 
-  Value bufferPtr = null(ptr);
+  Value bufferPtr = b.null(ptr);
 
   SmallVector<Value, 16> newArgs;
   if (args.size() >= 1) {
@@ -534,16 +541,16 @@ void TargetInfo::printf(RewriterBase &rewriter, Value formatStrStart,
                                         /*alignment=*/0);
 
     for (const auto &entry : llvm::enumerate(newArgs)) {
-      auto index = i32_val(entry.index());
+      auto index = b.i32_val(entry.index());
       auto fieldPtr =
-          gep(ptr_ty(ctx), structTy, allocated, ArrayRef<Value>{zero, index});
-      store(entry.value(), fieldPtr);
+          b.gep(ptr_ty(ctx), structTy, allocated, ArrayRef<Value>{zero, index});
+      b.store(entry.value(), fieldPtr);
     }
-    bufferPtr = bitcast(allocated, ptr);
+    bufferPtr = b.bitcast(allocated, ptr);
   }
 
   SmallVector<Value> operands{formatStrStart, bufferPtr};
-  call(funcOp, operands);
+  b.call(funcOp, operands);
 }
 
 void TargetInfo::printf(RewriterBase &rewriter, StringRef msg,
@@ -561,6 +568,7 @@ void TargetInfo::printf(RewriterBase &rewriter, StringRef msg,
 void TargetInfo::assertFail(RewriterBase &rewriter, Location loc,
                             StringRef message, StringRef file, StringRef func,
                             int line) const {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   auto funcOp = getAssertfailDeclaration(rewriter);
   auto moduleOp = rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();
   llvm::SmallString<64> messageString(message), fileString(file),
@@ -574,11 +582,11 @@ void TargetInfo::assertFail(RewriterBase &rewriter, Location loc,
       LLVM::addStringToModule(loc, rewriter, "assertFile_", fileString);
   Value funcStringVal =
       LLVM::addStringToModule(loc, rewriter, "assertFunc_", funcString);
-  Value lineNumber = i32_val(line);
-  Value charSize = int_val(sizeof(size_t) * 8, sizeof(char));
+  Value lineNumber = b.i32_val(line);
+  Value charSize = b.int_val(sizeof(size_t) * 8, sizeof(char));
   SmallVector<Value> operands = {messageStringVal, fileStringVal, lineNumber,
                                  funcStringVal, charSize};
-  call(funcOp, operands);
+  b.call(funcOp, operands);
 }
 
 int TargetInfo::getSharedAddressSpace() const { return 3; }
diff --git a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/TensorPtrOpsToLLVM.cpp b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/TensorPtrOpsToLLVM.cpp
index c117eb176..97e830967 100644
--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/TensorPtrOpsToLLVM.cpp
+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/TensorPtrOpsToLLVM.cpp
@@ -70,6 +70,7 @@ struct AdvanceOpConversion : public ConvertOpToLLVMPattern<triton::AdvanceOp> {
     // struct { offset0, offset1, shape0, shape1, stride0,
     // stride1, base_ptr};
     auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto ptrType = op.getPtr().getType();
     auto tensorPtr = adaptor.getPtr();
 
@@ -79,7 +80,7 @@ struct AdvanceOpConversion : public ConvertOpToLLVMPattern<triton::AdvanceOp> {
     SmallVector<Value, 2> newOffsets;
 
     for (auto [offset, oldOffset] : llvm::zip_first(offsets, elems)) {
-      newOffsets.push_back((add(offset, oldOffset)));
+      newOffsets.push_back((b.add(offset, oldOffset)));
     }
 
     for (size_t i = 0; i < newOffsets.size(); ++i) {
diff --git a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/UpcastMXFPToLLVM.cpp b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/UpcastMXFPToLLVM.cpp
index 722bf56cd..814dd8c44 100644
--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/UpcastMXFPToLLVM.cpp
+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/UpcastMXFPToLLVM.cpp
@@ -33,38 +33,39 @@ public:
   llvm::SmallVector<Value>
   unpackFP4Elements(Location loc, ConversionPatternRewriter &rewriter,
                     const llvm::SmallVector<Value> &vals, Value laneId) const {
-    auto fp4x2ToBf16x2 = [&loc, &rewriter](Value v) -> Value {
-      auto em0 = and_(v, i8_val(0x70));
-      auto em1 = and_(v, i8_val(0x7));
-      Value v0 = or_(shl(zext(i16_ty, em0), i16_val(2)),
-                     shl(zext(i16_ty, and_(v, i8_val(0x80))), i16_val(8)));
-      Value v1 = or_(shl(zext(i16_ty, em1), i16_val(6)),
-                     shl(zext(i16_ty, and_(v, i8_val(0x8))), i16_val(12)));
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
+    auto fp4x2ToBf16x2 = [&b, &loc, &rewriter](Value v) -> Value {
+      auto em0 = b.and_(v, b.i8_val(0x70));
+      auto em1 = b.and_(v, b.i8_val(0x7));
+      Value v0 = b.or_(b.shl(b.zext(i16_ty, em0), b.i16_val(2)),
+                       b.shl(b.zext(i16_ty, b.and_(v, b.i8_val(0x80))), b.i16_val(8)));
+      Value v1 = b.or_(b.shl(b.zext(i16_ty, em1), b.i16_val(6)),
+                       b.shl(b.zext(i16_ty, b.and_(v, b.i8_val(0x8))), b.i16_val(12)));
 
       // Three cases:
       // 1) x is normal and non-zero: Correct bias
-      v0 = select(icmp_ne(and_(em0, i8_val(0x60)), i8_val(0)),
-                  add(v0, i16_val((127 - 1) << 7)), v0);
-      v1 = select(icmp_ne(and_(em1, i8_val(0x6)), i8_val(0)),
-                  add(v1, i16_val((127 - 1) << 7)), v1);
+      v0 = b.select(b.icmp_ne(b.and_(em0, b.i8_val(0x60)), b.i8_val(0)),
+                    b.add(v0, b.i16_val((127 - 1) << 7)), v0);
+      v1 = b.select(b.icmp_ne(b.and_(em1, b.i8_val(0x6)), b.i8_val(0)),
+                    b.add(v1, b.i16_val((127 - 1) << 7)), v1);
 
       // 2) x is subnormal (x == 0bs001 where s is the sign): Map to +-0.5 in
       // bf16
-      v0 = select(icmp_eq(em0, i8_val(0x10)),
-                  or_(i16_val(16128), and_(v0, i16_val(0x8000))), v0);
-      v1 = select(icmp_eq(em1, i8_val(0x1)),
-                  or_(i16_val(16128), and_(v1, i16_val(0x8000))), v1);
+      v0 = b.select(b.icmp_eq(em0, b.i8_val(0x10)),
+                    b.or_(b.i16_val(16128), b.and_(v0, b.i16_val(0x8000))), v0);
+      v1 = b.select(b.icmp_eq(em1, b.i8_val(0x1)),
+                    b.or_(b.i16_val(16128), b.and_(v1, b.i16_val(0x8000))), v1);
       // 3) x is zero, nothing to do
 
       // Swap as they come packed in big endian
-      return or_(zext(i32_ty, v0), shl(zext(i32_ty, v1), i32_val(16)));
+      return b.or_(b.zext(i32_ty, v0), b.shl(b.zext(i32_ty, v1), b.i32_val(16)));
     };
 
-    auto fp4x8ToBf16x2 = [&loc, &rewriter, &fp4x2ToBf16x2](
+    auto fp4x8ToBf16x2 = [&b, &loc, &rewriter, &fp4x2ToBf16x2](
                              Value v) -> llvm::SmallVector<Value, 4> {
       llvm::SmallVector<Value, 4> results(4);
       for (int i = 0; i < 4; ++i) {
-        auto v_i = trunc(i8_ty, lshr(v, i32_val(8 * i)));
+        auto v_i = b.trunc(i8_ty, b.lshr(v, b.i32_val(8 * i)));
         results[i] = fp4x2ToBf16x2(v_i);
       }
       return results;
@@ -89,6 +90,7 @@ public:
                   ConversionPatternRewriter &rewriter) const override {
 
     auto loc = op.getLoc();
+    auto b = TritonLLVMOpBuilder(loc, rewriter);
     auto tyX = cast<RankedTensorType>(op->getOperandTypes()[0]);
     auto operands = adaptor.getOperands();
 
@@ -96,41 +98,41 @@ public:
     auto scaleVals = unpackLLElements(loc, operands[1], rewriter);
     auto fpType = op.getFpType();
 
-    Value tid = tid_val();
+    Value tid = b.tid_val();
     auto mod = op->getParentOfType<ModuleOp>();
     Value warpSize =
-        i32_val(triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod));
-    Value warpId = udiv(tid, warpSize);
-    Value laneId = urem(tid, warpSize);
+        b.i32_val(triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod));
+    Value warpId = b.udiv(tid, warpSize);
+    Value laneId = b.urem(tid, warpSize);
 
     if (fpType == F8F6F4Type::E2M1) {
       xVals = unpackFP4Elements(loc, rewriter, xVals, laneId);
     }
 
-    auto scaleBf16x2 = [&loc, &rewriter](Value v, Value s) -> Value {
+    auto scaleBf16x2 = [&b, &loc, &rewriter](Value v, Value s) -> Value {
       // Split bf16x2 into 2 bf16, scale each of them, and pack them back
       // TODO Is it true that the bfloats are always packed as bf16x2?
-      auto bf16_0 = bitcast(trunc(i16_ty, v), bf16_ty);
-      auto bf16_1 = bitcast(trunc(i16_ty, lshr(v, i32_val(16))), bf16_ty);
-      auto scaleIsNan = icmp_eq(s, i8_val(0xff));
-      auto scaleBf16 = bitcast(shl(zext(i16_ty, s), i16_val(7)), bf16_ty);
-      auto scaledBf16_0 = fmul(bf16_0, scaleBf16);
-      auto scaledBf16_1 = fmul(bf16_1, scaleBf16);
-      auto i16_0 = bitcast(scaledBf16_0, i16_ty);
-      auto i16_1 = bitcast(scaledBf16_1, i16_ty);
+      auto bf16_0 = b.bitcast(b.trunc(i16_ty, v), bf16_ty);
+      auto bf16_1 = b.bitcast(b.trunc(i16_ty, b.lshr(v, b.i32_val(16))), bf16_ty);
+      auto scaleIsNan = b.icmp_eq(s, b.i8_val(0xff));
+      auto scaleBf16 = b.bitcast(b.shl(b.zext(i16_ty, s), b.i16_val(7)), bf16_ty);
+      auto scaledBf16_0 = b.fmul(bf16_0, scaleBf16);
+      auto scaledBf16_1 = b.fmul(bf16_1, scaleBf16);
+      auto i16_0 = b.bitcast(scaledBf16_0, i16_ty);
+      auto i16_1 = b.bitcast(scaledBf16_1, i16_ty);
       auto packed =
-          or_(zext(i32_ty, i16_0), shl(zext(i32_ty, i16_1), i32_val(16)));
+          b.or_(b.zext(i32_ty, i16_0), b.shl(b.zext(i32_ty, i16_1), b.i32_val(16)));
       // Account for NaN in the scale as per the mxfp specification
-      auto packed_nan = select(scaleIsNan, i32_val(0x7fff7fff), packed);
+      auto packed_nan = b.select(scaleIsNan, b.i32_val(0x7fff7fff), packed);
       return packed_nan;
     };
 
     // Each thread owns elements of 4 mxfp vectors so we need 4 scales
     // Letting c = tid / 4 * 2, we need the elements from threads c, c + 1, c +
     // 16, c + 17
-    auto c = mul(udiv(laneId, i32_val(4)), i32_val(2));
-    std::array<Value, 4> ci = {c, add(c, i32_val(1)), add(c, i32_val(16)),
-                               add(c, i32_val(17))};
+    auto c = b.mul(b.udiv(laneId, b.i32_val(4)), b.i32_val(2));
+    std::array<Value, 4> ci = {c, b.add(c, b.i32_val(1)), b.add(c, b.i32_val(16)),
+                               b.add(c, b.i32_val(17))};
 
     for (auto [i, scaleVal] : llvm::enumerate(scaleVals)) {
       // column major as per the DotOperandEncoding(opidx=0) layout
diff --git a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/Utility.cpp b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/Utility.cpp
index 4963a13b7..13b21e962 100644
--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/Utility.cpp
+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/Utility.cpp
@@ -10,54 +10,59 @@ using namespace mlir::triton;
 
 static Value shuffleCommon(Location loc, RewriterBase &rewriter, Value val,
                            Value i, NVVM::ShflKind mode, Value clamp) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   unsigned bits = val.getType().getIntOrFloatBitWidth();
 
   if (bits == 64) {
     Type vecTy = vec_ty(f32_ty, 2);
-    Value vec = bitcast(val, vecTy);
-    Value val0 = extract_element(f32_ty, vec, i32_val(0));
-    Value val1 = extract_element(f32_ty, vec, i32_val(1));
+    Value vec = b.bitcast(val, vecTy);
+    Value val0 = b.extract_element(f32_ty, vec, b.i32_val(0));
+    Value val1 = b.extract_element(f32_ty, vec, b.i32_val(1));
     val0 = shuffleCommon(loc, rewriter, val0, i, mode, clamp);
     val1 = shuffleCommon(loc, rewriter, val1, i, mode, clamp);
-    vec = undef(vecTy);
-    vec = insert_element(vecTy, vec, val0, i32_val(0));
-    vec = insert_element(vecTy, vec, val1, i32_val(1));
-    return bitcast(vec, val.getType());
+    vec = b.undef(vecTy);
+    vec = b.insert_element(vecTy, vec, val0, b.i32_val(0));
+    vec = b.insert_element(vecTy, vec, val1, b.i32_val(1));
+    return b.bitcast(vec, val.getType());
   }
   Type type = val.getType();
   if (type != i32_ty) {
-    val = bitcast(val, int_ty(bits));
+    val = b.bitcast(val, int_ty(bits));
     if (bits < 32)
-      val = zext(i32_ty, val);
+      val = b.zext(i32_ty, val);
   }
-  Value mask = i32_val(0xFFFFFFFF);
+  Value mask = b.i32_val(0xFFFFFFFF);
   Value result = rewriter.create<NVVM::ShflOp>(loc, i32_ty, mask, val, i, clamp,
                                                mode, UnitAttr());
   if (type != i32_ty) {
     if (bits < 32)
-      result = trunc(int_ty(bits), result);
-    result = bitcast(result, type);
+      result = b.trunc(int_ty(bits), result);
+    result = b.bitcast(result, type);
   }
   return result;
 }
 
 Value shuffleXor(Location loc, RewriterBase &rewriter, Value val, int i) {
-  return shuffleCommon(loc, rewriter, val, i32_val(i), NVVM::ShflKind::bfly,
-                       i32_val(0x1f));
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  return shuffleCommon(loc, rewriter, val, b.i32_val(i), NVVM::ShflKind::bfly,
+                       b.i32_val(0x1f));
 }
 
 Value shuffleUp(Location loc, RewriterBase &rewriter, Value val, int i) {
-  return shuffleCommon(loc, rewriter, val, i32_val(i), NVVM::ShflKind::up,
-                       i32_val(0x0));
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  return shuffleCommon(loc, rewriter, val, b.i32_val(i), NVVM::ShflKind::up,
+                       b.i32_val(0x0));
 }
 
 Value shuffleIdx(Location loc, RewriterBase &rewriter, Value val, int i) {
-  return shuffleIdx(loc, rewriter, val, i32_val(i));
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
+  return shuffleIdx(loc, rewriter, val, b.i32_val(i));
 }
 
 Value shuffleIdx(Location loc, RewriterBase &rewriter, Value val, Value i) {
+  auto b = TritonLLVMOpBuilder(loc, rewriter);
   return shuffleCommon(loc, rewriter, val, i, NVVM::ShflKind::idx,
-                       i32_val(0x1f));
+                       b.i32_val(0x1f));
 }
 
 Value llGetPid(Location loc, RewriterBase &rewriter, ModuleOp moduleOp,
