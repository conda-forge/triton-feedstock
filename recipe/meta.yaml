{% set version = "2.3.0" %}
# Triton don't pin their releases (https://github.com/triton-lang/triton/issues/3535).
# PyTorch build a package called "torchtriton" using a commit in pytorch/.ci/docker/ci_commit_pins. Since we need triton
# solely as a required dependency for pytorch's cuda variant at the moment, we'll do the same. NOTE that for v2.3.0, the
# commit in pytorch/.ci/docker/ci_commit_pins actually isn't part of the repo tree any more, because the triton
# maintainers force-pushed the release branch. The torch_commit_pin below is a commit on the release branch with the
# same comtent as the commit in the pytorch file. Obviously, this is all far from ideal, and needs to be managed with
# some care. Set torch_commit_pin to "None" for usual release builds.
{% set torch_commit_pin = "3f8d91bb17f6e7bc33dc995ae0860db89d351c7b" %}

package:
{% if torch_commit_pin != None %}
  name: torchtriton
{% else %}
  name: triton
{% endif %}
  version: {{ version }}

source:
{% if torch_commit_pin != None %}
  git_url: https://github.com/openai/triton.git
  git_rev: {{ torch_commit_pin }}
{% else %}
  url: https://github.com/openai/triton/archive/refs/tags/v{{ version }}.tar.gz
{% endif %}
  sha256: 19b6de0d0bcce86e973258d112169cd321b677146808bcc5fed7f69046775cfd
  patches:
    # These patches are for unvendoring CUDA compiler tools.
    # This isn't being done with v2.3.0 as PyTorch doesn't support CUDA v12.4 yet,
    # and our CUDAtoolkit v11.8 doesn't have the compiler tools.
    # Use these patches for PyTorch v2.4.0 and above.
    #- patches/0001-do-not-package-third_party-folder.patch
    #- patches/0005-Unvendor-third-party-libs.patch
    #- patches/0008-Search-for-libs-in-CONDA_PREFIX-instead-of-third_par.patch
    # These patches are for unvendoring LLVM.
    # The version of LLVM used for triton v2.3.0 is an unpinned commit on LLVM project's
    # main branch, so we can't use a conda package for this.
    # These patches can be used to help unvendoring if this changes.
    # - patches/0004-Avoid-using-outdated-FindLLVM.patch
    # - patches/0007-Fix-TableGen-issues.patch
    # This patch applies even when llvm is vendored-in.
    - patches/0009-unpack-llvm-within-env.patch

build:
  number: 0
  # Triton only currently supports linux, and is a GPU optimization tool.
  # We only have a linux-64 GPU builder at the moment.
  # It's primarily for PyTorch, and they only use it for linux-64/GPU.
  skip: true  # [not (linux and x86_64)]
  # the torch.compile feature in PyTorch isn't supported on python 3.12:
  # https://github.com/pytorch/pytorch/blob/97ff6cfd9c86c5c09d7ce775ab64ec5c99230f5d/test/test_transformers.py#L3418
  skip: true  # [py>=312]
  # Put the cuda version variable back into the build string when we unvendor CUDA.
  # For triton v2.3.0, CUDA v12.3 is vendored-in.
  #string: cuda{{ cuda_compiler_version | replace('.', '') }}py{{ CONDA_PY }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}
  string: cuda123py{{ CONDA_PY }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}

requirements:
  build:
    - {{ compiler('cxx') }}
    - make
    - cmake
    - ninja
    {% if torch_commit_pin %}
    - git
    {% endif %}
  host:
    - python
    - pybind11
    - pip
    - setuptools
    - wheel
  run:
    - python
    - filelock
    # Triton compiles cuda kernels so needs the compiler toolchain at runtime.
    #
    # {{ compiler('cuda') }} provides libdevice, ptxas, cuda.h, and the include path to cuda.h.
    # cuda-cuobjdump provides cuobjdump and nvdisasm.
    # Currently we keep CUDAtookit vendored-in (see patches section for reasoning), but this should be un-commented
    # when this changes.
    #
    #- {{ compiler('cuda') }}
    #- cuda-cuobjdump
    #
    #  gcc is required whether the cuda tools are vendored-in or not, to support CUDA compilation. (nvcc is called via
    #  gcc when it processes .cu files, which are c++ syntax extended by CUDA syntax).
    - {{ compiler('cxx') }}
    - zlib

# Note that PyTorch is a test dependency here, and Triton is a dependency of (the CUDA variant of) PyTorch.
# So, you need to build Triton without running the tests (`conda build --no-test`), then build PyTorch, then run these tests.
test:
  imports:
    - triton
    - triton._C.libtriton
  requires:
    - pip
    - pytest
    - scipy
    - pytorch={{ version }}=*cuda*
  source_files:
    - python/test
  commands:
    - pip check
    # Here is a list of current test failures and reasoning why they're ok:
    #
    # test_dummy_backend                    - looks like it's using CUDA instead of CPU backend for this test, for some reason. We don't need to use the CPU backend anyway.
    # IndexError: map::at errors            - known issue for T4 GPUs https://github.com/triton-lang/triton/issues/3787
    # out of resource: shared memory errors - fine, just platform resource is less than expected
    # test_print[device_print_large-int32]  - assert False - looks like a print output error, works fine for other data types, should be ok
    # test_compile_in_forked_subproc        - AssertionError: assert 1 == 0 - also an IndexError: map::at output (shown in the stderr output)
    #
    # In general, the more important tests are the PyTorch tests. This package only supports PyTorch. See text at the top of the recipe.
    #
    # the test_performance tests are broken for compute capability 7.x, which applies to our current build instances.
    - pytest -v python/test --ignore=python/test/regression/test_performance.py || true

about:
  home: https://github.com/openai/triton
  license: MIT
  license_family: MIT
  license_file: LICENSE
  summary: Development repository for the Triton language and compiler
  description: |
    This is the development repository of Triton, a language and compiler for writing highly efficient custom Deep-Learning primitives.
    The aim of Triton is to provide an open-source environment to write fast code at higher productivity than CUDA, but also with higher flexibility than other existing DSLs.
  doc_url: https://triton-lang.org/
  dev_url: https://github.com/openai/triton

extra:
  recipe-maintainers:
    - erip
    - h-vetinari
