context:
  version: 3.2.0
  # Triton no longer tags releases, but there are release branches, e.g.
  # https://github.com/triton-lang/triton/commits/release/3.1.x/
  # Check if the commit id from Pytorch's latest pinned commit in
  # https://github.com/pytorch/pytorch/blob/v{{ pytorch_ver }}/.ci/docker/ci_commit_pins/triton.txt
  # can be found on one of those release branches, and use that as the version
  git_commit: 35c6c7c6284582b3f41c71c150e11b517acf074a
  build_number: 0

package:
  name: triton
  version: ${{ version }}

source:
  url: https://github.com/triton-lang/triton/archive/${{ git_commit }}.tar.gz
  sha256: 4ae5d426b118f657e9f3adade363832b9f4aee7f4cb8f1e17abaf837b5e32880
  patches:
    - patches/0001-Remove-Werror-that-cause-false-positive-build-failur.patch
    - patches/0002-Do-not-link-directly-to-LLVM-static-libraries.patch
    - patches/0003-Use-system-PATH-to-find-tools-in-CONDA_PREFIX.patch
    # https://github.com/triton-lang/triton/commit/0591b3756bd4143b7163235c0eca4d718948e982
    - patches/0004-Don-t-specify-A-x64-option-and-reuse-cmake-build-typ.patch
    # https://github.com/conda-forge/pytorch-cpu-feedstock/issues/317#issuecomment-2585460619
    - patches/0005-Add-conda-forge-include-dirs-to-list-of-include-dirs.patch
    # backport https://github.com/triton-lang/triton/pull/5765
    - patches/0006-release-3.2.x-Get-proper-PTX-version-for-CUDA-12.6.patch
    # backport patches for lifting llvm/mlir compatibility to(wards) v20, see
    # https://github.com/triton-lang/triton/commits/main/cmake/llvm-hash.txt
    - patches/0007-BACKEND-Update-LLVM-hash-5040.patch
    - patches/0008-BACKEND-Update-llvm-to-llvm-llvm-project-fa57c7a6a5f.patch
    - patches/0009-Update-to-llvm-llvm-project-bd9145c8c213-5180.patch
    - patches/0010-LLVM-Update-to-llvm-project-86b69c3-5242.patch
    - patches/0011-BACKEND-Update-LLVM-version-to-2fe947b4-5540.patch
    - patches/0012-BACKEND-bump-llvm-to-2f7ade4b-5599.patch
    - patches/0013-BACKEND-Update-LLVM-version-to-https-github.com-llvm.patch
    - patches/0014-BACKEND-bump-to-llvm-llvm-project-c118864223c6-5684.patch

build:
  number: ${{ build_number }}
  string: cuda${{ cuda_compiler_version | version_to_buildstring }}py${{ python | version_to_buildstring }}h${{ hash }}_${{ build_number }}
  # TODO: CPU-only support still under development
  # No success enabling Windows build as of 3.1.0:
  # https://github.com/conda-forge/triton-feedstock/pull/29#issuecomment-2564371725
  skip: win or cuda_compiler_version in ("None", "11.8")

requirements:
  build:
    - ${{ compiler('cxx') }}
    - ${{ compiler('cuda') }}
    - ${{ stdlib('c') }}
    - ninja
    - cmake
    - mlir
    - if: win
      then: m2-sed
      else: sed
    - if: build_platform != target_platform
      then:
        - python
        - cross-python_${{ target_platform }}
  host:
    - python
    - pybind11
    - pip
    - setuptools
    - llvm
    - llvmdev
    - mlir
    - zlib
    - nlohmann_json
    - cuda-cupti-dev
    - cuda-version ${{ cuda_compiler_version }}.*
  run:
    - python
    - setuptools
    - cuda-nvcc-tools
    - cuda-cuobjdump
    - cuda-cudart
    - cuda-cupti

tests:
  - if: build_platform == target_platform
    then:
      - python:
          imports:
            - triton
            - triton._C.libtriton
          pip_check: true
      - files:
          source:
            - python/test/
        requirements:
          run:
            - pip
            - pytest
            - scipy
        script:
          # test suite essentially depends on availability of a physical GPU,
          # see https://github.com/triton-lang/triton/issues/466;
          # run a test that does not require a GPU but checks
          # if triton.compile() works
          - pytest -v python/test/unit/tools/test_aot.py::test_ttgir_to_ptx

about:
  license: MIT
  license_file: LICENSE
  summary: Development repository for the Triton language and compiler
  description: |
    This is the development repository of Triton, a language and compiler for writing highly efficient custom Deep-Learning primitives.
    The aim of Triton is to provide an open-source environment to write fast code at higher productivity than CUDA, but also with higher flexibility than other existing DSLs.
  homepage: https://github.com/triton-lang/triton
  repository: https://github.com/triton-lang/triton
  documentation: https://triton-lang.org/

extra:
  recipe-maintainers:
    - h-vetinari
